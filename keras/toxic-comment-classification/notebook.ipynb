{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Toxic comment classification with Keras\n",
    "\n",
    "Original notebook: https://www.kaggle.com/sarvajna/keras-sequential-model-lb-0-052"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install bentoml\n",
    "!pip install keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Conv1D, GlobalMaxPooling1D\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_classes = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
    "max_features = 20000\n",
    "max_text_length = 400\n",
    "embedding_dims = 50\n",
    "filters = 250\n",
    "kernel_size = 3\n",
    "hidden_dims = 250\n",
    "batch_size = 32\n",
    "epochs = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download data\n",
    "Please Download data with Kaggle at https://www.kaggle.com/sarvajna/keras-sequential-model-lb-0-052/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('./train.csv')\n",
    "print(train_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = train_df['comment_text'].values\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = train_df[list_of_classes].values\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_tokenizer = text.Tokenizer(num_words=max_features)\n",
    "print(x_tokenizer)\n",
    "x_tokenizer.fit_on_texts(list(x))\n",
    "print(x_tokenizer)\n",
    "x_tokenized = x_tokenizer.texts_to_sequences(x) #list of lists(containing numbers), so basically a list of sequences, not a numpy array\n",
    "#pad_sequences:transform a list of num_samples sequences (lists of scalars) into a 2D Numpy array of shape \n",
    "x_train_val = sequence.pad_sequences(x_tokenized, maxlen=max_text_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_val, y_train, y_val = train_test_split(x_train_val, y, test_size=0.1, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Build model...')\n",
    "model = Sequential()\n",
    "\n",
    "# we start off with an efficient embedding layer which maps\n",
    "# our vocab indices into embedding_dims dimensions\n",
    "model.add(Embedding(max_features,\n",
    "                    embedding_dims,\n",
    "                    input_length=max_text_length))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "# we add a Convolution1D, which will learn filters\n",
    "# word group filters of size filter_length:\n",
    "model.add(Conv1D(filters,\n",
    "                 kernel_size,\n",
    "                 padding='valid',\n",
    "                 activation='relu',\n",
    "                 strides=1))\n",
    "# we use max pooling:\n",
    "model.add(GlobalMaxPooling1D())\n",
    "\n",
    "# We add a vanilla hidden layer:\n",
    "model.add(Dense(hidden_dims))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "# We project onto 6 output layers, and squash it with a sigmoid:\n",
    "model.add(Dense(6))\n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv('./test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = test_df['comment_text'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test_tokenized = x_tokenizer.texts_to_sequences(x_test)\n",
    "x_testing = sequence.pad_sequences(x_test_tokenized, maxlen=max_text_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_testing = model.predict(x_testing, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_submission = pd.read_csv(\"./sample_submission.csv\")\n",
    "sample_submission[list_of_classes] = y_testing\n",
    "sample_submission.to_csv(\"toxic_comment_classification.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Service with BentoML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile toxic_comment_classifier.py\n",
    "\n",
    "from bentoml import api, artifacts, env, BentoService\n",
    "from bentoml.artifact import PickleArtifact\n",
    "from bentoml.handlers import DataframeHandler\n",
    "\n",
    "from keras.preprocessing import text, sequence\n",
    "import numpy as np\n",
    "\n",
    "list_of_classes = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
    "max_text_length = 400\n",
    "\n",
    "@env(conda_pip_dependencies=['keras', 'pandas', 'numpy'])\n",
    "@artifacts([PickleArtifact('x_tokenizer'), PickleArtifact('model')])\n",
    "class ToxicCommentClassification(BentoService):\n",
    "    def tokenize_df(self, df):\n",
    "        comments = df['comment_text'].values\n",
    "        tokenized = self.artifacts.x_tokenizer.texts_to_sequences(comments)        \n",
    "        input_data = sequence.pad_sequences(tokenized, maxlen=max_text_length)\n",
    "        return input_data\n",
    "    \n",
    "    @api(DataframeHandler)\n",
    "    def predict(self, df):\n",
    "        input_data = self.tokenize_df(df)\n",
    "        prediction = self.artifacts.model.predict(input_data)\n",
    "        result = []\n",
    "        for i in prediction:\n",
    "            result.append(list_of_classes[np.argmax(i)])\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2019-07-26 14:58:36,376] INFO - Copying local python module '/Users/bozhaoyu/src/bento_gallery/keras/toxic-comment-classification/toxic_comment_classifier.py'\n",
      "[2019-07-26 14:58:36,378] INFO - Done copying local python dependant modules\n",
      "[2019-07-26 14:58:36,468] INFO - BentoService ToxicCommentClassification:2019_07_26_e5011225 saved to /tmp/bento_archive/ToxicCommentClassification/2019_07_26_e5011225\n"
     ]
    }
   ],
   "source": [
    "from toxic_comment_classifier import ToxicCommentClassification\n",
    "\n",
    "svc = ToxicCommentClassification.pack(x_tokenizer=x_tokenizer, model=model)\n",
    "\n",
    "saved_path = svc.save('/tmp/bento_archive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use the archived service in other python application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_test = test_df.iloc[40:45]\n",
    "\n",
    "from bentoml import load\n",
    "\n",
    "bento_service = load(saved_path)\n",
    "print(bento_service.predict(sample_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
