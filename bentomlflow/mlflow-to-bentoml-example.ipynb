{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mlflow to BentoML\n",
    "\n",
    "[BentoML](http://bentoml.ai) is an open-source framework for machine learning **model serving**, aiming to **bridge the gap between Data Science and DevOps**.\n",
    "\n",
    "[MLflow](https://mlflow.org/) is an open source platform for the machine learning lifecycle, including experimentation, reproducibility, deployment, and a central model registry.\n",
    "\n",
    "You might want to use Mlflow to keep track of your  training but you would prefer to use BentoML to deploy your models in productions. You can see a comparison between the 2 [here](https://docs.bentoml.org/en/latest/faq.html?highlight=mlflow#how-does-bentoml-compare-to-mlflow)\n",
    "\n",
    "This notebook will demonstrate you how you can load a model from Mlflow model and package it with BentoML for deployment. We will break it down in the following parts:\n",
    "1. Train a model based on iris dataset and save it using MLflow\n",
    "2. Load the model from MLflow and package it with BentoML\n",
    "3. Containerize the model with docker\n",
    "\n",
    "BentoML requires python 3.6 or above, install dependencies via `pip`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install PyPI packages required in this guide, including BentoML\n",
    "!pip install -q bentoml  # install preview version of BentoML for this guide\n",
    "!pip install -q 'scikit-learn>=0.23.2' 'mlflow>=1.13.1' 'matplotlib'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Train a model and save it using MLflow\n",
    "Like in the quick-start, let's train a classifier model on the [Iris data set](https://en.wikipedia.org/wiki/Iris_flower_data_set):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "from sklearn import datasets\n",
    "import mlflow\n",
    "from mlflow.models.signature import infer_signature\n",
    "\n",
    "# Load training data\n",
    "iris = datasets.load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "# Model Training and saving in MLflow\n",
    "clf = svm.SVC(gamma='scale')\n",
    "with mlflow.start_run() as run:\n",
    "    clf.fit(X, y)\n",
    "    mlflow.sklearn.log_model(\n",
    "        sk_model=clf,\n",
    "        artifact_path=\"model\",\n",
    "        signature=infer_signature(X),\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model has been trained and saved in Mlflow. You can see it using the mlflow ui by running "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2021-02-13 23:26:33 +0100] [12078] [INFO] Starting gunicorn 20.0.4\n",
      "[2021-02-13 23:26:33 +0100] [12078] [INFO] Listening at: http://127.0.0.1:5001 (12078)\n",
      "[2021-02-13 23:26:33 +0100] [12078] [INFO] Using worker: sync\n",
      "[2021-02-13 23:26:33 +0100] [12081] [INFO] Booting worker with pid: 12081\n",
      "^C\n",
      "[2021-02-13 23:27:18 +0100] [12078] [INFO] Handling signal: int\n",
      "[2021-02-13 23:27:18 +0100] [12081] [INFO] Worker exiting (pid: 12081)\n"
     ]
    }
   ],
   "source": [
    "!mlflow ui --port=5001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Load the model from MLflow and package it with BentoML\n",
    "\n",
    "Like in the quick-start, the first step is creating a\n",
    "prediction service class, which defines the models required and the inference APIs which\n",
    "contains the serving logic. Here is a minimal prediction service created for serving\n",
    "the iris classifier model trained above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting iris_classifier.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile iris_classifier.py\n",
    "import pandas as pd\n",
    "\n",
    "from bentoml import env, artifacts, api, BentoService\n",
    "from bentoml.adapters import DataframeInput\n",
    "from bentoml.frameworks.sklearn import SklearnModelArtifact\n",
    "\n",
    "@env(infer_pip_packages=True)\n",
    "@artifacts([SklearnModelArtifact('model')])\n",
    "class IrisClassifier(BentoService):\n",
    "    \"\"\"\n",
    "    A minimum prediction service exposing a Scikit-learn model\n",
    "    \"\"\"\n",
    "\n",
    "    @api(input=DataframeInput(), batch=True)\n",
    "    def predict(self, df: pd.DataFrame):\n",
    "        \"\"\"\n",
    "        An inference API named `predict` with Dataframe input adapter, which codifies\n",
    "        how HTTP requests or CSV files are converted to a pandas Dataframe object as the\n",
    "        inference API function input\n",
    "        \"\"\"\n",
    "        return self.artifacts.model.predict(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code defines a prediction service that packages a scikit-learn model and provides\n",
    "an inference API that expects a `pandas.Dataframe` object as its input. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now load this MLflow model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving model with uri=runs:/7f1e9a8ce364450596460d5ef0f2e35f/model\n"
     ]
    }
   ],
   "source": [
    "model_uri = f\"runs:/{run.info.run_id}/model\"\n",
    "print(f\"Retrieving model with uri={model_uri}\")\n",
    "mlflow_loaded_model = mlflow.sklearn.load_model(model_uri)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code packages the model loaded from MLflow with the prediction service class\n",
    "`IrisClassifier` defined above, and then saves the IrisClassifier instance to disk \n",
    "in the BentoML format for distribution and deployment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2021-02-13 23:27:20,667] INFO - BentoService bundle 'IrisClassifier:20210213232719_B5F4D1' saved to: /home/theodore/bentoml/repository/IrisClassifier/20210213232719_B5F4D1\n"
     ]
    }
   ],
   "source": [
    "# import the IrisClassifier class defined above\n",
    "from iris_classifier import IrisClassifier\n",
    "\n",
    "# Create a iris classifier service instance\n",
    "iris_classifier_service = IrisClassifier()\n",
    "\n",
    "# Pack the newly trained model artifact\n",
    "iris_classifier_service.pack('model', mlflow_loaded_model)\n",
    "\n",
    "# Save the prediction service to disk for model serving\n",
    "saved_path = iris_classifier_service.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BentoML stores all packaged model files under the\n",
    "`~/bentoml/{service_name}/{service_version}` directory by default.\n",
    "The BentoML file format contains all the code, files, and configs required to \n",
    "deploy the model for serving.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## REST API Model Serving\n",
    "\n",
    "\n",
    "\n",
    "To start a REST API model server with the `IrisClassifier` saved above, use \n",
    "the `bentoml serve` command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2021-02-13 23:27:52,275] INFO - Getting latest version IrisClassifier:20210213232719_B5F4D1\n",
      "[2021-02-13 23:27:52,276] INFO - Starting BentoML API server in development mode..\n",
      " * Serving Flask app \"IrisClassifier\" (lazy loading)\n",
      " * Environment: production\n",
      "\u001b[31m   WARNING: This is a development server. Do not use it in a production deployment.\u001b[0m\n",
      "\u001b[2m   Use a production WSGI server instead.\u001b[0m\n",
      " * Debug mode: off\n",
      " * Running on http://127.0.0.1:5000/ (Press CTRL+C to quit)\n",
      "^C\n"
     ]
    }
   ],
   "source": [
    "!bentoml serve IrisClassifier:latest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are running this notebook from Google Colab, you can start the dev server with `--run-with-ngrok` option, to gain acccess to the API endpoint via a public endpoint managed by [ngrok](https://ngrok.com/): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2021-02-13 23:28:07,942] INFO - Getting latest version IrisClassifier:20210213232719_B5F4D1\n",
      "[2021-02-13 23:28:07,942] INFO - Starting BentoML API server in development mode..\n",
      "^C\n"
     ]
    }
   ],
   "source": [
    "!bentoml serve IrisClassifier:latest --run-with-ngrok"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `IrisClassifier` model is now served at `localhost:5000`. Use `curl` command to send\n",
    "a prediction request:\n",
    "\n",
    "```bash\n",
    "curl -i \\\n",
    "--header \"Content-Type: application/json\" \\\n",
    "--request POST \\\n",
    "--data '[[5.1, 3.5, 1.4, 0.2]]' \\\n",
    "localhost:5000/predict\n",
    "```\n",
    "\n",
    "Or with `python` and [request library](https://requests.readthedocs.io/):\n",
    "```python\n",
    "import requests\n",
    "response = requests.post(\"http://127.0.0.1:5000/predict\", json=[[5.1, 3.5, 1.4, 0.2]])\n",
    "print(response.text)\n",
    "```\n",
    "\n",
    "Note that BentoML API server automatically converts the Dataframe JSON format into a\n",
    "`pandas.DataFrame` object before sending it to the user-defined inference API function.\n",
    "\n",
    "The BentoML API server also provides a simple web UI dashboard.\n",
    "Go to http://localhost:5000 in the browser and use the Web UI to send\n",
    "prediction request:\n",
    "\n",
    "![BentoML API Server Web UI Screenshot](https://raw.githubusercontent.com/bentoml/BentoML/master/guides/quick-start/bento-api-server-web-ui.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Containerize the model with docker\n",
    "\n",
    "One common way of distributing this model API server for production deployment, is via\n",
    "Docker containers. And BentoML provides a convenient way to do that.\n",
    "\n",
    "Note that `docker` is __not available in Google Colab__. You will need to download and run this notebook locally to try out this containerization with docker feature.\n",
    "\n",
    "If you already have docker configured, simply run the follow command to product a \n",
    "docker container serving the `IrisClassifier` prediction service created above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2021-02-13 23:17:15,147] INFO - Getting latest version IrisClassifier:20210213230147_11DA63\n",
      "\u001b[39mFound Bento: /home/theodore/bentoml/repository/IrisClassifier/20210213230147_11DA63\u001b[0m\n",
      "[2021-02-13 23:17:15,185] WARNING - Using BentoML installed in `editable` model, the local BentoML repository including all code changes will be packaged together with saved bundle created, under the './bundled_pip_dependencies' directory of the saved bundle.\n",
      "[2021-02-13 23:17:15,212] WARNING - Saved BentoService bundle version mismatch: loading BentoService bundle create with BentoML version 0.11.0, but loading from BentoML version 0.11.0+17.g3ca0cc1\n",
      "Containerizing IrisClassifier:20210213230147_11DA63 with local YataiService and docker daemon from local environment-WARNING: No swap limit support\n",
      "\b|\u001b[32mBuild container image: iris-classifier:latest\u001b[0m\n",
      "\b \r"
     ]
    }
   ],
   "source": [
    "!bentoml containerize IrisClassifier:latest -t iris-classifier:latest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start a container with the docker image built in the previous step:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2021-02-13 22:17:26,238] INFO - Starting BentoML API server in production mode..\n",
      "[2021-02-13 22:17:26,262] INFO - Running micro batch service on :5000\n",
      "[2021-02-13 22:17:26 +0000] [1] [INFO] Starting gunicorn 20.0.4\n",
      "[2021-02-13 22:17:26 +0000] [10] [INFO] Starting gunicorn 20.0.4\n",
      "[2021-02-13 22:17:26 +0000] [1] [INFO] Listening at: http://0.0.0.0:55989 (1)\n",
      "[2021-02-13 22:17:26 +0000] [10] [INFO] Listening at: http://0.0.0.0:5000 (10)\n",
      "[2021-02-13 22:17:26 +0000] [10] [INFO] Using worker: aiohttp.worker.GunicornWebWorker\n",
      "[2021-02-13 22:17:26 +0000] [1] [INFO] Using worker: sync\n",
      "[2021-02-13 22:17:26 +0000] [12] [INFO] Booting worker with pid: 12\n",
      "[2021-02-13 22:17:26 +0000] [11] [INFO] Booting worker with pid: 11\n",
      "[2021-02-13 22:17:26,304] WARNING - Using BentoML installed in `editable` model, the local BentoML repository including all code changes will be packaged together with saved bundle created, under the './bundled_pip_dependencies' directory of the saved bundle.\n",
      "[2021-02-13 22:17:26,327] WARNING - Saved BentoService bundle version mismatch: loading BentoService bundle create with BentoML version 0.11.0, but loading from BentoML version 0.11.0+17.g3ca0cc1.dirty\n",
      "[2021-02-13 22:17:26,330] INFO - Micro batch enabled for API `predict` max-latency: 10000 max-batch-size 2000\n",
      "[2021-02-13 22:17:26,330] INFO - Your system nofile limit is 1048576, which means each instance of microbatch service is able to hold this number of connections at same time. You can increase the number of file descriptors for the server process, or launch more microbatch instances to accept more concurrent connection.\n",
      "[2021-02-13 22:17:27,126] WARNING - Using BentoML installed in `editable` model, the local BentoML repository including all code changes will be packaged together with saved bundle created, under the './bundled_pip_dependencies' directory of the saved bundle.\n",
      "[2021-02-13 22:17:27,150] WARNING - Saved BentoService bundle version mismatch: loading BentoService bundle create with BentoML version 0.11.0, but loading from BentoML version 0.11.0+17.g3ca0cc1.dirty\n"
     ]
    }
   ],
   "source": [
    "!docker run -p 5000:5000 iris-classifier:latest --workers=1 --enable-microbatch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This made it possible to deploy BentoML bundled ML models with platforms such as\n",
    "[Kubeflow](https://www.kubeflow.org/docs/components/serving/bentoml/),\n",
    "[Knative](https://knative.dev/community/samples/serving/machinelearning-python-bentoml/),\n",
    "[Kubernetes](https://docs.bentoml.org/en/latest/deployment/kubernetes.html), which\n",
    "provides advanced model deployment features such as auto-scaling, A/B testing,\n",
    "scale-to-zero, canary rollout and multi-armed bandit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "This is a very short example how you can load a model from MLflow and serve it using BentoML. \n",
    "\n",
    "We recently looked into building the integration, the idea was to make BentoML support and serve the model format created in MLFlow directly. Although the team has concluded it is probably a really bad idea. The main difficulty of doing that is MLFlow's model format is not really designed for serving. And when turning a trained model to a prediction service, there are a number of things that may require the users' attention, which is not supported in MLFlow. In particular, what is the input/output data schema of the prediction endpoint, what are the local code dependencies, and how to preprocess a batch of input data, so it can take advantage of the micro-batching mechanism provided by BentoML, etc.\n",
    "\n",
    "There might be other ways we can improve the integration with MLFlow, but for now, we decided to get started with this documentation on how users can potentially build a workflow that takes advantage of both frameworks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('bentoml': conda)",
   "language": "python",
   "name": "python38564bitbentomlcondac0f4429e8fbd4928bbc051166957c36a"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
