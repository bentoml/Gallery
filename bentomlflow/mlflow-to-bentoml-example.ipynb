{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mlflow to BentoML\n",
    "\n",
    "[BentoML](http://bentoml.ai) is an open-source framework for machine learning **model serving**, aiming to **bridge the gap between Data Science and DevOps**.\n",
    "\n",
    "[MLflow](https://mlflow.org/) is an open source platform for the machine learning lifecycle, including experimentation, reproducibility, deployment, and a central model registry.\n",
    "\n",
    "You might want to use Mlflow to keep track of your  training but you would prefer to use BentoML to deploy your models in productions. You can see a comparison between the two [here](https://docs.bentoml.org/en/latest/faq.html?highlight=mlflow#how-does-bentoml-compare-to-mlflow).\n",
    "\n",
    "This notebook will demonstrate you how you can load a model from Mlflow model and package it with BentoML for deployment. We will break it down in the following parts:\n",
    "1. Train a model based on iris dataset and save it using MLflow\n",
    "2. Load the model from MLflow and package it with BentoML\n",
    "3. Containerize the model with docker\n",
    "\n",
    "BentoML requires python 3.6 or above, install dependencies via `pip`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install PyPI packages required in this guide, including BentoML\n",
    "!pip install -q 'bentoml' 'scikit-learn>=0.23.2' 'mlflow>=1.13.1' 'matplotlib'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Train a model and save it using MLflow\n",
    "Like in the quick-start, let's train a classifier model on the [Iris data set](https://en.wikipedia.org/wiki/Iris_flower_data_set):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "from sklearn import datasets\n",
    "import mlflow\n",
    "\n",
    "# Load training data\n",
    "iris = datasets.load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "# Model Training and saving in MLflow\n",
    "clf = svm.SVC(gamma='scale')\n",
    "with mlflow.start_run() as run:\n",
    "    clf.fit(X, y)\n",
    "    mlflow.sklearn.log_model(\n",
    "        sk_model=clf,\n",
    "        artifact_path=\"model\",\n",
    "        signature=mlflow.models.signature.infer_signature(X),\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model has been trained and saved in Mlflow. You can see it using the mlflow ui by running "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2021-02-14 21:34:12 +0100] [25225] [INFO] Starting gunicorn 20.0.4\n",
      "[2021-02-14 21:34:12 +0100] [25225] [INFO] Listening at: http://127.0.0.1:5001 (25225)\n",
      "[2021-02-14 21:34:12 +0100] [25225] [INFO] Using worker: sync\n",
      "[2021-02-14 21:34:12 +0100] [25227] [INFO] Booting worker with pid: 25227\n",
      "^C\n",
      "[2021-02-14 21:34:14 +0100] [25225] [INFO] Handling signal: int\n",
      "[2021-02-14 21:34:14 +0100] [25227] [INFO] Worker exiting (pid: 25227)\n"
     ]
    }
   ],
   "source": [
    "# We use a different port than bentoml default service to avoid conflicts\n",
    "!mlflow ui --port=5001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Load the model from MLflow and package it with BentoML\n",
    "\n",
    "Like in the quick-start, the first step is creating a\n",
    "prediction service class, which defines the models required and the inference APIs which\n",
    "contains the serving logic. Here is a minimal prediction service created for serving\n",
    "the iris classifier model trained above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting iris_classifier.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile iris_classifier.py\n",
    "import pandas as pd\n",
    "\n",
    "from bentoml import env, artifacts, api, BentoService\n",
    "from bentoml.adapters import DataframeInput\n",
    "from bentoml.frameworks.sklearn import SklearnModelArtifact\n",
    "\n",
    "@env(infer_pip_packages=True)\n",
    "@artifacts([SklearnModelArtifact('model')])\n",
    "class IrisClassifier(BentoService):\n",
    "    \"\"\"\n",
    "    A minimum prediction service exposing a Scikit-learn model\n",
    "    \"\"\"\n",
    "\n",
    "    @api(input=DataframeInput(), batch=True)\n",
    "    def predict(self, df: pd.DataFrame):\n",
    "        \"\"\"\n",
    "        An inference API named `predict` with Dataframe input adapter, which codifies\n",
    "        how HTTP requests or CSV files are converted to a pandas Dataframe object as the\n",
    "        inference API function input\n",
    "        \"\"\"\n",
    "        return self.artifacts.model.predict(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code defines a prediction service that packages a scikit-learn model and provides\n",
    "an inference API that expects a `pandas.Dataframe` object as its input. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now load this MLflow model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving model with uri=runs:/227b660c7c1c496aa14918758eb401c9/model\n"
     ]
    }
   ],
   "source": [
    "model_uri = f\"runs:/{run.info.run_id}/model\"\n",
    "print(f\"Retrieving model with uri={model_uri}\")\n",
    "mlflow_loaded_model = mlflow.sklearn.load_model(model_uri)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code packages the model loaded from MLflow with the prediction service class\n",
    "`IrisClassifier` defined above, and then saves the IrisClassifier instance to disk \n",
    "in the BentoML format for distribution and deployment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2021-02-14 21:34:46,624] INFO - BentoService bundle 'IrisClassifier:20210214213445_DB9735' saved to: /home/theodore/bentoml/repository/IrisClassifier/20210214213445_DB9735\n"
     ]
    }
   ],
   "source": [
    "# import the IrisClassifier class defined above\n",
    "from iris_classifier import IrisClassifier\n",
    "\n",
    "# Create a iris classifier service instance\n",
    "iris_classifier_service = IrisClassifier()\n",
    "\n",
    "# Pack the newly trained model artifact\n",
    "iris_classifier_service.pack('model', mlflow_loaded_model)\n",
    "\n",
    "# Save the prediction service to disk for model serving\n",
    "saved_path = iris_classifier_service.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The MLflow model has been packed by BentoML  and stored under the default directory\n",
    "`~/bentoml/{service_name}/{service_version}` .\n",
    "The BentoML file format contains all the code, files, and configs required to \n",
    "deploy the model for serving.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Containerize the model with docker\n",
    "\n",
    "One common way of distributing this model API server for production deployment, is via\n",
    "Docker containers. And BentoML provides a convenient way to do that.\n",
    "\n",
    "Note that `docker` is __not available in Google Colab__. You will need to download and run this notebook locally to try out this containerization with docker feature.\n",
    "\n",
    "If you already have docker configured, simply run the follow command to product a \n",
    "docker container serving the `IrisClassifier` prediction service created above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2021-02-14 21:37:35,172] INFO - Getting latest version IrisClassifier:20210214213445_DB9735\n",
      "\u001b[39mFound Bento: /home/theodore/bentoml/repository/IrisClassifier/20210214213445_DB9735\u001b[0m\n",
      "Containerizing IrisClassifier:20210214213445_DB9735 with local YataiService and docker daemon from local environment\\WARNING: No swap limit support\n",
      "\b|\u001b[32mBuild container image: iris-classifier:latest\u001b[0m\n",
      "\b \r"
     ]
    }
   ],
   "source": [
    "!bentoml containerize IrisClassifier:latest -t iris-classifier:latest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start a container with the docker image built in the previous step:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2021-02-14 20:39:08,593] INFO - Starting BentoML API server in production mode..\n",
      "[2021-02-14 20:39:08,848] INFO - Running micro batch service on :5000\n",
      "[2021-02-14 20:39:08 +0000] [1] [INFO] Starting gunicorn 20.0.4\n",
      "[2021-02-14 20:39:08 +0000] [10] [INFO] Starting gunicorn 20.0.4\n",
      "[2021-02-14 20:39:08 +0000] [1] [INFO] Listening at: http://0.0.0.0:42851 (1)\n",
      "[2021-02-14 20:39:08 +0000] [10] [INFO] Listening at: http://0.0.0.0:5000 (10)\n",
      "[2021-02-14 20:39:08 +0000] [10] [INFO] Using worker: aiohttp.worker.GunicornWebWorker\n",
      "[2021-02-14 20:39:08 +0000] [1] [INFO] Using worker: sync\n",
      "[2021-02-14 20:39:08 +0000] [11] [INFO] Booting worker with pid: 11\n",
      "[2021-02-14 20:39:08 +0000] [12] [INFO] Booting worker with pid: 12\n",
      "[2021-02-14 20:39:08,923] INFO - Micro batch enabled for API `predict` max-latency: 10000 max-batch-size 2000\n",
      "[2021-02-14 20:39:08,923] INFO - Your system nofile limit is 1048576, which means each instance of microbatch service is able to hold this number of connections at same time. You can increase the number of file descriptors for the server process, or launch more microbatch instances to accept more concurrent connection.\n",
      "^C\n",
      "[2021-02-14 20:39:39 +0000] [1] [INFO] Handling signal: int\n",
      "[2021-02-14 20:39:40 +0000] [11] [INFO] Worker exiting (pid: 11)\n"
     ]
    }
   ],
   "source": [
    "!docker run -p 5000:5000 iris-classifier:latest --workers=1 --enable-microbatch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This made it possible to deploy BentoML bundled ML models with platforms such as\n",
    "[Kubeflow](https://www.kubeflow.org/docs/components/serving/bentoml/),\n",
    "[Knative](https://knative.dev/community/samples/serving/machinelearning-python-bentoml/),\n",
    "[Kubernetes](https://docs.bentoml.org/en/latest/deployment/kubernetes.html), which\n",
    "provides advanced model deployment features such as auto-scaling, A/B testing,\n",
    "scale-to-zero, canary rollout and multi-armed bandit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "This is a very short example how you can load a model from MLflow and serve it using BentoML. \n",
    "\n",
    "We recently looked into building an automated integration, the idea was to make BentoML support and serve the model format created in MLFlow directly. Although the team has concluded it is probably a really bad idea. The main difficulty of doing that is MLFlow's model format is not really designed for serving. And when turning a trained model to a prediction service, there are a number of things that may require the users' attention, which is not supported in MLFlow. In particular, what is the input/output data schema of the prediction endpoint, what are the local code dependencies, and how to preprocess a batch of input data, so it can take advantage of the micro-batching mechanism provided by BentoML, etc.\n",
    "\n",
    "There might be other ways we can improve the integration with MLFlow, but for now, we decided to get started with this documentation on how users can potentially build a workflow that takes advantage of both frameworks.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('bentoml': conda)",
   "language": "python",
   "name": "python38564bitbentomlcondac0f4429e8fbd4928bbc051166957c36a"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}