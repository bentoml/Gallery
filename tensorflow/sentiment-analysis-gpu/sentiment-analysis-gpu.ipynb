{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cb748d78",
   "metadata": {},
   "source": [
    "# BentoML Example: Tensorflow GPU Serving\n",
    "\n",
    "BentoML makes moving trained ML models to production easy:\n",
    "\n",
    "    Package models trained with any ML framework and reproduce them for model serving in production\n",
    "    Deploy anywhere for online API serving or offline batch serving\n",
    "    High-Performance API model server with adaptive micro-batching support\n",
    "    Central hub for managing models and deployment process via Web UI and APIs\n",
    "    Modular and flexible design making it adaptable to your infrastrcuture\n",
    "\n",
    "BentoML is a framework for serving, managing, and deploying machine learning models. It is aiming to bridge the gap between Data Science and DevOps, and enable teams to deliver prediction services in a fast, repeatable, and scalable way. Before reading this example project, be sure to check out the Getting started guide to learn about the basic concepts in BentoML.\n",
    "\n",
    "This notebook demonstrates how to serve your Tensorflow2.0 model with BentoML, building a Docker Images that has GPU supports. Please refers to [GPU Serving guides](https://docs.bentoml.org/en/latest/guides/gpu_serving.html) for more information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4b1e9d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3388d34e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q bentoml tensorflow==2.5.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23ecec62",
   "metadata": {},
   "source": [
    "We are building a sentiment analysis classifier with IMDB dataset, retrieved from [Standford's](https://ai.stanford.edu/~amaas/data/sentiment/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1840fb22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import string\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "\n",
    "from bentoml import BentoService, api, artifacts, env\n",
    "from bentoml.adapters import JsonInput\n",
    "from bentoml.frameworks.keras import KerasModelArtifact\n",
    "from bentoml.service.artifacts.common import PickleArtifact\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import config\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer, text_to_word_sequence\n",
    "from tensorflow.keras.models import model_from_json\n",
    "from tensorflow.keras.preprocessing.text import tokenizer_from_json\n",
    "from tensorflow.keras.layers import Activation, Dense, Dropout, Embedding, LSTM\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.optimizers import RMSprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2c9de2c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.config.list_physical_devices(\"GPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1006968",
   "metadata": {},
   "source": [
    "## Preprocessing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "16acc839",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(s):\n",
    "    return strip_punctuation(remove_br(s.lower()))\n",
    "\n",
    "\n",
    "def strip_punctuation(s):\n",
    "    for c in string.punctuation + \"â€™\":\n",
    "        s = s.replace(c, \"\")\n",
    "    return s\n",
    "\n",
    "\n",
    "def remove_br(s):\n",
    "    return s.replace(\"<br /><br />\", \"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d5d41a2",
   "metadata": {},
   "source": [
    "We will build our custom IMDB DataLoader using `sklearn.preprocessing.LabelEncoder` and `tf.keras.preprocessing.text.Tokenizer`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7e032b1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class IMDB:\n",
    "    def __init__(self, max_seq_len, vocab_size):\n",
    "        self.MAX_SEQ_LEN = max_seq_len\n",
    "        self.VOCAB_SIZE = vocab_size\n",
    "\n",
    "        print('Loading IMDB dataset')\n",
    "        df = pd.read_csv('data/imdb.csv', names=[\"X\", \"Y\"], skiprows=1)\n",
    "        # print(df.head())\n",
    "\n",
    "        # cast X to str and preprocess\n",
    "        df['X'] = df.X.apply(str)\n",
    "        df['X'] = df.X.apply(preprocess)\n",
    "\n",
    "        X = df.X\n",
    "        Y = df.Y\n",
    "\n",
    "        # encode labels\n",
    "        label_encoder = LabelEncoder()\n",
    "        Y = label_encoder.fit_transform(Y)\n",
    "        Y = Y.reshape(-1, 1)\n",
    "\n",
    "        # 15/85 train test split\n",
    "        self.X_train, self.X_test, self.Y_train, self.Y_test = train_test_split(\n",
    "            X, Y, test_size=0.15\n",
    "        )\n",
    "\n",
    "        self.tokenizer = Tokenizer(num_words=self.VOCAB_SIZE, oov_token=\"<OOV>\")\n",
    "        self.tokenizer.fit_on_texts(self.X_train)\n",
    "\n",
    "        self.tokenize()\n",
    "        self.pad()\n",
    "\n",
    "    def tokenize(self):\n",
    "        self.X_train = self.tokenizer.texts_to_sequences(self.X_train)\n",
    "        self.X_test = self.tokenizer.texts_to_sequences(self.X_test)\n",
    "\n",
    "    def pad(self):\n",
    "        self.X_train = pad_sequences(self.X_train, maxlen=self.MAX_SEQ_LEN, padding=\"post\")\n",
    "        self.X_test = pad_sequences(self.X_test, maxlen=self.MAX_SEQ_LEN, padding=\"post\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f3551b7",
   "metadata": {},
   "source": [
    "## Define our Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86000f20",
   "metadata": {},
   "source": [
    "We will build a simple Bidirectional RNN with LSTM using `tf.keras.models.Sequential`\n",
    "\n",
    "image source: [PaperWithCode](https://paperswithcode.com/method/bilstm#)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cdac55e",
   "metadata": {},
   "source": [
    "![bidirectional-lstm](./bidirectional-lstm.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5691a142",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF RNN model.\n",
    "def RNN(max_seq_len, vocab_size):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(vocab_size, 64, input_length=max_seq_len))\n",
    "    model.add(LSTM(64, return_sequences=True))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(LSTM(64))\n",
    "    model.add(Dense(256, name='fc1'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(1, name='out'))\n",
    "    model.add(Activation('sigmoid'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7adeb5e",
   "metadata": {},
   "source": [
    "## Preparing Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c4d029f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONSTANT\n",
    "VOCAB_SIZE = 5000\n",
    "MAX_SEQ_LEN = 100\n",
    "SEED = 1234"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f1902af",
   "metadata": {},
   "source": [
    "## Train and save our model locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "48b5ce66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 100, 64)           320000    \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 100, 64)           33024     \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 100, 64)           0         \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 64)                33024     \n",
      "_________________________________________________________________\n",
      "fc1 (Dense)                  (None, 256)               16640     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "out (Dense)                  (None, 1)                 257       \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 402,945\n",
      "Trainable params: 402,945\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Loading IMDB dataset\n"
     ]
    }
   ],
   "source": [
    "gpu = tf.config.experimental.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_memory_growth(gpu[0], True)  # gpu name: /GPU:0\n",
    "\n",
    "model = RNN(MAX_SEQ_LEN, VOCAB_SIZE)\n",
    "model.summary()\n",
    "model.compile(loss='binary_crossentropy', optimizer=RMSprop(), metrics=['accuracy'])\n",
    "\n",
    "imdb = IMDB(MAX_SEQ_LEN, VOCAB_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0dd71d3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "67/67 [==============================] - 7s 60ms/step - loss: 0.5635 - accuracy: 0.7247 - val_loss: 0.3957 - val_accuracy: 0.8127\n",
      "Epoch 2/10\n",
      "67/67 [==============================] - 4s 58ms/step - loss: 0.3692 - accuracy: 0.8395 - val_loss: 0.3747 - val_accuracy: 0.8507\n",
      "Epoch 3/10\n",
      "67/67 [==============================] - 3s 50ms/step - loss: 0.3250 - accuracy: 0.8640 - val_loss: 0.4527 - val_accuracy: 0.8072\n",
      "Epoch 4/10\n",
      "67/67 [==============================] - 4s 55ms/step - loss: 0.3031 - accuracy: 0.8736 - val_loss: 0.3747 - val_accuracy: 0.8491\n",
      "Epoch 5/10\n",
      "67/67 [==============================] - 4s 58ms/step - loss: 0.2861 - accuracy: 0.8822 - val_loss: 0.3445 - val_accuracy: 0.8518\n",
      "Epoch 6/10\n",
      "67/67 [==============================] - 3s 49ms/step - loss: 0.2726 - accuracy: 0.8901 - val_loss: 0.3840 - val_accuracy: 0.8280\n",
      "Epoch 7/10\n",
      "67/67 [==============================] - 3s 47ms/step - loss: 0.2577 - accuracy: 0.8961 - val_loss: 0.3581 - val_accuracy: 0.8542\n",
      "Epoch 00007: early stopping\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.3949 - accuracy: 0.8407\n",
      "Test set\n",
      "  Loss: 0.3949\n",
      "  Accuracy: 84.07\n",
      "Saved model to disk\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists(\"model\"):\n",
    "    os.makedirs(\"model\", exist_ok=True)\n",
    "    \n",
    "with tf.device(\"/GPU:0\"):\n",
    "    # Model Training\n",
    "    model.fit(\n",
    "        imdb.X_train,\n",
    "        imdb.Y_train,\n",
    "        batch_size=512,\n",
    "        epochs=10,\n",
    "        validation_split=0.2,\n",
    "        callbacks=[EarlyStopping(patience=2, verbose=1)],\n",
    "    )\n",
    "\n",
    "    # Run model on test set\n",
    "    accr = model.evaluate(imdb.X_test, imdb.Y_test)\n",
    "    print(\n",
    "        'Test set\\n  Loss: {:0.4f}\\n  Accuracy: {:0.2f}'.format(\n",
    "            accr[0], accr[1] * 100\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # save weights as HDF5\n",
    "    model.save(\"model/weights.h5\")\n",
    "    print(\"Saved model to disk\")\n",
    "\n",
    "    # save model as JSON\n",
    "    model_json = model.to_json()\n",
    "    with open(\"model/model.json\", \"w\") as file:\n",
    "        file.write(model_json)\n",
    "\n",
    "    # save tokenizer as JSON\n",
    "    tokenizer_json = imdb.tokenizer.to_json()\n",
    "    with open(\"model/tokenizer.json\", 'w', encoding='utf-8') as file:\n",
    "        file.write(json.dumps(tokenizer_json, ensure_ascii=True))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "360d1e44",
   "metadata": {},
   "source": [
    "## Defining our BentoService\n",
    "\n",
    "Please refers to our [GPU Serving guide](https://docs.bentoml.org/en/latest/guides/gpu_serving.html) to setup your environment correctly.\n",
    "\n",
    "We will be using Docker images provided by *BentoML* : `bentoml/model-server:0.12.1-py38-gpu` to prepare our CUDA-enabled images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "38ebcb49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting bento_svc.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile bento_svc.py\n",
    "\n",
    "import string\n",
    "\n",
    "from bentoml import BentoService, api, artifacts, env\n",
    "from bentoml.adapters import JsonInput\n",
    "from bentoml.frameworks.keras import KerasModelArtifact\n",
    "from bentoml.service.artifacts.common import PickleArtifact\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import text_to_word_sequence\n",
    "\n",
    "def preprocess(s):\n",
    "    return strip_punctuation(remove_br(s.lower()))\n",
    "\n",
    "\n",
    "def strip_punctuation(s):\n",
    "    for c in string.punctuation + \"â€™\":\n",
    "        s = s.replace(c, \"\")\n",
    "    return s\n",
    "\n",
    "\n",
    "def remove_br(s):\n",
    "    return s.replace(\"<br /><br />\", \"\")\n",
    "\n",
    "@env(requirements_txt_file=\"./requirements.txt\", docker_base_image=\"bentoml/model-server:0.12.1-py38-gpu\")\n",
    "@artifacts([KerasModelArtifact('model'), PickleArtifact('tokenizer')])\n",
    "class TensorflowService(BentoService):\n",
    "    def word_to_index(self, word):\n",
    "        if word in self.artifacts.tokenizer and self.artifacts.tokenizer[word] <= 5000:\n",
    "            return self.artifacts.tokenizer[word]\n",
    "        else:\n",
    "            return self.artifacts.tokenizer[\"<OOV>\"]\n",
    "\n",
    "    def preprocessing(self, text_str):\n",
    "        proc = text_to_word_sequence(preprocess(text_str))\n",
    "        tokens = list(map(self.word_to_index, proc))\n",
    "        return tokens\n",
    "\n",
    "    @api(input=JsonInput())\n",
    "    def predict(self, parsed_json):\n",
    "        raw = self.preprocessing(parsed_json['text'])\n",
    "        input_data = [raw[: n + 1] for n in range(len(raw))]\n",
    "        input_data = pad_sequences(input_data, maxlen=100, padding=\"post\")\n",
    "        return self.artifacts.model.predict(input_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85e0a733",
   "metadata": {},
   "source": [
    "## Pack our BentoService"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "88d4d6ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2021-06-04 12:20:08,777] WARNING - Using BentoML installed in `editable` model, the local BentoML repository including all code changes will be packaged together with saved bundle created, under the './bundled_pip_dependencies' directory of the saved bundle.\n",
      "[2021-06-04 12:20:08,875] INFO - Using user specified docker base image: `bentoml/model-server:0.12.1-py38-gpu`, usermust make sure that the base image either has Python 3.8 or conda installed.\n",
      "[2021-06-04 12:20:16,042] INFO - Detected non-PyPI-released BentoML installed, copying local BentoML modulefiles to target saved bundle path..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aarnphm/.pyenv/versions/3.8.8/lib/python3.8/site-packages/setuptools/distutils_patch.py:25: UserWarning: Distutils was imported before Setuptools. This usage is discouraged and may exhibit undesirable behaviors or errors. Please use Setuptools' objects directly or at least import Setuptools first.\n",
      "  warnings.warn(\n",
      "warning: no previously-included files matching '*~' found anywhere in distribution\n",
      "warning: no previously-included files matching '*.pyo' found anywhere in distribution\n",
      "warning: no previously-included files matching '.git' found anywhere in distribution\n",
      "warning: no previously-included files matching '.ipynb_checkpoints' found anywhere in distribution\n",
      "warning: no previously-included files matching '__pycache__' found anywhere in distribution\n",
      "no previously-included directories found matching 'e2e_tests'\n",
      "no previously-included directories found matching 'tests'\n",
      "no previously-included directories found matching 'benchmark'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UPDATING BentoML-0.12.1+53.g9d8b599/bentoml/_version.py\n",
      "set BentoML-0.12.1+53.g9d8b599/bentoml/_version.py to '0.12.1+53.g9d8b599'\n",
      "[2021-06-04 12:20:24,130] WARNING - Saved BentoService bundle version mismatch: loading BentoService bundle create with BentoML version 0.12.1, but loading from BentoML version 0.12.1+53.g9d8b599\n",
      "[2021-06-04 12:20:24,200] INFO - BentoService bundle 'TensorflowService:20210604122013_B189A7' saved to: /home/aarnphm/bentoml/repository/TensorflowService/20210604122013_B189A7\n"
     ]
    }
   ],
   "source": [
    "from bento_svc import TensorflowService\n",
    "\n",
    "gpu = config.experimental.list_physical_devices('GPU')\n",
    "config.experimental.set_memory_growth(gpu[0], True)\n",
    "\n",
    "def load_tokenizer():\n",
    "    with open('model/tokenizer.json', 'r') as f:\n",
    "        data = json.load(f)\n",
    "        tokenizer = tokenizer_from_json(data)\n",
    "        j = tokenizer.get_config()['word_index']\n",
    "        return json.loads(j)\n",
    "\n",
    "\n",
    "def load_model():\n",
    "    # load json and create model\n",
    "    json_file = open('model/model.json', 'r')\n",
    "    loaded_model_json = json_file.read()\n",
    "    json_file.close()\n",
    "    model = model_from_json(loaded_model_json)\n",
    "    model.load_weights(\"model/weights.h5\")\n",
    "    return model\n",
    "\n",
    "\n",
    "model = load_model()\n",
    "tokenizer = load_tokenizer()\n",
    "\n",
    "bento_svc = TensorflowService()\n",
    "bento_svc.pack('model', model)\n",
    "bento_svc.pack('tokenizer', tokenizer)\n",
    "\n",
    "saved_path = bento_svc.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e45ffcbe",
   "metadata": {},
   "source": [
    "## REST API Model Serving\n",
    "\n",
    "To start a REST API model server with the BentoService save above, use the `serve` command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3b448904",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2021-06-04 12:20:25,979] INFO - Getting latest version TensorflowService:20210604122013_B189A7\n",
      "[2021-06-04 12:20:25,988] INFO - Starting BentoML API proxy in development mode..\n",
      "[2021-06-04 12:20:25,990] INFO - Starting BentoML API server in development mode..\n",
      "[2021-06-04 12:20:26,042] WARNING - Using BentoML installed in `editable` model, the local BentoML repository including all code changes will be packaged together with saved bundle created, under the './bundled_pip_dependencies' directory of the saved bundle.\n",
      "[2021-06-04 12:20:26,042] WARNING - Using BentoML installed in `editable` model, the local BentoML repository including all code changes will be packaged together with saved bundle created, under the './bundled_pip_dependencies' directory of the saved bundle.\n",
      "[2021-06-04 12:20:26,147] WARNING - Saved BentoService bundle version mismatch: loading BentoService bundle create with BentoML version 0.12.1, but loading from BentoML version 0.12.1+53.g9d8b599\n",
      "[2021-06-04 12:20:26,149] INFO - Your system nofile limit is 4096, which means each instance of microbatch service is able to hold this number of connections at same time. You can increase the number of file descriptors for the server process, or launch more microbatch instances to accept more concurrent connection.\n",
      "[2021-06-04 12:20:26,154] WARNING - Saved BentoService bundle version mismatch: loading BentoService bundle create with BentoML version 0.12.1, but loading from BentoML version 0.12.1+53.g9d8b599\n",
      "======== Running on http://0.0.0.0:5000 ========\n",
      "(Press CTRL+C to quit)\n",
      "2021-06-04 12:20:26.380507: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
      "[2021-06-04 12:20:28,087] INFO - Using user specified docker base image: `bentoml/model-server:0.12.1-py38-gpu`, usermust make sure that the base image either has Python 3.8 or conda installed.\n",
      "2021-06-04 12:20:28.099245: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcuda.so.1\n",
      "2021-06-04 12:20:28.118931: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-06-04 12:20:28.119434: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1733] Found device 0 with properties: \n",
      "pciBusID: 0000:01:00.0 name: NVIDIA GeForce GTX 1060 computeCapability: 6.1\n",
      "coreClock: 1.6705GHz coreCount: 10 deviceMemorySize: 5.94GiB deviceMemoryBandwidth: 178.99GiB/s\n",
      "2021-06-04 12:20:28.119502: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
      "2021-06-04 12:20:28.131970: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcublas.so.11\n",
      "2021-06-04 12:20:28.132117: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcublasLt.so.11\n",
      "2021-06-04 12:20:28.137961: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcufft.so.10\n",
      "2021-06-04 12:20:28.360009: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcurand.so.10\n",
      "2021-06-04 12:20:28.480193: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcusolver.so.11\n",
      "2021-06-04 12:20:28.484125: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcusparse.so.11\n",
      "2021-06-04 12:20:28.485443: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudnn.so.8\n",
      "2021-06-04 12:20:28.485582: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-06-04 12:20:28.485970: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-06-04 12:20:28.486218: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1871] Adding visible gpu devices: 0\n",
      "2021-06-04 12:20:28.486567: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-06-04 12:20:28.487296: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-06-04 12:20:28.487621: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1733] Found device 0 with properties: \n",
      "pciBusID: 0000:01:00.0 name: NVIDIA GeForce GTX 1060 computeCapability: 6.1\n",
      "coreClock: 1.6705GHz coreCount: 10 deviceMemorySize: 5.94GiB deviceMemoryBandwidth: 178.99GiB/s\n",
      "2021-06-04 12:20:28.487680: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-06-04 12:20:28.487956: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-06-04 12:20:28.488192: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1871] Adding visible gpu devices: 0\n",
      "2021-06-04 12:20:28.488229: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
      "2021-06-04 12:20:29.393920: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1258] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2021-06-04 12:20:29.393954: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1264]      0 \n",
      "2021-06-04 12:20:29.393964: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1277] 0:   N \n",
      "2021-06-04 12:20:29.394176: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-06-04 12:20:29.394601: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-06-04 12:20:29.394955: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-06-04 12:20:29.395281: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1418] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 4503 MB memory) -> physical GPU (device: 0, name: NVIDIA GeForce GTX 1060, pci bus id: 0000:01:00.0, compute capability: 6.1)\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:From /mnt/Centralized/documents/cs/BentoML/bentoml/frameworks/keras.py:122: The name tf.keras.backend.get_session is deprecated. Please use tf.compat.v1.keras.backend.get_session instead.\n",
      "\n",
      "2021-06-04 12:20:30.652331: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-06-04 12:20:30.652660: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1733] Found device 0 with properties: \n",
      "pciBusID: 0000:01:00.0 name: NVIDIA GeForce GTX 1060 computeCapability: 6.1\n",
      "coreClock: 1.6705GHz coreCount: 10 deviceMemorySize: 5.94GiB deviceMemoryBandwidth: 178.99GiB/s\n",
      "2021-06-04 12:20:30.652770: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-06-04 12:20:30.653035: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-06-04 12:20:30.653233: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1871] Adding visible gpu devices: 0\n",
      "2021-06-04 12:20:30.653273: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1258] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2021-06-04 12:20:30.653290: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1264]      0 \n",
      "2021-06-04 12:20:30.653303: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1277] 0:   N \n",
      "2021-06-04 12:20:30.653407: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-06-04 12:20:30.653663: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-06-04 12:20:30.653883: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1418] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 4503 MB memory) -> physical GPU (device: 0, name: NVIDIA GeForce GTX 1060, pci bus id: 0000:01:00.0, compute capability: 6.1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app 'TensorflowService' (lazy loading)\n",
      " * Environment: production\n",
      "\u001b[31m   WARNING: This is a development server. Do not use it in a production deployment.\u001b[0m\n",
      "\u001b[2m   Use a production WSGI server instead.\u001b[0m\n",
      " * Debug mode: off\n",
      "INFO:werkzeug: * Running on http://127.0.0.1:54691/ (Press CTRL+C to quit)\n",
      "INFO:werkzeug:127.0.0.1 - - [04/Jun/2021 12:20:35] \"GET / HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:127.0.0.1 - - [04/Jun/2021 12:20:35] \"GET /static_content/main.css HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:127.0.0.1 - - [04/Jun/2021 12:20:35] \"GET /static_content/swagger-ui.css HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:127.0.0.1 - - [04/Jun/2021 12:20:35] \"GET /static_content/readme.css HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:127.0.0.1 - - [04/Jun/2021 12:20:35] \"GET /static_content/marked.min.js HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:127.0.0.1 - - [04/Jun/2021 12:20:35] \"\u001b[36mGET /static_content/swagger-ui-bundle.js HTTP/1.1\u001b[0m\" 304 -\n",
      "INFO:werkzeug:127.0.0.1 - - [04/Jun/2021 12:20:36] \"GET /docs.json HTTP/1.1\" 200 -\n",
      "2021-06-04 12:20:56.380680: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:176] None of the MLIR Optimization Passes are enabled (registered 2)\n",
      "2021-06-04 12:20:56.399949: I tensorflow/core/platform/profile_utils/cpu_utils.cc:114] CPU Frequency: 2200660000 Hz\n",
      "2021-06-04 12:20:57.115011: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudnn.so.8\n",
      "2021-06-04 12:20:57.610306: I tensorflow/stream_executor/cuda/cuda_dnn.cc:359] Loaded cuDNN version 8200\n",
      "2021-06-04 12:20:57.750504: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcublas.so.11\n",
      "2021-06-04 12:20:58.283961: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcublasLt.so.11\n",
      "[2021-06-04 12:20:58,286] INFO - {'service_name': 'TensorflowService', 'service_version': '20210604122013_B189A7', 'api': 'predict', 'task': {'data': '{\"text\":\"I love you so much\"}', 'task_id': '90d2c57d-1b03-457a-83c3-f8fd9d0608c6', 'http_headers': (('Host', 'localhost:5000'), ('User-Agent', 'Mozilla/5.0 (X11; Linux x86_64; rv:88.0) Gecko/20100101 Firefox/88.0'), ('Accept', '*/*'), ('Accept-Language', 'en-US,en;q=0.5'), ('Accept-Encoding', 'gzip, deflate'), ('Referer', 'http://localhost:5000/'), ('Content-Type', 'application/json'), ('Origin', 'http://localhost:5000'), ('Content-Length', '29'), ('Dnt', '1'), ('Connection', 'keep-alive'), ('Cookie', 'username-localhost-8888=\"2|1:0|10:1622781183|23:username-localhost-8888|44:OTAyZmUwZjlkZjI0NDUzZWJhNGIyZjkwMTk0ZDEzM2Y=|2839c399f1b58b774ed0b74f4c4fadd492f06a523985ee04636762b8a624351c\"; _xsrf=2|33d87053|2d1249d56e8ad5d63c884ea9a243cc2a|1622740800; username-localhost-8889=\"2|1:0|10:1622781169|23:username-localhost-8889|44:MDYwN2JlMDFmYWMyNGRlNmIwOWRjOTNjZDI5MmMwOWQ=|9dbbfeba548b71497371eb9688b6aa246ca19720c5e2b043cfbaa436a77ce27a\"'))}, 'result': {'data': '[[0.26499098539352417], [0.7981948256492615], [0.8961592316627502], [0.9163526296615601], [0.8221032023429871]]', 'http_status': 200, 'http_headers': (('Content-Type', 'application/json'),)}, 'request_id': '90d2c57d-1b03-457a-83c3-f8fd9d0608c6'}\n",
      "INFO:werkzeug:127.0.0.1 - - [04/Jun/2021 12:20:58] \"POST /predict HTTP/1.1\" 200 -\n",
      "^C\n"
     ]
    }
   ],
   "source": [
    "!bentoml serve TensorflowService:latest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "577cdfb0",
   "metadata": {},
   "source": [
    "Check if `BentoService` is running on GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c43eaca8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Jun  4 12:21:07 2021       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 465.31       Driver Version: 465.31       CUDA Version: 11.3     |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|                               |                      |               MIG M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  NVIDIA GeForce ...  Off  | 00000000:01:00.0 Off |                  N/A |\r\n",
      "| N/A   77C    P2    31W /  N/A |    891MiB /  6078MiB |      0%      Default |\r\n",
      "|                               |                      |                  N/A |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                                  |\r\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\r\n",
      "|        ID   ID                                                   Usage      |\r\n",
      "|=============================================================================|\r\n",
      "|    0   N/A  N/A      1191      G   /usr/lib/Xorg                       4MiB |\r\n",
      "|    0   N/A  N/A    481731      C   ...sions/3.8.8/bin/python3.8      883MiB |\r\n",
      "+-----------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16977b18",
   "metadata": {},
   "source": [
    "If you are running this notebook from Google Colab, start the dev server with `--run-with-ngrok` option to gain access to the API endpoint via a public endpoint managed by [ngrok](https://ngrok.com/):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71841580",
   "metadata": {},
   "outputs": [],
   "source": [
    "!bentoml serve PyTorchFashionClassifier:latest --run-with-ngrok"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9804f147",
   "metadata": {},
   "source": [
    "## Containerize our model server with Docker\n",
    "\n",
    "One common way of distributing this model API server for production deployment, is via Docker containers. And BentoML provides a convenient way to do that.\n",
    "\n",
    "Note that docker is not available in Google Colab. You will need to download and run this notebook locally to try out this containerization with docker feature.\n",
    "\n",
    "If you already have docker configured, simply run the follow command to product a docker container serving the IrisClassifier prediction service created above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c78e18fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2021-06-04 12:21:09,517] INFO - Getting latest version TensorflowService:20210604122013_B189A7\n",
      "\u001b[39mFound Bento: /home/aarnphm/bentoml/repository/TensorflowService/20210604122013_B189A7\u001b[0m\n",
      "[2021-06-04 12:21:09,545] WARNING - Using BentoML installed in `editable` model, the local BentoML repository including all code changes will be packaged together with saved bundle created, under the './bundled_pip_dependencies' directory of the saved bundle.\n",
      "[2021-06-04 12:21:09,617] WARNING - Saved BentoService bundle version mismatch: loading BentoService bundle create with BentoML version 0.12.1, but loading from BentoML version 0.12.1+53.g9d8b599\n",
      "Containerizing TensorflowService:20210604122013_B189A7 with local YataiService and docker daemon from local environment\\^C\n",
      "\b \r"
     ]
    }
   ],
   "source": [
    "!bentoml containerize TensorflowService:latest -t tensorflow-service-gpu:latest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82ad1d18",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2021-06-04 05:21:16,787] INFO - Starting BentoML proxy in production mode..\n",
      "[2021-06-04 05:21:16,788] INFO - Starting BentoML API server in production mode..\n",
      "[2021-06-04 05:21:16,803] INFO - Running micro batch service on :5000\n",
      "[2021-06-04 05:21:16 +0000] [19] [INFO] Starting gunicorn 20.1.0\n",
      "[2021-06-04 05:21:16 +0000] [19] [INFO] Listening at: http://0.0.0.0:53393 (19)\n",
      "[2021-06-04 05:21:16 +0000] [19] [INFO] Using worker: sync\n",
      "[2021-06-04 05:21:16 +0000] [20] [INFO] Booting worker with pid: 20\n",
      "[2021-06-04 05:21:16,823] WARNING - Using BentoML not from official PyPI release. In order to find the same version of BentoML when deploying your BentoService, you must set the 'core/bentoml_deploy_version' config to a http/git location of your BentoML fork, e.g.: 'bentoml_deploy_version = git+https://github.com/{username}/bentoml.git@{branch}'\n",
      "[2021-06-04 05:21:16,849] WARNING - Saved BentoService bundle version mismatch: loading BentoService bundle create with BentoML version 0.12.1, but loading from BentoML version 0.12.1+53.g9d8b599\n",
      "[2021-06-04 05:21:16 +0000] [1] [INFO] Starting gunicorn 20.1.0\n",
      "[2021-06-04 05:21:16 +0000] [1] [INFO] Listening at: http://0.0.0.0:5000 (1)\n",
      "[2021-06-04 05:21:16 +0000] [1] [INFO] Using worker: aiohttp.worker.GunicornWebWorker\n",
      "[2021-06-04 05:21:16 +0000] [21] [INFO] Booting worker with pid: 21\n",
      "[2021-06-04 05:21:16,947] WARNING - Using BentoML not from official PyPI release. In order to find the same version of BentoML when deploying your BentoService, you must set the 'core/bentoml_deploy_version' config to a http/git location of your BentoML fork, e.g.: 'bentoml_deploy_version = git+https://github.com/{username}/bentoml.git@{branch}'\n",
      "[2021-06-04 05:21:16,971] WARNING - Saved BentoService bundle version mismatch: loading BentoService bundle create with BentoML version 0.12.1, but loading from BentoML version 0.12.1+53.g9d8b599\n",
      "[2021-06-04 05:21:16,974] INFO - Your system nofile limit is 1048576, which means each instance of microbatch service is able to hold this number of connections at same time. You can increase the number of file descriptors for the server process, or launch more microbatch instances to accept more concurrent connection.\n",
      "2021-06-04 05:21:17.342130: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
      "[2021-06-04 05:21:19,523] INFO - Using user specified docker base image: `bentoml/model-server:0.12.1-py38-gpu`, usermust make sure that the base image either has Python 3.8 or conda installed.\n",
      "2021-06-04 05:21:19.536261: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcuda.so.1\n",
      "2021-06-04 05:21:19.558690: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-06-04 05:21:19.559138: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1733] Found device 0 with properties: \n",
      "pciBusID: 0000:01:00.0 name: NVIDIA GeForce GTX 1060 computeCapability: 6.1\n",
      "coreClock: 1.6705GHz coreCount: 10 deviceMemorySize: 5.94GiB deviceMemoryBandwidth: 178.99GiB/s\n",
      "2021-06-04 05:21:19.559228: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
      "2021-06-04 05:21:19.586639: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcublas.so.11\n",
      "2021-06-04 05:21:19.586756: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcublasLt.so.11\n",
      "2021-06-04 05:21:19.593522: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcufft.so.10\n",
      "2021-06-04 05:21:19.597825: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcurand.so.10\n",
      "2021-06-04 05:21:19.601710: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcusolver.so.11\n",
      "2021-06-04 05:21:19.608838: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcusparse.so.11\n",
      "2021-06-04 05:21:19.610037: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudnn.so.8\n",
      "2021-06-04 05:21:19.610199: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-06-04 05:21:19.610730: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-06-04 05:21:19.611722: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1871] Adding visible gpu devices: 0\n",
      "2021-06-04 05:21:19.612498: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-06-04 05:21:19.614042: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-06-04 05:21:19.614548: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1733] Found device 0 with properties: \n",
      "pciBusID: 0000:01:00.0 name: NVIDIA GeForce GTX 1060 computeCapability: 6.1\n",
      "coreClock: 1.6705GHz coreCount: 10 deviceMemorySize: 5.94GiB deviceMemoryBandwidth: 178.99GiB/s\n",
      "2021-06-04 05:21:19.614702: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-06-04 05:21:19.615167: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-06-04 05:21:19.615578: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1871] Adding visible gpu devices: 0\n",
      "2021-06-04 05:21:19.616112: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
      "2021-06-04 05:21:20.642554: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1258] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2021-06-04 05:21:20.642580: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1264]      0 \n",
      "2021-06-04 05:21:20.642587: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1277] 0:   N \n",
      "2021-06-04 05:21:20.642757: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-06-04 05:21:20.643107: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-06-04 05:21:20.643407: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-06-04 05:21:20.643675: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1418] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 4503 MB memory) -> physical GPU (device: 0, name: NVIDIA GeForce GTX 1060, pci bus id: 0000:01:00.0, compute capability: 6.1)\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:From /opt/conda/lib/python3.8/site-packages/bentoml/frameworks/keras.py:122: The name tf.keras.backend.get_session is deprecated. Please use tf.compat.v1.keras.backend.get_session instead.\n",
      "\n",
      "2021-06-04 05:21:21.537830: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-06-04 05:21:21.538129: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1733] Found device 0 with properties: \n",
      "pciBusID: 0000:01:00.0 name: NVIDIA GeForce GTX 1060 computeCapability: 6.1\n",
      "coreClock: 1.6705GHz coreCount: 10 deviceMemorySize: 5.94GiB deviceMemoryBandwidth: 178.99GiB/s\n",
      "2021-06-04 05:21:21.538243: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-06-04 05:21:21.538486: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-06-04 05:21:21.538691: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1871] Adding visible gpu devices: 0\n",
      "2021-06-04 05:21:21.538733: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1258] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2021-06-04 05:21:21.538739: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1264]      0 \n",
      "2021-06-04 05:21:21.538745: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1277] 0:   N \n",
      "2021-06-04 05:21:21.538848: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-06-04 05:21:21.539087: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-06-04 05:21:21.539287: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1418] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 4503 MB memory) -> physical GPU (device: 0, name: NVIDIA GeForce GTX 1060, pci bus id: 0000:01:00.0, compute capability: 6.1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-06-04 05:21:34.021454: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:176] None of the MLIR Optimization Passes are enabled (registered 2)\n",
      "2021-06-04 05:21:34.039973: I tensorflow/core/platform/profile_utils/cpu_utils.cc:114] CPU Frequency: 2200660000 Hz\n",
      "2021-06-04 05:21:34.761591: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudnn.so.8\n",
      "2021-06-04 05:21:35.535064: I tensorflow/stream_executor/cuda/cuda_dnn.cc:359] Loaded cuDNN version 8200\n",
      "2021-06-04 05:21:35.735112: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcublas.so.11\n",
      "2021-06-04 05:21:36.770853: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcublasLt.so.11\n",
      "[2021-06-04 05:21:36,780] INFO - {'service_name': 'TensorflowService', 'service_version': '20210604120626_2294FE', 'api': 'predict', 'task': {'data': '{\"text\":\"I love you so much\"}', 'task_id': 'a7f4d75e-a078-4549-8591-24022920b4f7', 'http_headers': (('Host', 'localhost:5000'), ('User-Agent', 'Mozilla/5.0 (X11; Linux x86_64; rv:88.0) Gecko/20100101 Firefox/88.0'), ('Accept', '*/*'), ('Accept-Language', 'en-US,en;q=0.5'), ('Accept-Encoding', 'gzip, deflate'), ('Referer', 'http://localhost:5000/'), ('Content-Type', 'application/json'), ('Origin', 'http://localhost:5000'), ('Content-Length', '29'), ('Dnt', '1'), ('Connection', 'keep-alive'), ('Cookie', 'username-localhost-8888=\"2|1:0|10:1622781183|23:username-localhost-8888|44:OTAyZmUwZjlkZjI0NDUzZWJhNGIyZjkwMTk0ZDEzM2Y=|2839c399f1b58b774ed0b74f4c4fadd492f06a523985ee04636762b8a624351c\"; _xsrf=2|33d87053|2d1249d56e8ad5d63c884ea9a243cc2a|1622740800; username-localhost-8889=\"2|1:0|10:1622781169|23:username-localhost-8889|44:MDYwN2JlMDFmYWMyNGRlNmIwOWRjOTNjZDI5MmMwOWQ=|9dbbfeba548b71497371eb9688b6aa246ca19720c5e2b043cfbaa436a77ce27a\"'))}, 'result': {'data': '[[0.9863787889480591], [0.9890207648277283], [0.989767849445343], [0.9897157549858093], [0.9891697764396667]]', 'http_status': 200, 'http_headers': (('Content-Type', 'application/json'),)}, 'request_id': 'a7f4d75e-a078-4549-8591-24022920b4f7'}\n"
     ]
    }
   ],
   "source": [
    "!docker run --gpus all --device /dev/nvidia0 --device /dev/nvidiactl --device /dev/nvidia-modeset --device /dev/nvidia-uvm --device /dev/nvidia-uvm-tools -p 5000:5000 tensorflow-service-gpu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0f81452",
   "metadata": {},
   "source": [
    "## Deployment Options\n",
    "\n",
    "If you are at a small team with limited engineering or DevOps resources, try out automated deployment with BentoML CLI, currently supporting AWS Lambda, AWS SageMaker, and Azure Functions:\n",
    "- [AWS Lambda Deployment Guide](https://docs.bentoml.org/en/latest/deployment/aws_lambda.html)\n",
    "- [AWS SageMaker Deployment Guide](https://docs.bentoml.org/en/latest/deployment/aws_sagemaker.html)\n",
    "- [Azure Functions Deployment Guide](https://docs.bentoml.org/en/latest/deployment/azure_functions.html)\n",
    "\n",
    "If the cloud platform you are working with is not on the list above, try out these step-by-step guide on manually deploying BentoML packaged model to cloud platforms:\n",
    "- [AWS ECS Deployment](https://docs.bentoml.org/en/latest/deployment/aws_ecs.html)\n",
    "- [Google Cloud Run Deployment](https://docs.bentoml.org/en/latest/deployment/google_cloud_run.html)\n",
    "- [Azure container instance Deployment](https://docs.bentoml.org/en/latest/deployment/azure_container_instance.html)\n",
    "- [Heroku Deployment](https://docs.bentoml.org/en/latest/deployment/heroku.html)\n",
    "\n",
    "Lastly, if you have a DevOps or ML Engineering team who's operating a Kubernetes or OpenShift cluster, use the following guides as references for implementating your deployment strategy:\n",
    "- [Kubernetes Deployment](https://docs.bentoml.org/en/latest/deployment/kubernetes.html)\n",
    "- [Knative Deployment](https://docs.bentoml.org/en/latest/deployment/knative.html)\n",
    "- [Kubeflow Deployment](https://docs.bentoml.org/en/latest/deployment/kubeflow.html)\n",
    "- [KFServing Deployment](https://docs.bentoml.org/en/latest/deployment/kfserving.html)\n",
    "- [Clipper.ai Deployment Guide](https://docs.bentoml.org/en/latest/deployment/clipper.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8f0046f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
