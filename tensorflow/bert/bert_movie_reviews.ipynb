{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "j0a4mTk9o1Qg"
   },
   "outputs": [],
   "source": [
    "# Copyright 2019 Google Inc.\n",
    "\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dCpvgG0vwXAZ"
   },
   "source": [
    "# Predicting Movie Review Sentiment with [kpe/bert-for-tf2](https://github.com/kpe/bert-for-tf2)\n",
    "\n",
    "\n",
    "A modification of https://github.com/kpe/bert-for-tf2/blob/master/examples/gpu_movie_reviews.ipynb,\n",
    "which is a modification of https://github/google-research/bert/blob/master/predicting_movie_reviews_with_bert_on_tf_hub.ipynb using the Tensorflow 2.0 Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tqdm in /opt/anaconda3/envs/bentoml-dev-py36/lib/python3.6/site-packages (4.45.0)\n",
      "Requirement already satisfied: bert-for-tf2 in /opt/anaconda3/envs/bentoml-dev-py36/lib/python3.6/site-packages (0.14.1)\n",
      "Requirement already satisfied: params-flow>=0.8.0 in /home/bentoml/.local/lib/python3.6/site-packages (from bert-for-tf2) (0.8.0)\n",
      "Requirement already satisfied: py-params>=0.9.6 in /home/bentoml/.local/lib/python3.6/site-packages (from bert-for-tf2) (0.9.7)\n",
      "Requirement already satisfied: numpy in /opt/anaconda3/envs/bentoml-dev-py36/lib/python3.6/site-packages (from params-flow>=0.8.0->bert-for-tf2) (1.18.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install tqdm bert-for-tf2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hsZvic2YxnTz"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import math\n",
    "import datetime\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# tf.config.set_visible_devices([], 'GPU')  # disable GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "Evlk1N78HIXM",
    "outputId": "9ada0ad2-3297-414d-b7bb-748063302382"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow:  2.1.0\n",
      "Python:  3.6.10 |Anaconda, Inc.| (default, Jan  7 2020, 21:14:29) \n",
      "[GCC 7.3.0]\n",
      "WARNING:tensorflow:From <ipython-input-2-7068fd7facab>:3: is_gpu_available (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.config.list_physical_devices('GPU')` instead.\n",
      "GPU:  True\n"
     ]
    }
   ],
   "source": [
    "print(\"Tensorflow: \", tf.__version__)\n",
    "print(\"Python: \", sys.version)\n",
    "print(\"GPU: \", tf.test.is_gpu_available())\n",
    "assert sys.version_info.major == 3 and sys.version_info.minor == 6  # required by clipper benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZtI7cKWDbUVc"
   },
   "outputs": [],
   "source": [
    "import bert\n",
    "from bert import BertModelLayer\n",
    "from bert.loader import StockBertConfig, map_stock_config_to_params, load_stock_weights\n",
    "from bert.tokenization.bert_tokenization import FullTokenizer\n",
    "from tensorflow import keras\n",
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fom_ff20gyy6"
   },
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "import os\n",
    "import re\n",
    "\n",
    "# Load all files from a directory in a DataFrame.\n",
    "def load_directory_data(directory):\n",
    "  data = {}\n",
    "  data[\"sentence\"] = []\n",
    "  data[\"sentiment\"] = []\n",
    "  for file_path in tqdm(os.listdir(directory), desc=os.path.basename(directory)):\n",
    "    with tf.io.gfile.GFile(os.path.join(directory, file_path), \"r\") as f:\n",
    "      data[\"sentence\"].append(f.read())\n",
    "      data[\"sentiment\"].append(re.match(\"\\d+_(\\d+)\\.txt\", file_path).group(1))\n",
    "  return pd.DataFrame.from_dict(data)\n",
    "\n",
    "# Merge positive and negative examples, add a polarity column and shuffle.\n",
    "def load_dataset(directory):\n",
    "    pos_df = load_directory_data(os.path.join(directory, \"pos\"))\n",
    "    neg_df = load_directory_data(os.path.join(directory, \"neg\"))\n",
    "    pos_df[\"polarity\"] = 1\n",
    "    neg_df[\"polarity\"] = 0\n",
    "    return pd.concat([pos_df, neg_df]).sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "# Download and process the dataset files.\n",
    "def download_and_load_datasets(force_download=False):\n",
    "    dataset = tf.keras.utils.get_file(\n",
    "        fname=\"aclImdb.tar.gz\", \n",
    "        origin=\"http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\", \n",
    "        extract=True)\n",
    "\n",
    "    train_df = load_dataset(os.path.join(os.path.dirname(dataset), \n",
    "                           \"aclImdb\", \"train\"))\n",
    "    test_df = load_dataset(os.path.join(os.path.dirname(dataset), \n",
    "                          \"aclImdb\", \"test\"))\n",
    "\n",
    "    return train_df, test_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CaE2G_2DdzVg"
   },
   "source": [
    "Let's use the `MovieReviewData` class below, to prepare/encode \n",
    "the data for feeding into our BERT model, by:\n",
    "  - tokenizing the text\n",
    "  - trim or pad it to a `max_seq_len` length\n",
    "  - append the special tokens `[CLS]` and `[SEP]`\n",
    "  - convert the string tokens to numerical `ID`s using the original model's token encoding from `vocab.txt`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2abfwdn-g135"
   },
   "outputs": [],
   "source": [
    "import bert\n",
    "from bert import BertModelLayer\n",
    "from bert.loader import StockBertConfig, map_stock_config_to_params, load_stock_weights\n",
    "\n",
    "\n",
    "class MovieReviewData:\n",
    "    DATA_COLUMN = \"sentence\"\n",
    "    LABEL_COLUMN = \"polarity\"\n",
    "\n",
    "    def __init__(self, tokenizer: FullTokenizer, sample_size=None, max_seq_len=1024):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.sample_size = sample_size\n",
    "        self.max_seq_len = 0\n",
    "        train, test = download_and_load_datasets()\n",
    "        \n",
    "        train, test = map(lambda df: df.reindex(df[MovieReviewData.DATA_COLUMN].str.len().sort_values().index), \n",
    "                          [train, test])\n",
    "                \n",
    "        if sample_size is not None:\n",
    "            train, test = train.head(sample_size), test.head(sample_size)\n",
    "            # train, test = map(lambda df: df.sample(sample_size), [train, test])\n",
    "        \n",
    "        ((self.train_x, self.train_y),\n",
    "         (self.test_x, self.test_y)) = map(self._prepare, [train, test])\n",
    "\n",
    "        print(\"max seq_len\", self.max_seq_len)\n",
    "        self.max_seq_len = min(self.max_seq_len, max_seq_len)\n",
    "        ((self.train_x, self.train_x_token_types),\n",
    "         (self.test_x, self.test_x_token_types)) = map(self._pad, \n",
    "                                                       [self.train_x, self.test_x])\n",
    "\n",
    "    def _prepare(self, df):\n",
    "        x, y = [], []\n",
    "        with tqdm(total=df.shape[0], unit_scale=True) as pbar:\n",
    "            for ndx, row in df.iterrows():\n",
    "                text, label = row[MovieReviewData.DATA_COLUMN], row[MovieReviewData.LABEL_COLUMN]\n",
    "                tokens = self.tokenizer.tokenize(text)\n",
    "                tokens = [\"[CLS]\"] + tokens + [\"[SEP]\"]\n",
    "                token_ids = self.tokenizer.convert_tokens_to_ids(tokens)\n",
    "                self.max_seq_len = max(self.max_seq_len, len(token_ids))\n",
    "                x.append(token_ids)\n",
    "                y.append(int(label))\n",
    "                pbar.update()\n",
    "        return np.array(x), np.array(y)\n",
    "\n",
    "    def _pad(self, ids):\n",
    "        x, t = [], []\n",
    "        token_type_ids = [0] * self.max_seq_len\n",
    "        for input_ids in ids:\n",
    "            input_ids = input_ids[:min(len(input_ids), self.max_seq_len - 2)]\n",
    "            input_ids = input_ids + [0] * (self.max_seq_len - len(input_ids))\n",
    "            x.append(np.array(input_ids))\n",
    "            t.append(token_type_ids)\n",
    "        return np.array(x), np.array(t)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SGL0mEoNFGlP"
   },
   "source": [
    "## A tweak\n",
    "\n",
    "Because of a `tf.train.load_checkpoint` limitation requiring list permissions on the google storage bucket, we need to copy the pre-trained BERT weights locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lw_F488eixTV"
   },
   "outputs": [],
   "source": [
    "asset_path = 'asset'\n",
    "bert_model_name = \"uncased_L-12_H-768_A-12\"\n",
    "bert_ckpt_dir    = os.path.join(asset_path, bert_model_name)\n",
    "bert_ckpt_file   = os.path.join(bert_ckpt_dir, \"bert_model.ckpt\")\n",
    "bert_config_file = os.path.join(bert_ckpt_dir, \"bert_config.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "if [ ! -f asset/uncased_L-12_H-768_A-12.zip ]; then\n",
    "    curl -o asset/uncased_L-12_H-768_A-12.zip --create-dirs https://storage.googleapis.com/bert_models/2018_10_18/uncased_L-12_H-768_A-12.zip\n",
    "fi\n",
    "if [ ! -d asset/uncased_L-12_H-768_A-12 ]; then\n",
    "    unzip asset/uncased_L-12_H-768_A-12.zip -d asset/\n",
    "fi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "G4xPTleh2X2b"
   },
   "source": [
    "# Preparing the Data\n",
    "\n",
    "Now let's fetch and prepare the data by taking the first `max_seq_len` tokenens after tokenizing with the BERT tokenizer, und use `sample_size` examples for both training and testing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XA8WHJgzhIZf"
   },
   "source": [
    "To keep training fast, we'll take a sample of about 2500 train and test examples, respectively, and use the first 128 tokens only (transformers memory and computation requirements scale quadraticly with the sequence length - so with a TPU you might use `max_seq_len=512`, but on a GPU this would be too slow, and you will have to use a very small `batch_size`s to fit the model into the GPU memory)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 171
    },
    "colab_type": "code",
    "id": "kF_3KhGQ0GTc",
    "outputId": "fd993d23-61ce-4aae-c992-5b5591123198"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pos: 100%|██████████| 12500/12500 [00:00<00:00, 19564.03it/s]\n",
      "neg: 100%|██████████| 12500/12500 [00:00<00:00, 19615.21it/s]\n",
      "pos: 100%|██████████| 12500/12500 [00:00<00:00, 19864.62it/s]\n",
      "neg: 100%|██████████| 12500/12500 [00:00<00:00, 19896.49it/s]\n",
      "100%|██████████| 2.56k/2.56k [00:02<00:00, 897it/s]  \n",
      "100%|██████████| 2.56k/2.56k [00:02<00:00, 922it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max seq_len 178\n",
      "CPU times: user 19 s, sys: 4.6 s, total: 23.6 s\n",
      "Wall time: 23.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "tokenizer = FullTokenizer(vocab_file=os.path.join(bert_ckpt_dir, \"vocab.txt\"))\n",
    "data = MovieReviewData(tokenizer, \n",
    "                       sample_size=10*128*2, #10*128*2\n",
    "                       max_seq_len=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 103
    },
    "colab_type": "code",
    "id": "prRQM8pDi8xI",
    "outputId": "b98433d4-c7e6-4bb5-af54-941f52e0b8e7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            train_x (2560, 128)\n",
      "train_x_token_types (2560, 128)\n",
      "            train_y (2560,)\n",
      "             test_x (2560, 128)\n",
      "        max_seq_len 128\n"
     ]
    }
   ],
   "source": [
    "print(\"            train_x\", data.train_x.shape)\n",
    "print(\"train_x_token_types\", data.train_x_token_types.shape)\n",
    "print(\"            train_y\", data.train_y.shape)\n",
    "\n",
    "print(\"             test_x\", data.test_x.shape)\n",
    "\n",
    "print(\"        max_seq_len\", data.max_seq_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sfRnHSz3iSXz"
   },
   "source": [
    "## Adapter BERT\n",
    "\n",
    "If we decide to use [adapter-BERT](https://arxiv.org/abs/1902.00751) we need some helpers for freezing the original BERT layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IuMOGwFui4it"
   },
   "outputs": [],
   "source": [
    "def flatten_layers(root_layer):\n",
    "    if isinstance(root_layer, keras.layers.Layer):\n",
    "        yield root_layer\n",
    "    for layer in root_layer._layers:\n",
    "        for sub_layer in flatten_layers(layer):\n",
    "            yield sub_layer\n",
    "\n",
    "\n",
    "def freeze_bert_layers(l_bert):\n",
    "    \"\"\"\n",
    "    Freezes all but LayerNorm and adapter layers - see arXiv:1902.00751.\n",
    "    \"\"\"\n",
    "    for layer in flatten_layers(l_bert):\n",
    "        if layer.name in [\"LayerNorm\", \"adapter-down\", \"adapter-up\"]:\n",
    "            layer.trainable = True\n",
    "        elif len(layer._layers) == 0:\n",
    "            layer.trainable = False\n",
    "        l_bert.embeddings_layer.trainable = False\n",
    "\n",
    "\n",
    "def create_learning_rate_scheduler(max_learn_rate=5e-5,\n",
    "                                   end_learn_rate=1e-7,\n",
    "                                   warmup_epoch_count=10,\n",
    "                                   total_epoch_count=90):\n",
    "\n",
    "    def lr_scheduler(epoch):\n",
    "        if epoch < warmup_epoch_count:\n",
    "            res = (max_learn_rate/warmup_epoch_count) * (epoch + 1)\n",
    "        else:\n",
    "            res = max_learn_rate*math.exp(\n",
    "                math.log(end_learn_rate/max_learn_rate)*(epoch-warmup_epoch_count+1)/(total_epoch_count-warmup_epoch_count+1))\n",
    "        return float(res)\n",
    "    learning_rate_scheduler = tf.keras.callbacks.LearningRateScheduler(lr_scheduler, verbose=1)\n",
    "\n",
    "    return learning_rate_scheduler\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ccp5trMwRtmr"
   },
   "source": [
    "# Creating a model\n",
    "\n",
    "Now let's create a classification model using [adapter-BERT](https//arxiv.org/abs/1902.00751), which is clever way of reducing the trainable parameter count, by freezing the original BERT weights, and adapting them with two FFN bottlenecks (i.e. `adapter_size` bellow) in every BERT layer.\n",
    "\n",
    "**N.B.** The commented out code below show how to feed a `token_type_ids`/`segment_ids` sequence (which is not needed in our case)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6o2a5ZIvRcJq"
   },
   "outputs": [],
   "source": [
    "def create_model(max_seq_len, adapter_size=64):\n",
    "  \"\"\"Creates a classification model.\"\"\"\n",
    "\n",
    "  #adapter_size = 64  # see - arXiv:1902.00751\n",
    "\n",
    "  # create the bert layer\n",
    "  with tf.io.gfile.GFile(bert_config_file, \"r\") as reader:\n",
    "      bc = StockBertConfig.from_json_string(reader.read())\n",
    "      bert_params = map_stock_config_to_params(bc)\n",
    "      bert_params.adapter_size = adapter_size\n",
    "      bert = BertModelLayer.from_params(bert_params, name=\"bert\")\n",
    "        \n",
    "  input_ids      = keras.layers.Input(shape=(max_seq_len,), dtype='int32', name=\"input_ids\")\n",
    "  # token_type_ids = keras.layers.Input(shape=(max_seq_len,), dtype='int32', name=\"token_type_ids\")\n",
    "  # output         = bert([input_ids, token_type_ids])\n",
    "  output         = bert(input_ids)\n",
    "\n",
    "  print(\"bert shape\", output.shape)\n",
    "  cls_out = keras.layers.Lambda(lambda seq: seq[:, 0, :])(output)\n",
    "  cls_out = keras.layers.Dropout(0.5)(cls_out)\n",
    "  logits = keras.layers.Dense(units=768, activation=\"tanh\")(cls_out)\n",
    "  logits = keras.layers.Dropout(0.5)(logits)\n",
    "  logits = keras.layers.Dense(units=2, activation=\"softmax\")(logits)\n",
    "\n",
    "  # model = keras.Model(inputs=[input_ids, token_type_ids], outputs=logits)\n",
    "  # model.build(input_shape=[(None, max_seq_len), (None, max_seq_len)])\n",
    "  model = keras.Model(inputs=input_ids, outputs=logits)\n",
    "  model.build(input_shape=(None, max_seq_len))\n",
    "\n",
    "  # load the pre-trained model weights\n",
    "  load_stock_weights(bert, bert_ckpt_file)\n",
    "\n",
    "  # freeze weights if adapter-BERT is used\n",
    "  if adapter_size is not None:\n",
    "      freeze_bert_layers(bert)\n",
    "\n",
    "  model.compile(optimizer=keras.optimizers.Adam(),\n",
    "                loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "                metrics=[keras.metrics.SparseCategoricalAccuracy(name=\"acc\")])\n",
    "\n",
    "  model.summary()\n",
    "        \n",
    "  return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 517
    },
    "colab_type": "code",
    "id": "bZnmtDc7HlEm",
    "outputId": "fcd96c78-792c-4032-d188-73a9c21ec304"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert shape (None, 128, 768)\n",
      "Done loading 196 BERT weights from: asset/uncased_L-12_H-768_A-12/bert_model.ckpt into <bert.model.BertModelLayer object at 0x7f44990ff9e8> (prefix:bert). Count of weights not found in the checkpoint was: [0]. Count of weights with mismatched shape: [0]\n",
      "Unused weights from checkpoint: \n",
      "\tbert/embeddings/token_type_embeddings\n",
      "\tbert/pooler/dense/bias\n",
      "\tbert/pooler/dense/kernel\n",
      "\tcls/predictions/output_bias\n",
      "\tcls/predictions/transform/LayerNorm/beta\n",
      "\tcls/predictions/transform/LayerNorm/gamma\n",
      "\tcls/predictions/transform/dense/bias\n",
      "\tcls/predictions/transform/dense/kernel\n",
      "\tcls/seq_relationship/output_bias\n",
      "\tcls/seq_relationship/output_weights\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_ids (InputLayer)       [(None, 128)]             0         \n",
      "_________________________________________________________________\n",
      "bert (BertModelLayer)        (None, 128, 768)          108890112 \n",
      "_________________________________________________________________\n",
      "lambda (Lambda)              (None, 768)               0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 768)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 768)               590592    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 768)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2)                 1538      \n",
      "=================================================================\n",
      "Total params: 109,482,242\n",
      "Trainable params: 109,482,242\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "adapter_size = None # use None to fine-tune all of BERT\n",
    "model = create_model(data.max_seq_len, adapter_size=adapter_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "ZuLOkwonF-9S",
    "outputId": "ce1451d4-310c-41f1-ddd3-a2e0841d8528"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2304 samples, validate on 256 samples\n",
      "\n",
      "Epoch 00001: LearningRateScheduler reducing learning rate to 5.000000000000001e-07.\n",
      "Epoch 1/20\n",
      "2304/2304 [==============================] - 130s 56ms/sample - loss: 0.7038 - acc: 0.5556 - val_loss: 0.6826 - val_acc: 0.5273\n",
      "\n",
      "Epoch 00002: LearningRateScheduler reducing learning rate to 1.0000000000000002e-06.\n",
      "Epoch 2/20\n",
      "2304/2304 [==============================] - 124s 54ms/sample - loss: 0.6928 - acc: 0.5499 - val_loss: 0.6581 - val_acc: 0.5781\n",
      "\n",
      "Epoch 00003: LearningRateScheduler reducing learning rate to 1.5000000000000002e-06.\n",
      "Epoch 3/20\n",
      "2304/2304 [==============================] - 124s 54ms/sample - loss: 0.6723 - acc: 0.5864 - val_loss: 0.6119 - val_acc: 0.7344\n",
      "\n",
      "Epoch 00004: LearningRateScheduler reducing learning rate to 2.0000000000000003e-06.\n",
      "Epoch 4/20\n",
      "2304/2304 [==============================] - 122s 53ms/sample - loss: 0.5676 - acc: 0.7296 - val_loss: 0.4608 - val_acc: 0.8555\n",
      "\n",
      "Epoch 00005: LearningRateScheduler reducing learning rate to 2.5000000000000006e-06.\n",
      "Epoch 5/20\n",
      "2304/2304 [==============================] - 124s 54ms/sample - loss: 0.4319 - acc: 0.8837 - val_loss: 0.4156 - val_acc: 0.9062\n",
      "\n",
      "Epoch 00006: LearningRateScheduler reducing learning rate to 3.0000000000000005e-06.\n",
      "Epoch 6/20\n",
      "2304/2304 [==============================] - 127s 55ms/sample - loss: 0.3982 - acc: 0.9158 - val_loss: 0.4173 - val_acc: 0.8945\n",
      "\n",
      "Epoch 00007: LearningRateScheduler reducing learning rate to 3.5000000000000004e-06.\n",
      "Epoch 7/20\n",
      "2304/2304 [==============================] - 127s 55ms/sample - loss: 0.3786 - acc: 0.9358 - val_loss: 0.4217 - val_acc: 0.8867\n",
      "\n",
      "Epoch 00008: LearningRateScheduler reducing learning rate to 4.000000000000001e-06.\n",
      "Epoch 8/20\n",
      "2304/2304 [==============================] - 128s 55ms/sample - loss: 0.3640 - acc: 0.9488 - val_loss: 0.4215 - val_acc: 0.8906\n",
      "\n",
      "Epoch 00009: LearningRateScheduler reducing learning rate to 4.500000000000001e-06.\n",
      "Epoch 9/20\n",
      "2304/2304 [==============================] - 128s 55ms/sample - loss: 0.3548 - acc: 0.9583 - val_loss: 0.4161 - val_acc: 0.8945\n",
      "\n",
      "Epoch 00010: LearningRateScheduler reducing learning rate to 5.000000000000001e-06.\n",
      "Epoch 10/20\n",
      "2304/2304 [==============================] - 127s 55ms/sample - loss: 0.3493 - acc: 0.9653 - val_loss: 0.4306 - val_acc: 0.8789\n",
      "\n",
      "Epoch 00011: LearningRateScheduler reducing learning rate to 5.500000000000001e-06.\n",
      "Epoch 11/20\n",
      "2304/2304 [==============================] - 127s 55ms/sample - loss: 0.3485 - acc: 0.9631 - val_loss: 0.4167 - val_acc: 0.8945\n",
      "\n",
      "Epoch 00012: LearningRateScheduler reducing learning rate to 6.000000000000001e-06.\n",
      "Epoch 12/20\n",
      "2304/2304 [==============================] - 126s 55ms/sample - loss: 0.3458 - acc: 0.9674 - val_loss: 0.4385 - val_acc: 0.8672\n",
      "\n",
      "Epoch 00013: LearningRateScheduler reducing learning rate to 6.500000000000001e-06.\n",
      "Epoch 13/20\n",
      "2304/2304 [==============================] - 126s 55ms/sample - loss: 0.3438 - acc: 0.9701 - val_loss: 0.4144 - val_acc: 0.8906\n",
      "\n",
      "Epoch 00014: LearningRateScheduler reducing learning rate to 7.000000000000001e-06.\n",
      "Epoch 14/20\n",
      "2304/2304 [==============================] - 126s 55ms/sample - loss: 0.3440 - acc: 0.9683 - val_loss: 0.4520 - val_acc: 0.8594\n",
      "\n",
      "Epoch 00015: LearningRateScheduler reducing learning rate to 7.500000000000001e-06.\n",
      "Epoch 15/20\n",
      "2304/2304 [==============================] - 128s 56ms/sample - loss: 0.3446 - acc: 0.9688 - val_loss: 0.4258 - val_acc: 0.8867\n",
      "\n",
      "Epoch 00016: LearningRateScheduler reducing learning rate to 8.000000000000001e-06.\n",
      "Epoch 16/20\n",
      "2304/2304 [==============================] - 130s 56ms/sample - loss: 0.3451 - acc: 0.9679 - val_loss: 0.4414 - val_acc: 0.8711\n",
      "\n",
      "Epoch 00017: LearningRateScheduler reducing learning rate to 8.500000000000002e-06.\n",
      "Epoch 17/20\n",
      "2304/2304 [==============================] - 128s 56ms/sample - loss: 0.3388 - acc: 0.9748 - val_loss: 0.4401 - val_acc: 0.8711\n",
      "\n",
      "Epoch 00018: LearningRateScheduler reducing learning rate to 9.000000000000002e-06.\n",
      "Epoch 18/20\n",
      "2304/2304 [==============================] - 129s 56ms/sample - loss: 0.3370 - acc: 0.9761 - val_loss: 0.4362 - val_acc: 0.8711\n",
      "\n",
      "Epoch 00019: LearningRateScheduler reducing learning rate to 9.500000000000002e-06.\n",
      "Epoch 19/20\n",
      "2304/2304 [==============================] - 128s 56ms/sample - loss: 0.3394 - acc: 0.9735 - val_loss: 0.4440 - val_acc: 0.8672\n",
      "\n",
      "Epoch 00020: LearningRateScheduler reducing learning rate to 1.0000000000000003e-05.\n",
      "Epoch 20/20\n",
      "2304/2304 [==============================] - 127s 55ms/sample - loss: 0.3408 - acc: 0.9727 - val_loss: 0.4424 - val_acc: 0.8711\n",
      "CPU times: user 41min 17s, sys: 14min 34s, total: 55min 52s\n",
      "Wall time: 42min 15s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f43e31a13c8>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "log_dir = \".log/movie_reviews/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%s\")\n",
    "tensorboard_callback = keras.callbacks.TensorBoard(log_dir=log_dir)\n",
    "\n",
    "total_epoch_count = 20\n",
    "# model.fit(x=(data.train_x, data.train_x_token_types), y=data.train_y,\n",
    "model.fit(x=data.train_x, y=data.train_y,\n",
    "          validation_split=0.1,\n",
    "          batch_size=12,\n",
    "          shuffle=True,\n",
    "          epochs=total_epoch_count,\n",
    "          callbacks=[create_learning_rate_scheduler(max_learn_rate=1e-5,\n",
    "                                                    end_learn_rate=1e-7,\n",
    "                                                    warmup_epoch_count=20,\n",
    "                                                    total_epoch_count=total_epoch_count),\n",
    "                     keras.callbacks.EarlyStopping(patience=20, restore_best_weights=True),\n",
    "                     tensorboard_callback])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights('./movie_reviews.h5', overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 120
    },
    "colab_type": "code",
    "id": "BSqMu64oHzqy",
    "outputId": "95a8284b-b3b2-4a7d-f335-c4ff65f8f920"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2560/2560 [==============================] - 34s 13ms/sample - loss: 0.3419 - acc: 0.9711\n",
      "2560/2560 [==============================] - 33s 13ms/sample - loss: 0.3903 - acc: 0.9230\n",
      "train acc 0.9710938\n",
      " test acc 0.9230469\n",
      "CPU times: user 1min 6s, sys: 194 ms, total: 1min 7s\n",
      "Wall time: 1min 6s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "_, train_acc = model.evaluate(data.train_x, data.train_y)\n",
    "_, test_acc = model.evaluate(data.test_x, data.test_y)\n",
    "\n",
    "print(\"train acc\", train_acc)\n",
    "print(\" test acc\", test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xSKDZEnVabnl"
   },
   "source": [
    "# Evaluation\n",
    "\n",
    "To evaluate the trained model, let's load the saved weights in a new model instance, and evaluate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert shape (None, 128, 768)\n",
      "Done loading 196 BERT weights from: asset/uncased_L-12_H-768_A-12/bert_model.ckpt into <bert.model.BertModelLayer object at 0x7ff6b45c4160> (prefix:bert). Count of weights not found in the checkpoint was: [0]. Count of weights with mismatched shape: [0]\n",
      "Unused weights from checkpoint: \n",
      "\tbert/embeddings/token_type_embeddings\n",
      "\tbert/pooler/dense/bias\n",
      "\tbert/pooler/dense/kernel\n",
      "\tcls/predictions/output_bias\n",
      "\tcls/predictions/transform/LayerNorm/beta\n",
      "\tcls/predictions/transform/LayerNorm/gamma\n",
      "\tcls/predictions/transform/dense/bias\n",
      "\tcls/predictions/transform/dense/kernel\n",
      "\tcls/seq_relationship/output_bias\n",
      "\tcls/seq_relationship/output_weights\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_ids (InputLayer)       [(None, 128)]             0         \n",
      "_________________________________________________________________\n",
      "bert (BertModelLayer)        (None, 128, 768)          108890112 \n",
      "_________________________________________________________________\n",
      "lambda (Lambda)              (None, 768)               0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 768)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 768)               590592    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 768)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2)                 1538      \n",
      "=================================================================\n",
      "Total params: 109,482,242\n",
      "Trainable params: 109,482,242\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = create_model(data.max_seq_len, adapter_size=None)\n",
    "model.load_weights(\"./movie_reviews.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 531
    },
    "colab_type": "code",
    "id": "qCpabQ15WS3U",
    "outputId": "220b2b55-dd78-4795-d3b6-d364c8323734"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2560/2560 [==============================] - 35s 14ms/sample - loss: 0.3903 - acc: 0.9230\n",
      " test acc 0.9230469\n",
      "CPU times: user 34.6 s, sys: 113 ms, total: 34.7 s\n",
      "Wall time: 34.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "# _, train_acc = model.evaluate(data.train_x, data.train_y)\n",
    "_, test_acc = model.evaluate(data.test_x, data.test_y)\n",
    "\n",
    "# print(\"train acc\", train_acc)\n",
    "print(\" test acc\", test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5uzdOFQ5awM1"
   },
   "source": [
    "# Prediction\n",
    "\n",
    "For prediction, we need to prepare the input text the same way as we did for training - tokenize, adding the special `[CLS]` and `[SEP]` token at begin and end of the token sequence, and pad to match the model input shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 322 ms, sys: 36 ms, total: 358 ms\n",
      "Wall time: 360 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['negative', 'negative', 'positive', 'positive']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "CLASSES = [\"negative\",\"positive\"]\n",
    "max_seq_len = 128\n",
    "pred_sentences = [\n",
    "  \"That movie was absolutely awful\",\n",
    "  \"The acting was a bit lacking\",\n",
    "  \"The film was creative and surprising\",\n",
    "  \"Absolutely fantastic!\",\n",
    "]\n",
    "\n",
    "inputs = pd.DataFrame(pred_sentences)\n",
    "\n",
    "pred_tokens    = map(tokenizer.tokenize, inputs.to_numpy()[:, 0].tolist())\n",
    "pred_tokens    = map(lambda tok: [\"[CLS]\"] + tok + [\"[SEP]\"], pred_tokens)\n",
    "pred_token_ids = list(map(tokenizer.convert_tokens_to_ids, pred_tokens))\n",
    "pred_token_ids = map(lambda tids: tids + [0] * (max_seq_len-len(tids)), pred_token_ids)\n",
    "pred_token_ids = np.array(list(pred_token_ids))\n",
    "\n",
    "res = model(pred_token_ids).numpy().argmax(axis=-1)\n",
    "[CLASSES[i] for i in res]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build & Save bentoml service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting bentoml_service.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile bentoml_service.py\n",
    "\n",
    "import bentoml\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from bentoml.artifact import (\n",
    "    TensorflowSavedModelArtifact, PickleArtifact,\n",
    ")\n",
    "from bentoml.handlers import DataframeHandler, ClipperStringsHandler\n",
    "\n",
    "\n",
    "CLASSES = [\"negative\",\"positive\"]\n",
    "max_seq_len = 128\n",
    "\n",
    "try:\n",
    "    tf.config.set_visible_devices([], 'GPU')  # disable GPU, required when served in docker\n",
    "except:\n",
    "    pass\n",
    "\n",
    "\n",
    "@bentoml.env(pip_dependencies=['tensorflow', 'bert-for-tf2'])\n",
    "@bentoml.artifacts([TensorflowSavedModelArtifact('model'), PickleArtifact('tokenizer')])\n",
    "class Service(bentoml.BentoService):\n",
    "\n",
    "    def tokenize(self, inputs: pd.DataFrame):\n",
    "        tokenizer = self.artifacts.tokenizer\n",
    "        if isinstance(inputs, pd.DataFrame):\n",
    "            inputs = inputs.to_numpy()[:, 0].tolist()\n",
    "        else:\n",
    "            inputs = inputs.tolist()  # for predict_clipper\n",
    "        pred_tokens = map(tokenizer.tokenize, inputs)\n",
    "        pred_tokens = map(lambda tok: [\"[CLS]\"] + tok + [\"[SEP]\"], pred_tokens)\n",
    "        pred_token_ids = list(map(tokenizer.convert_tokens_to_ids, pred_tokens))\n",
    "        pred_token_ids = map(lambda tids: tids + [0] * (max_seq_len - len(tids)), pred_token_ids)\n",
    "        pred_token_ids = tf.constant(list(pred_token_ids), dtype=tf.int32)\n",
    "        return pred_token_ids\n",
    "\n",
    "    @bentoml.api(DataframeHandler, mb_max_latency=300, mb_max_batch_size=20)\n",
    "    def predict(self, inputs):\n",
    "        model = self.artifacts.model\n",
    "        pred_token_ids = self.tokenize(inputs)\n",
    "        res = model(pred_token_ids).numpy().argmax(axis=-1)\n",
    "        return [CLASSES[i] for i in res]\n",
    "\n",
    "    @bentoml.api(ClipperStringsHandler)\n",
    "    def predict_clipper(self, strings):\n",
    "        model = self.artifacts.model\n",
    "        pred_token_ids = self.tokenize(strings)\n",
    "        res = model(pred_token_ids).numpy().argmax(axis=-1)\n",
    "        return [CLASSES[i] for i in res]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2020-04-23 23:37:54,466] WARNING - BentoML local changes detected - Local BentoML repository including all code changes will be bundled together with the BentoService bundle. When used with docker, the base docker image will be default to same version as last PyPI release at version: 0.5.2. You can also force bentoml to use a specific version for deploying your BentoService bundle, by setting the config 'core/bentoml_deploy_version' to a pinned version or your custom BentoML on github, e.g.:'bentoml_deploy_version = git+https://github.com/{username}/bentoml.git@{branch}'\n",
      "[2020-04-23 23:37:54,814] WARNING - BentoML local changes detected - Local BentoML repository including all code changes will be bundled together with the BentoService bundle. When used with docker, the base docker image will be default to same version as last PyPI release at version: 0.5.2. You can also force bentoml to use a specific version for deploying your BentoService bundle, by setting the config 'core/bentoml_deploy_version' to a pinned version or your custom BentoML on github, e.g.:'bentoml_deploy_version = git+https://github.com/{username}/bentoml.git@{branch}'\n",
      "WARNING:tensorflow:Skipping full serialization of Keras layer <tensorflow.python.keras.layers.embeddings.Embedding object at 0x7ff62d9c9ef0>, because it is not built.\n",
      "WARNING:tensorflow:From /opt/anaconda3/envs/bentoml-dev-py36/lib/python3.6/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1786: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "INFO:tensorflow:Assets written to: /tmp/bentoml-temp-whqt7e5s/Service/artifacts/model_saved_model/assets\n",
      "[2020-04-23 23:39:01,739] WARNING - BentoML local changes detected - Local BentoML repository including all code changes will be bundled together with the BentoService bundle. When used with docker, the base docker image will be default to same version as last PyPI release at version: 0.5.2. You can also force bentoml to use a specific version for deploying your BentoService bundle, by setting the config 'core/bentoml_deploy_version' to a pinned version or your custom BentoML on github, e.g.:'bentoml_deploy_version = git+https://github.com/{username}/bentoml.git@{branch}'\n",
      "running sdist\n",
      "running egg_info\n",
      "writing BentoML.egg-info/PKG-INFO\n",
      "writing dependency_links to BentoML.egg-info/dependency_links.txt\n",
      "writing entry points to BentoML.egg-info/entry_points.txt\n",
      "writing requirements to BentoML.egg-info/requires.txt\n",
      "writing top-level names to BentoML.egg-info/top_level.txt\n",
      "reading manifest file 'BentoML.egg-info/SOURCES.txt'\n",
      "reading manifest template 'MANIFEST.in'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "warning: no previously-included files matching '*~' found anywhere in distribution\n",
      "warning: no previously-included files matching '*.pyo' found anywhere in distribution\n",
      "warning: no previously-included files matching '.git' found anywhere in distribution\n",
      "warning: no previously-included files matching '.ipynb_checkpoints' found anywhere in distribution\n",
      "warning: no previously-included files matching '__pycache__' found anywhere in distribution\n",
      "warning: no directories found matching 'bentoml/yatai/web/dist'\n",
      "no previously-included directories found matching 'e2e_tests'\n",
      "no previously-included directories found matching 'tests'\n",
      "no previously-included directories found matching 'benchmark'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "writing manifest file 'BentoML.egg-info/SOURCES.txt'\n",
      "running check\n",
      "creating BentoML-0.5.2+173.ga4278d4.dirty\n",
      "creating BentoML-0.5.2+173.ga4278d4.dirty/BentoML.egg-info\n",
      "creating BentoML-0.5.2+173.ga4278d4.dirty/bentoml\n",
      "creating BentoML-0.5.2+173.ga4278d4.dirty/bentoml/artifact\n",
      "creating BentoML-0.5.2+173.ga4278d4.dirty/bentoml/bundler\n",
      "creating BentoML-0.5.2+173.ga4278d4.dirty/bentoml/cli\n",
      "creating BentoML-0.5.2+173.ga4278d4.dirty/bentoml/clipper\n",
      "creating BentoML-0.5.2+173.ga4278d4.dirty/bentoml/configuration\n",
      "creating BentoML-0.5.2+173.ga4278d4.dirty/bentoml/configuration/__pycache__\n",
      "creating BentoML-0.5.2+173.ga4278d4.dirty/bentoml/deployment\n",
      "creating BentoML-0.5.2+173.ga4278d4.dirty/bentoml/deployment/aws_lambda\n",
      "creating BentoML-0.5.2+173.ga4278d4.dirty/bentoml/deployment/sagemaker\n",
      "creating BentoML-0.5.2+173.ga4278d4.dirty/bentoml/handlers\n",
      "creating BentoML-0.5.2+173.ga4278d4.dirty/bentoml/marshal\n",
      "creating BentoML-0.5.2+173.ga4278d4.dirty/bentoml/migrations\n",
      "creating BentoML-0.5.2+173.ga4278d4.dirty/bentoml/migrations/__pycache__\n",
      "creating BentoML-0.5.2+173.ga4278d4.dirty/bentoml/migrations/versions\n",
      "creating BentoML-0.5.2+173.ga4278d4.dirty/bentoml/migrations/versions/__pycache__\n",
      "creating BentoML-0.5.2+173.ga4278d4.dirty/bentoml/proto\n",
      "creating BentoML-0.5.2+173.ga4278d4.dirty/bentoml/repository\n",
      "creating BentoML-0.5.2+173.ga4278d4.dirty/bentoml/server\n",
      "creating BentoML-0.5.2+173.ga4278d4.dirty/bentoml/server/static\n",
      "creating BentoML-0.5.2+173.ga4278d4.dirty/bentoml/utils\n",
      "creating BentoML-0.5.2+173.ga4278d4.dirty/bentoml/utils/validator\n",
      "creating BentoML-0.5.2+173.ga4278d4.dirty/bentoml/yatai\n",
      "creating BentoML-0.5.2+173.ga4278d4.dirty/bentoml/yatai/client\n",
      "copying files to BentoML-0.5.2+173.ga4278d4.dirty...\n",
      "copying LICENSE -> BentoML-0.5.2+173.ga4278d4.dirty\n",
      "copying MANIFEST.in -> BentoML-0.5.2+173.ga4278d4.dirty\n",
      "copying README.md -> BentoML-0.5.2+173.ga4278d4.dirty\n",
      "copying pyproject.toml -> BentoML-0.5.2+173.ga4278d4.dirty\n",
      "copying setup.cfg -> BentoML-0.5.2+173.ga4278d4.dirty\n",
      "copying setup.py -> BentoML-0.5.2+173.ga4278d4.dirty\n",
      "copying versioneer.py -> BentoML-0.5.2+173.ga4278d4.dirty\n",
      "copying BentoML.egg-info/PKG-INFO -> BentoML-0.5.2+173.ga4278d4.dirty/BentoML.egg-info\n",
      "copying BentoML.egg-info/SOURCES.txt -> BentoML-0.5.2+173.ga4278d4.dirty/BentoML.egg-info\n",
      "copying BentoML.egg-info/dependency_links.txt -> BentoML-0.5.2+173.ga4278d4.dirty/BentoML.egg-info\n",
      "copying BentoML.egg-info/entry_points.txt -> BentoML-0.5.2+173.ga4278d4.dirty/BentoML.egg-info\n",
      "copying BentoML.egg-info/requires.txt -> BentoML-0.5.2+173.ga4278d4.dirty/BentoML.egg-info\n",
      "copying BentoML.egg-info/top_level.txt -> BentoML-0.5.2+173.ga4278d4.dirty/BentoML.egg-info\n",
      "copying bentoml/__init__.py -> BentoML-0.5.2+173.ga4278d4.dirty/bentoml\n",
      "copying bentoml/_version.py -> BentoML-0.5.2+173.ga4278d4.dirty/bentoml\n",
      "copying bentoml/alembic.ini -> BentoML-0.5.2+173.ga4278d4.dirty/bentoml\n",
      "copying bentoml/db.py -> BentoML-0.5.2+173.ga4278d4.dirty/bentoml\n",
      "copying bentoml/exceptions.py -> BentoML-0.5.2+173.ga4278d4.dirty/bentoml\n",
      "copying bentoml/service.py -> BentoML-0.5.2+173.ga4278d4.dirty/bentoml\n",
      "copying bentoml/service_env.py -> BentoML-0.5.2+173.ga4278d4.dirty/bentoml\n",
      "copying bentoml/artifact/__init__.py -> BentoML-0.5.2+173.ga4278d4.dirty/bentoml/artifact\n",
      "copying bentoml/artifact/artifact.py -> BentoML-0.5.2+173.ga4278d4.dirty/bentoml/artifact\n",
      "copying bentoml/artifact/fastai_model_artifact.py -> BentoML-0.5.2+173.ga4278d4.dirty/bentoml/artifact\n",
      "copying bentoml/artifact/fasttext_model_artifact.py -> BentoML-0.5.2+173.ga4278d4.dirty/bentoml/artifact\n",
      "copying bentoml/artifact/h2o_model_artifact.py -> BentoML-0.5.2+173.ga4278d4.dirty/bentoml/artifact\n",
      "copying bentoml/artifact/keras_model_artifact.py -> BentoML-0.5.2+173.ga4278d4.dirty/bentoml/artifact\n",
      "copying bentoml/artifact/lightgbm_model_artifact.py -> BentoML-0.5.2+173.ga4278d4.dirty/bentoml/artifact\n",
      "copying bentoml/artifact/pickle_artifact.py -> BentoML-0.5.2+173.ga4278d4.dirty/bentoml/artifact\n",
      "copying bentoml/artifact/pytorch_model_artifact.py -> BentoML-0.5.2+173.ga4278d4.dirty/bentoml/artifact\n",
      "copying bentoml/artifact/sklearn_model_artifact.py -> BentoML-0.5.2+173.ga4278d4.dirty/bentoml/artifact\n",
      "copying bentoml/artifact/text_file_artifact.py -> BentoML-0.5.2+173.ga4278d4.dirty/bentoml/artifact\n",
      "copying bentoml/artifact/tf_savedmodel_artifact.py -> BentoML-0.5.2+173.ga4278d4.dirty/bentoml/artifact\n",
      "copying bentoml/artifact/xgboost_model_artifact.py -> BentoML-0.5.2+173.ga4278d4.dirty/bentoml/artifact\n",
      "copying bentoml/bundler/__init__.py -> BentoML-0.5.2+173.ga4278d4.dirty/bentoml/bundler\n",
      "copying bentoml/bundler/bundler.py -> BentoML-0.5.2+173.ga4278d4.dirty/bentoml/bundler\n",
      "copying bentoml/bundler/config.py -> BentoML-0.5.2+173.ga4278d4.dirty/bentoml/bundler\n",
      "copying bentoml/bundler/loader.py -> BentoML-0.5.2+173.ga4278d4.dirty/bentoml/bundler\n",
      "copying bentoml/bundler/py_module_utils.py -> BentoML-0.5.2+173.ga4278d4.dirty/bentoml/bundler\n",
      "copying bentoml/bundler/templates.py -> BentoML-0.5.2+173.ga4278d4.dirty/bentoml/bundler\n",
      "copying bentoml/bundler/utils.py -> BentoML-0.5.2+173.ga4278d4.dirty/bentoml/bundler\n",
      "copying bentoml/cli/__init__.py -> BentoML-0.5.2+173.ga4278d4.dirty/bentoml/cli\n",
      "copying bentoml/cli/aws_lambda.py -> BentoML-0.5.2+173.ga4278d4.dirty/bentoml/cli\n",
      "copying bentoml/cli/aws_sagemaker.py -> BentoML-0.5.2+173.ga4278d4.dirty/bentoml/cli\n",
      "copying bentoml/cli/bento.py -> BentoML-0.5.2+173.ga4278d4.dirty/bentoml/cli\n",
      "copying bentoml/cli/click_utils.py -> BentoML-0.5.2+173.ga4278d4.dirty/bentoml/cli\n",
      "copying bentoml/cli/config.py -> BentoML-0.5.2+173.ga4278d4.dirty/bentoml/cli\n",
      "copying bentoml/cli/deployment.py -> BentoML-0.5.2+173.ga4278d4.dirty/bentoml/cli\n",
      "copying bentoml/cli/utils.py -> BentoML-0.5.2+173.ga4278d4.dirty/bentoml/cli\n",
      "copying bentoml/cli/yatai_service.py -> BentoML-0.5.2+173.ga4278d4.dirty/bentoml/cli\n",
      "copying bentoml/clipper/__init__.py -> BentoML-0.5.2+173.ga4278d4.dirty/bentoml/clipper\n",
      "copying bentoml/configuration/__init__.py -> BentoML-0.5.2+173.ga4278d4.dirty/bentoml/configuration\n",
      "copying bentoml/configuration/configparser.py -> BentoML-0.5.2+173.ga4278d4.dirty/bentoml/configuration\n",
      "copying bentoml/configuration/default_bentoml.cfg -> BentoML-0.5.2+173.ga4278d4.dirty/bentoml/configuration\n",
      "copying bentoml/configuration/__pycache__/__init__.cpython-36.pyc -> BentoML-0.5.2+173.ga4278d4.dirty/bentoml/configuration/__pycache__\n",
      "copying bentoml/configuration/__pycache__/__init__.cpython-37.pyc -> BentoML-0.5.2+173.ga4278d4.dirty/bentoml/configuration/__pycache__\n",
      "copying bentoml/configuration/__pycache__/__init__.cpython-38.pyc -> BentoML-0.5.2+173.ga4278d4.dirty/bentoml/configuration/__pycache__\n",
      "copying bentoml/configuration/__pycache__/configparser.cpython-36.pyc -> BentoML-0.5.2+173.ga4278d4.dirty/bentoml/configuration/__pycache__\n",
      "copying bentoml/configuration/__pycache__/configparser.cpython-37.pyc -> BentoML-0.5.2+173.ga4278d4.dirty/bentoml/configuration/__pycache__\n",
      "copying bentoml/configuration/__pycache__/configparser.cpython-38.pyc -> BentoML-0.5.2+173.ga4278d4.dirty/bentoml/configuration/__pycache__\n",
      "copying bentoml/deployment/__init__.py -> BentoML-0.5.2+173.ga4278d4.dirty/bentoml/deployment\n",
      "copying bentoml/deployment/operator.py -> BentoML-0.5.2+173.ga4278d4.dirty/bentoml/deployment\n",
      "copying bentoml/deployment/store.py -> BentoML-0.5.2+173.ga4278d4.dirty/bentoml/deployment\n",
      "copying bentoml/deployment/utils.py -> BentoML-0.5.2+173.ga4278d4.dirty/bentoml/deployment\n",
      "copying bentoml/deployment/aws_lambda/__init__.py -> BentoML-0.5.2+173.ga4278d4.dirty/bentoml/deployment/aws_lambda\n",
      "copying bentoml/deployment/aws_lambda/download_extra_resources.py -> BentoML-0.5.2+173.ga4278d4.dirty/bentoml/deployment/aws_lambda\n",
      "copying bentoml/deployment/aws_lambda/lambda_app.py -> BentoML-0.5.2+173.ga4278d4.dirty/bentoml/deployment/aws_lambda\n",
      "copying bentoml/deployment/aws_lambda/utils.py -> BentoML-0.5.2+173.ga4278d4.dirty/bentoml/deployment/aws_lambda\n",
      "copying bentoml/deployment/sagemaker/__init__.py -> BentoML-0.5.2+173.ga4278d4.dirty/bentoml/deployment/sagemaker\n",
      "copying bentoml/deployment/sagemaker/sagemaker_nginx.conf -> BentoML-0.5.2+173.ga4278d4.dirty/bentoml/deployment/sagemaker\n",
      "copying bentoml/deployment/sagemaker/sagemaker_serve.py -> BentoML-0.5.2+173.ga4278d4.dirty/bentoml/deployment/sagemaker\n",
      "copying bentoml/deployment/sagemaker/sagemaker_wsgi.py -> BentoML-0.5.2+173.ga4278d4.dirty/bentoml/deployment/sagemaker\n",
      "copying bentoml/handlers/__init__.py -> BentoML-0.5.2+173.ga4278d4.dirty/bentoml/handlers\n",
      "copying bentoml/handlers/base_handlers.py -> BentoML-0.5.2+173.ga4278d4.dirty/bentoml/handlers\n",
      "copying bentoml/handlers/clipper_handler.py -> BentoML-0.5.2+173.ga4278d4.dirty/bentoml/handlers\n",
      "copying bentoml/handlers/dataframe_handler.py -> BentoML-0.5.2+173.ga4278d4.dirty/bentoml/handlers\n",
      "copying bentoml/handlers/fastai_image_handler.py -> BentoML-0.5.2+173.ga4278d4.dirty/bentoml/handlers\n",
      "copying bentoml/handlers/image_handler.py -> BentoML-0.5.2+173.ga4278d4.dirty/bentoml/handlers\n",
      "copying bentoml/handlers/json_handler.py -> BentoML-0.5.2+173.ga4278d4.dirty/bentoml/handlers\n",
      "copying bentoml/handlers/pytorch_tensor_handler.py -> BentoML-0.5.2+173.ga4278d4.dirty/bentoml/handlers\n",
      "copying bentoml/handlers/tensorflow_tensor_handler.py -> BentoML-0.5.2+173.ga4278d4.dirty/bentoml/handlers\n",
      "copying bentoml/handlers/utils.py -> BentoML-0.5.2+173.ga4278d4.dirty/bentoml/handlers\n",
      "copying bentoml/marshal/__init__.py -> BentoML-0.5.2+173.ga4278d4.dirty/bentoml/marshal\n",
      "copying bentoml/marshal/dispatcher.py -> BentoML-0.5.2+173.ga4278d4.dirty/bentoml/marshal\n",
      "copying bentoml/marshal/marshal.py -> BentoML-0.5.2+173.ga4278d4.dirty/bentoml/marshal\n",
      "copying bentoml/marshal/utils.py -> BentoML-0.5.2+173.ga4278d4.dirty/bentoml/marshal\n",
      "copying bentoml/migrations/README -> BentoML-0.5.2+173.ga4278d4.dirty/bentoml/migrations\n",
      "copying bentoml/migrations/env.py -> BentoML-0.5.2+173.ga4278d4.dirty/bentoml/migrations\n",
      "copying bentoml/migrations/script.py.mako -> BentoML-0.5.2+173.ga4278d4.dirty/bentoml/migrations\n",
      "copying bentoml/migrations/__pycache__/env.cpython-36.pyc -> BentoML-0.5.2+173.ga4278d4.dirty/bentoml/migrations/__pycache__\n",
      "copying bentoml/migrations/__pycache__/env.cpython-37.pyc -> BentoML-0.5.2+173.ga4278d4.dirty/bentoml/migrations/__pycache__\n",
      "copying bentoml/migrations/versions/a6b00ae45279_add_last_updated_at_for_deployments.py -> BentoML-0.5.2+173.ga4278d4.dirty/bentoml/migrations/versions\n",
      "copying bentoml/migrations/versions/__pycache__/a6b00ae45279_add_last_updated_at_for_deployments.cpython-36.pyc -> BentoML-0.5.2+173.ga4278d4.dirty/bentoml/migrations/versions/__pycache__\n",
      "copying bentoml/migrations/versions/__pycache__/a6b00ae45279_add_last_updated_at_for_deployments.cpython-37.pyc -> BentoML-0.5.2+173.ga4278d4.dirty/bentoml/migrations/versions/__pycache__\n",
      "copying bentoml/proto/__init__.py -> BentoML-0.5.2+173.ga4278d4.dirty/bentoml/proto\n",
      "copying bentoml/proto/deployment_pb2.py -> BentoML-0.5.2+173.ga4278d4.dirty/bentoml/proto\n",
      "copying bentoml/proto/repository_pb2.py -> BentoML-0.5.2+173.ga4278d4.dirty/bentoml/proto\n",
      "copying bentoml/proto/status_pb2.py -> BentoML-0.5.2+173.ga4278d4.dirty/bentoml/proto\n",
      "copying bentoml/proto/yatai_service_pb2.py -> BentoML-0.5.2+173.ga4278d4.dirty/bentoml/proto\n",
      "copying bentoml/proto/yatai_service_pb2_grpc.py -> BentoML-0.5.2+173.ga4278d4.dirty/bentoml/proto\n",
      "copying bentoml/repository/__init__.py -> BentoML-0.5.2+173.ga4278d4.dirty/bentoml/repository\n",
      "copying bentoml/repository/metadata_store.py -> BentoML-0.5.2+173.ga4278d4.dirty/bentoml/repository\n",
      "copying bentoml/server/__init__.py -> BentoML-0.5.2+173.ga4278d4.dirty/bentoml/server\n",
      "copying bentoml/server/bento_api_server.py -> BentoML-0.5.2+173.ga4278d4.dirty/bentoml/server\n",
      "copying bentoml/server/bento_sagemaker_server.py -> BentoML-0.5.2+173.ga4278d4.dirty/bentoml/server\n",
      "copying bentoml/server/gunicorn_config.py -> BentoML-0.5.2+173.ga4278d4.dirty/bentoml/server\n",
      "copying bentoml/server/gunicorn_server.py -> BentoML-0.5.2+173.ga4278d4.dirty/bentoml/server\n",
      "copying bentoml/server/marshal_server.py -> BentoML-0.5.2+173.ga4278d4.dirty/bentoml/server\n",
      "copying bentoml/server/middlewares.py -> BentoML-0.5.2+173.ga4278d4.dirty/bentoml/server\n",
      "copying bentoml/server/utils.py -> BentoML-0.5.2+173.ga4278d4.dirty/bentoml/server\n",
      "copying bentoml/server/static/swagger-ui-bundle.js -> BentoML-0.5.2+173.ga4278d4.dirty/bentoml/server/static\n",
      "copying bentoml/server/static/swagger-ui.css -> BentoML-0.5.2+173.ga4278d4.dirty/bentoml/server/static\n",
      "copying bentoml/utils/__init__.py -> BentoML-0.5.2+173.ga4278d4.dirty/bentoml/utils\n",
      "copying bentoml/utils/benchmark.py -> BentoML-0.5.2+173.ga4278d4.dirty/bentoml/utils\n",
      "copying bentoml/utils/cloudpickle.py -> BentoML-0.5.2+173.ga4278d4.dirty/bentoml/utils\n",
      "copying bentoml/utils/hybridmethod.py -> BentoML-0.5.2+173.ga4278d4.dirty/bentoml/utils\n",
      "copying bentoml/utils/log.py -> BentoML-0.5.2+173.ga4278d4.dirty/bentoml/utils\n",
      "copying bentoml/utils/pip_pkg.py -> BentoML-0.5.2+173.ga4278d4.dirty/bentoml/utils\n",
      "copying bentoml/utils/s3.py -> BentoML-0.5.2+173.ga4278d4.dirty/bentoml/utils\n",
      "copying bentoml/utils/tempdir.py -> BentoML-0.5.2+173.ga4278d4.dirty/bentoml/utils\n",
      "copying bentoml/utils/trace.py -> BentoML-0.5.2+173.ga4278d4.dirty/bentoml/utils\n",
      "copying bentoml/utils/usage_stats.py -> BentoML-0.5.2+173.ga4278d4.dirty/bentoml/utils\n",
      "copying bentoml/utils/validator/__init__.py -> BentoML-0.5.2+173.ga4278d4.dirty/bentoml/utils/validator\n",
      "copying bentoml/yatai/__init__.py -> BentoML-0.5.2+173.ga4278d4.dirty/bentoml/yatai\n",
      "copying bentoml/yatai/deployment_utils.py -> BentoML-0.5.2+173.ga4278d4.dirty/bentoml/yatai\n",
      "copying bentoml/yatai/status.py -> BentoML-0.5.2+173.ga4278d4.dirty/bentoml/yatai\n",
      "copying bentoml/yatai/utils.py -> BentoML-0.5.2+173.ga4278d4.dirty/bentoml/yatai\n",
      "copying bentoml/yatai/yatai_service_impl.py -> BentoML-0.5.2+173.ga4278d4.dirty/bentoml/yatai\n",
      "copying bentoml/yatai/client/__init__.py -> BentoML-0.5.2+173.ga4278d4.dirty/bentoml/yatai/client\n",
      "copying bentoml/yatai/client/bento_repository_api.py -> BentoML-0.5.2+173.ga4278d4.dirty/bentoml/yatai/client\n",
      "copying bentoml/yatai/client/deployment_api.py -> BentoML-0.5.2+173.ga4278d4.dirty/bentoml/yatai/client\n",
      "Writing BentoML-0.5.2+173.ga4278d4.dirty/setup.cfg\n",
      "UPDATING BentoML-0.5.2+173.ga4278d4.dirty/bentoml/_version.py\n",
      "set BentoML-0.5.2+173.ga4278d4.dirty/bentoml/_version.py to '0.5.2+173.ga4278d4.dirty'\n",
      "Creating tar archive\n",
      "removing 'BentoML-0.5.2+173.ga4278d4.dirty' (and everything under it)\n",
      "[2020-04-23 23:39:03,215] WARNING - BentoML local changes detected - Local BentoML repository including all code changes will be bundled together with the BentoService bundle. When used with docker, the base docker image will be default to same version as last PyPI release at version: 0.5.2. You can also force bentoml to use a specific version for deploying your BentoService bundle, by setting the config 'core/bentoml_deploy_version' to a pinned version or your custom BentoML on github, e.g.:'bentoml_deploy_version = git+https://github.com/{username}/bentoml.git@{branch}'\n",
      "[2020-04-23 23:39:03,230] WARNING - Saved BentoService bundle version mismatch: loading BentoServie bundle create with BentoML version 0.5.2,  but loading from BentoML version 0.5.2+173.ga4278d4.dirty\n",
      "[2020-04-23 23:39:03,559] INFO - BentoService bundle 'Service:20200423233754_0DF217' saved to: /home/bentoml/bentoml/repository/Service/20200423233754_0DF217\n"
     ]
    }
   ],
   "source": [
    "from bentoml_service import Service\n",
    "\n",
    "bento_svc = Service()\n",
    "bento_svc.pack(\"model\", model)\n",
    "bento_svc.pack(\"tokenizer\", tokenizer)\n",
    "saved_path = bento_svc.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/bentoml/bentoml/repository/Service/20200423233754_0DF217\n"
     ]
    }
   ],
   "source": [
    "print(saved_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Serving\n",
    "\n",
    "Since BERT is a large model, if you met OOM bellow,  \n",
    "you may need to restart this kernel to release the RAM/GRAM used by training model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "bentoml_bundle_path = '/home/bentoml/bentoml/repository/Service/20200423233754_0DF217' # saved_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56717\n"
     ]
    }
   ],
   "source": [
    "from bentoml.utils import detect_free_port\n",
    "PORT = detect_free_port()\n",
    "print(PORT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bentoml serve-gunicorn /home/bentoml/bentoml/repository/Service/20200423231933_91D0AB --port 40159 --enable-microbatch --workers 1\n"
     ]
    }
   ],
   "source": [
    "# Option 1: serve directly\n",
    "print(f\"bentoml serve-gunicorn {bentoml_bundle_path} --port {PORT} --enable-microbatch --workers 1\")\n",
    "\n",
    "!bentoml serve-gunicorn {bentoml_bundle_path} --port {PORT} --enable-microbatch --workers 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sending build context to Docker daemon  457.3MB\n",
      "Step 1/15 : FROM continuumio/miniconda3:4.7.12\n",
      " ---> 406f2b43ea59\n",
      "Step 2/15 : ENTRYPOINT [ \"/bin/bash\", \"-c\" ]\n",
      " ---> Using cache\n",
      " ---> 72ac38cf396d\n",
      "Step 3/15 : EXPOSE 5000\n",
      " ---> Using cache\n",
      " ---> 8475dc08cadd\n",
      "Step 4/15 : RUN set -x      && apt-get update      && apt-get install --no-install-recommends --no-install-suggests -y libpq-dev build-essential      && rm -rf /var/lib/apt/lists/*\n",
      " ---> Using cache\n",
      " ---> 0fc16fc9a6ea\n",
      "Step 5/15 : RUN conda install pip numpy scipy       && pip install gunicorn\n",
      " ---> Running in a7a9e19f6122\n",
      "Collecting package metadata (current_repodata.json): ...working... done\n",
      "Solving environment: ...working... done\n",
      "\n",
      "## Package Plan ##\n",
      "\n",
      "  environment location: /opt/conda\n",
      "\n",
      "  added / updated specs:\n",
      "    - numpy\n",
      "    - pip\n",
      "    - scipy\n",
      "\n",
      "\n",
      "The following packages will be downloaded:\n",
      "\n",
      "    package                    |            build\n",
      "    ---------------------------|-----------------\n",
      "    blas-1.0                   |              mkl           6 KB\n",
      "    ca-certificates-2020.1.1   |                0         125 KB\n",
      "    certifi-2020.4.5.1         |           py37_0         155 KB\n",
      "    conda-4.8.3                |           py37_0         2.8 MB\n",
      "    intel-openmp-2020.0        |              166         756 KB\n",
      "    libgfortran-ng-7.3.0       |       hdf63c60_0        1006 KB\n",
      "    mkl-2020.0                 |              166       128.9 MB\n",
      "    mkl-service-2.3.0          |   py37he904b0f_0         218 KB\n",
      "    mkl_fft-1.0.15             |   py37ha843d7b_0         154 KB\n",
      "    mkl_random-1.1.0           |   py37hd6b4f25_0         321 KB\n",
      "    numpy-1.18.1               |   py37h4f9e942_0           5 KB\n",
      "    numpy-base-1.18.1          |   py37hde5b4d6_1         4.2 MB\n",
      "    openssl-1.1.1g             |       h7b6447c_0         2.5 MB\n",
      "    pip-20.0.2                 |           py37_1         1.7 MB\n",
      "    scipy-1.4.1                |   py37h0b6359f_0        14.5 MB\n",
      "    ------------------------------------------------------------\n",
      "                                           Total:       157.3 MB\n",
      "\n",
      "The following NEW packages will be INSTALLED:\n",
      "\n",
      "  blas               pkgs/main/linux-64::blas-1.0-mkl\n",
      "  intel-openmp       pkgs/main/linux-64::intel-openmp-2020.0-166\n",
      "  libgfortran-ng     pkgs/main/linux-64::libgfortran-ng-7.3.0-hdf63c60_0\n",
      "  mkl                pkgs/main/linux-64::mkl-2020.0-166\n",
      "  mkl-service        pkgs/main/linux-64::mkl-service-2.3.0-py37he904b0f_0\n",
      "  mkl_fft            pkgs/main/linux-64::mkl_fft-1.0.15-py37ha843d7b_0\n",
      "  mkl_random         pkgs/main/linux-64::mkl_random-1.1.0-py37hd6b4f25_0\n",
      "  numpy              pkgs/main/linux-64::numpy-1.18.1-py37h4f9e942_0\n",
      "  numpy-base         pkgs/main/linux-64::numpy-base-1.18.1-py37hde5b4d6_1\n",
      "  scipy              pkgs/main/linux-64::scipy-1.4.1-py37h0b6359f_0\n",
      "\n",
      "The following packages will be UPDATED:\n",
      "\n",
      "  ca-certificates                               2019.8.28-0 --> 2020.1.1-0\n",
      "  certifi                                  2019.9.11-py37_0 --> 2020.4.5.1-py37_0\n",
      "  conda                                       4.7.12-py37_0 --> 4.8.3-py37_0\n",
      "  openssl                                 1.1.1d-h7b6447c_2 --> 1.1.1g-h7b6447c_0\n",
      "  pip                                         19.2.3-py37_0 --> 20.0.2-py37_1\n",
      "\n",
      "\n",
      "Proceed ([y]/n)? \n",
      "\n",
      "Downloading and Extracting Packages\n",
      "pip-20.0.2           | 1.7 MB    | ########## | 100% \n",
      "openssl-1.1.1g       | 2.5 MB    | ########## | 100% \n",
      "mkl-service-2.3.0    | 218 KB    | ########## | 100% \n",
      "intel-openmp-2020.0  | 756 KB    | ########## | 100% \n",
      "mkl-2020.0           | 128.9 MB  | ########## | 100% \n",
      "libgfortran-ng-7.3.0 | 1006 KB   | ########## | 100% \n",
      "numpy-base-1.18.1    | 4.2 MB    | ########## | 100% \n",
      "numpy-1.18.1         | 5 KB      | ########## | 100% \n",
      "mkl_fft-1.0.15       | 154 KB    | ########## | 100% \n",
      "mkl_random-1.1.0     | 321 KB    | ########## | 100% \n",
      "ca-certificates-2020 | 125 KB    | ########## | 100% \n",
      "certifi-2020.4.5.1   | 155 KB    | ########## | 100% \n",
      "scipy-1.4.1          | 14.5 MB   | ########## | 100% \n",
      "conda-4.8.3          | 2.8 MB    | ########## | 100% \n",
      "blas-1.0             | 6 KB      | ########## | 100% \n",
      "Preparing transaction: ...working... done\n",
      "Verifying transaction: ...working... done\n",
      "Executing transaction: ...working... done\n",
      "Collecting gunicorn\n",
      "  Downloading gunicorn-20.0.4-py2.py3-none-any.whl (77 kB)\n",
      "Requirement already satisfied: setuptools>=3.0 in /opt/conda/lib/python3.7/site-packages (from gunicorn) (41.4.0)\n",
      "Installing collected packages: gunicorn\n",
      "Successfully installed gunicorn-20.0.4\n",
      "Removing intermediate container a7a9e19f6122\n",
      " ---> b2dc0adaeadb\n",
      "Step 6/15 : COPY . /bento\n",
      " ---> d7b975c97702\n",
      "Step 7/15 : WORKDIR /bento\n",
      " ---> Running in 2c4d019b96d4\n",
      "Removing intermediate container 2c4d019b96d4\n",
      " ---> 226cff48a45e\n",
      "Step 8/15 : RUN if [ -f /bento/setup.sh ]; then /bin/bash -c /bento/setup.sh; fi\n",
      " ---> Running in de65a1259575\n",
      "Removing intermediate container de65a1259575\n",
      " ---> 2131a1a56473\n",
      "Step 9/15 : RUN conda env update -n base -f /bento/environment.yml\n",
      " ---> Running in ac705abc8f5e\n",
      "Collecting package metadata (repodata.json): ...working... done\n",
      "Solving environment: ...working... done\n",
      "\n",
      "Downloading and Extracting Packages\n",
      "pyopenssl-19.1.0     | 87 KB     | ########## | 100% \n",
      "wheel-0.34.2         | 51 KB     | ########## | 100% \n",
      "idna-2.9             | 49 KB     | ########## | 100% \n",
      "tqdm-4.36.1          | 50 KB     | ########## | 100% \n",
      "pysocks-1.7.1        | 30 KB     | ########## | 100% \n",
      "pycosat-0.6.3        | 82 KB     | ########## | 100% \n",
      "pycparser-2.20       | 92 KB     | ########## | 100% \n",
      "ld_impl_linux-64-2.3 | 568 KB    | ########## | 100% \n",
      "chardet-3.0.4        | 180 KB    | ########## | 100% \n",
      "cffi-1.14.0          | 223 KB    | ########## | 100% \n",
      "asn1crypto-1.3.0     | 164 KB    | ########## | 100% \n",
      "conda-4.8.3          | 2.8 MB    | ########## | 100% \n",
      "cryptography-2.8     | 552 KB    | ########## | 100% \n",
      "sqlite-3.31.1        | 1.1 MB    | ########## | 100% \n",
      "setuptools-46.1.3    | 513 KB    | ########## | 100% \n",
      "ruamel_yaml-0.15.87  | 257 KB    | ########## | 100% \n",
      "certifi-2020.4.5.1   | 155 KB    | ########## | 100% \n",
      "urllib3-1.25.8       | 169 KB    | ########## | 100% \n",
      "scipy-1.2.1          | 13.7 MB   | ########## | 100% \n",
      "six-1.14.0           | 27 KB     | ########## | 100% \n",
      "requests-2.23.0      | 91 KB     | ########## | 100% \n",
      "numpy-1.14.2         | 3.2 MB    | ########## | 100% \n",
      "conda-package-handli | 795 KB    | ########## | 100% \n",
      "pip-20.0.2           | 1.7 MB    | ########## | 100% \n",
      "python-3.6.10        | 29.7 MB   | ########## | 100% \n",
      "Preparing transaction: ...working... done\n",
      "Verifying transaction: ...working... done\n",
      "Executing transaction: ...working... done\n",
      "#\n",
      "# To activate this environment, use\n",
      "#\n",
      "#     $ conda activate base\n",
      "#\n",
      "# To deactivate an active environment, use\n",
      "#\n",
      "#     $ conda deactivate\n",
      "\n",
      "Removing intermediate container ac705abc8f5e\n",
      " ---> d013d1cc37d3\n",
      "Step 10/15 : ARG PIP_TRUSTED_HOST\n",
      " ---> Running in 8d1c29741c56\n",
      "Removing intermediate container 8d1c29741c56\n",
      " ---> aec02ee008bd\n",
      "Step 11/15 : ARG PIP_INDEX_URL\n",
      " ---> Running in d07477dc3ed6\n",
      "Removing intermediate container d07477dc3ed6\n",
      " ---> 1e3ecadeccd5\n",
      "Step 12/15 : RUN pip install -r /bento/requirements.txt\n",
      " ---> Running in 45ef9a05351b\n",
      "Collecting bentoml==0.5.2\n",
      "  Downloading BentoML-0.5.2-py3-none-any.whl (521 kB)\n",
      "Collecting tensorflow\n",
      "  Downloading tensorflow-2.1.0-cp36-cp36m-manylinux2010_x86_64.whl (421.8 MB)\n",
      "Collecting bert-for-tf2\n",
      "  Downloading bert-for-tf2-0.14.4.tar.gz (40 kB)\n",
      "Collecting cerberus\n",
      "  Downloading Cerberus-1.3.2.tar.gz (52 kB)\n",
      "Collecting configparser\n",
      "  Downloading configparser-5.0.0-py3-none-any.whl (22 kB)\n",
      "Collecting sqlalchemy>=1.3.0\n",
      "  Downloading SQLAlchemy-1.3.16-cp36-cp36m-manylinux2010_x86_64.whl (1.2 MB)\n",
      "Collecting pandas\n",
      "  Downloading pandas-1.0.3-cp36-cp36m-manylinux1_x86_64.whl (10.0 MB)\n",
      "Collecting packaging\n",
      "  Downloading packaging-20.3-py2.py3-none-any.whl (37 kB)\n",
      "Collecting humanfriendly\n",
      "  Downloading humanfriendly-8.2-py2.py3-none-any.whl (86 kB)\n",
      "Collecting protobuf>=3.6.0\n",
      "  Downloading protobuf-3.11.3-cp36-cp36m-manylinux1_x86_64.whl (1.3 MB)\n",
      "Collecting python-json-logger\n",
      "  Downloading python-json-logger-0.1.11.tar.gz (6.0 kB)\n",
      "Collecting alembic\n",
      "  Downloading alembic-1.4.2.tar.gz (1.1 MB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "    Preparing wheel metadata: started\n",
      "    Preparing wheel metadata: finished with status 'done'\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.6/site-packages (from bentoml==0.5.2->-r /bento/requirements.txt (line 1)) (1.14.0)\n",
      "Collecting ruamel.yaml>=0.15.0\n",
      "  Downloading ruamel.yaml-0.16.10-py2.py3-none-any.whl (111 kB)\n",
      "Collecting gunicorn\n",
      "  Using cached gunicorn-20.0.4-py2.py3-none-any.whl (77 kB)\n",
      "Collecting tabulate\n",
      "  Downloading tabulate-0.8.7-py3-none-any.whl (24 kB)\n",
      "Collecting grpcio\n",
      "  Downloading grpcio-1.28.1-cp36-cp36m-manylinux2010_x86_64.whl (2.8 MB)\n",
      "Collecting click>=7.0\n",
      "  Downloading click-7.1.1-py2.py3-none-any.whl (82 kB)\n",
      "Collecting flask\n",
      "  Downloading Flask-1.1.2-py2.py3-none-any.whl (94 kB)\n",
      "Collecting prometheus-client\n",
      "  Downloading prometheus_client-0.7.1.tar.gz (38 kB)\n",
      "Collecting pathlib2\n",
      "  Downloading pathlib2-2.3.5-py2.py3-none-any.whl (18 kB)\n",
      "Collecting boto3\n",
      "  Downloading boto3-1.12.44-py2.py3-none-any.whl (128 kB)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.6/site-packages (from bentoml==0.5.2->-r /bento/requirements.txt (line 1)) (2.23.0)\n",
      "Collecting docker\n",
      "  Downloading docker-4.2.0-py2.py3-none-any.whl (143 kB)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.6/site-packages (from bentoml==0.5.2->-r /bento/requirements.txt (line 1)) (1.14.2)\n",
      "Collecting termcolor>=1.1.0\n",
      "  Downloading termcolor-1.1.0.tar.gz (3.9 kB)\n",
      "Collecting tensorflow-estimator<2.2.0,>=2.1.0rc0\n",
      "  Downloading tensorflow_estimator-2.1.0-py2.py3-none-any.whl (448 kB)\n",
      "Collecting absl-py>=0.7.0\n",
      "  Downloading absl-py-0.9.0.tar.gz (104 kB)\n",
      "Collecting keras-applications>=1.0.8\n",
      "  Downloading Keras_Applications-1.0.8-py3-none-any.whl (50 kB)\n",
      "Collecting google-pasta>=0.1.6\n",
      "  Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "Collecting opt-einsum>=2.3.2\n",
      "  Downloading opt_einsum-3.2.1-py3-none-any.whl (63 kB)\n",
      "Collecting astor>=0.6.0\n",
      "  Downloading astor-0.8.1-py2.py3-none-any.whl (27 kB)\n",
      "Collecting tensorboard<2.2.0,>=2.1.0\n",
      "  Downloading tensorboard-2.1.1-py3-none-any.whl (3.8 MB)\n",
      "Collecting gast==0.2.2\n",
      "  Downloading gast-0.2.2.tar.gz (10 kB)\n",
      "Collecting keras-preprocessing>=1.1.0\n",
      "  Downloading Keras_Preprocessing-1.1.0-py2.py3-none-any.whl (41 kB)\n",
      "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /opt/conda/lib/python3.6/site-packages (from tensorflow->-r /bento/requirements.txt (line 2)) (0.34.2)\n",
      "Collecting wrapt>=1.11.1\n",
      "  Downloading wrapt-1.12.1.tar.gz (27 kB)\n",
      "Collecting scipy==1.4.1; python_version >= \"3\"\n",
      "  Downloading scipy-1.4.1-cp36-cp36m-manylinux1_x86_64.whl (26.1 MB)\n",
      "Collecting py-params>=0.9.6\n",
      "  Downloading py-params-0.9.7.tar.gz (6.8 kB)\n",
      "Collecting params-flow>=0.8.0\n",
      "  Downloading params-flow-0.8.1.tar.gz (19 kB)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.6/site-packages (from cerberus->bentoml==0.5.2->-r /bento/requirements.txt (line 1)) (46.1.3.post20200330)\n",
      "Collecting python-dateutil>=2.6.1\n",
      "  Downloading python_dateutil-2.8.1-py2.py3-none-any.whl (227 kB)\n",
      "Collecting pytz>=2017.2\n",
      "  Downloading pytz-2019.3-py2.py3-none-any.whl (509 kB)\n",
      "Collecting pyparsing>=2.0.2\n",
      "  Downloading pyparsing-2.4.7-py2.py3-none-any.whl (67 kB)\n",
      "Collecting python-editor>=0.3\n",
      "  Downloading python_editor-1.0.4-py3-none-any.whl (4.9 kB)\n",
      "Collecting Mako\n",
      "  Downloading Mako-1.1.2-py2.py3-none-any.whl (75 kB)\n",
      "Collecting ruamel.yaml.clib>=0.1.2; platform_python_implementation == \"CPython\" and python_version < \"3.9\"\n",
      "  Downloading ruamel.yaml.clib-0.2.0-cp36-cp36m-manylinux1_x86_64.whl (548 kB)\n",
      "Collecting itsdangerous>=0.24\n",
      "  Downloading itsdangerous-1.1.0-py2.py3-none-any.whl (16 kB)\n",
      "Collecting Jinja2>=2.10.1\n",
      "  Downloading Jinja2-2.11.2-py2.py3-none-any.whl (125 kB)\n",
      "Collecting Werkzeug>=0.15\n",
      "  Downloading Werkzeug-1.0.1-py2.py3-none-any.whl (298 kB)\n",
      "Collecting s3transfer<0.4.0,>=0.3.0\n",
      "  Downloading s3transfer-0.3.3-py2.py3-none-any.whl (69 kB)\n",
      "Collecting jmespath<1.0.0,>=0.7.1\n",
      "  Downloading jmespath-0.9.5-py2.py3-none-any.whl (24 kB)\n",
      "Collecting botocore<1.16.0,>=1.15.44\n",
      "  Downloading botocore-1.15.44-py2.py3-none-any.whl (6.1 MB)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.6/site-packages (from requests->bentoml==0.5.2->-r /bento/requirements.txt (line 1)) (2020.4.5.1)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /opt/conda/lib/python3.6/site-packages (from requests->bentoml==0.5.2->-r /bento/requirements.txt (line 1)) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/lib/python3.6/site-packages (from requests->bentoml==0.5.2->-r /bento/requirements.txt (line 1)) (1.25.8)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.6/site-packages (from requests->bentoml==0.5.2->-r /bento/requirements.txt (line 1)) (2.9)\n",
      "Collecting websocket-client>=0.32.0\n",
      "  Downloading websocket_client-0.57.0-py2.py3-none-any.whl (200 kB)\n",
      "Collecting h5py\n",
      "  Downloading h5py-2.10.0-cp36-cp36m-manylinux1_x86_64.whl (2.9 MB)\n",
      "Collecting google-auth-oauthlib<0.5,>=0.4.1\n",
      "  Downloading google_auth_oauthlib-0.4.1-py2.py3-none-any.whl (18 kB)\n",
      "Collecting markdown>=2.6.8\n",
      "  Downloading Markdown-3.2.1-py2.py3-none-any.whl (88 kB)\n",
      "Collecting google-auth<2,>=1.6.3\n",
      "  Downloading google_auth-1.14.1-py2.py3-none-any.whl (89 kB)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.6/site-packages (from params-flow>=0.8.0->bert-for-tf2->-r /bento/requirements.txt (line 3)) (4.36.1)\n",
      "Collecting MarkupSafe>=0.9.2\n",
      "  Downloading MarkupSafe-1.1.1-cp36-cp36m-manylinux1_x86_64.whl (27 kB)\n",
      "Collecting docutils<0.16,>=0.10\n",
      "  Downloading docutils-0.15.2-py3-none-any.whl (547 kB)\n",
      "Collecting requests-oauthlib>=0.7.0\n",
      "  Downloading requests_oauthlib-1.3.0-py2.py3-none-any.whl (23 kB)\n",
      "Collecting rsa<4.1,>=3.1.4\n",
      "  Downloading rsa-4.0-py2.py3-none-any.whl (38 kB)\n",
      "Collecting pyasn1-modules>=0.2.1\n",
      "  Downloading pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\n",
      "Collecting cachetools<5.0,>=2.0.0\n",
      "  Downloading cachetools-4.1.0-py3-none-any.whl (10 kB)\n",
      "Collecting oauthlib>=3.0.0\n",
      "  Downloading oauthlib-3.1.0-py2.py3-none-any.whl (147 kB)\n",
      "Collecting pyasn1>=0.1.3\n",
      "  Downloading pyasn1-0.4.8-py2.py3-none-any.whl (77 kB)\n",
      "Building wheels for collected packages: bert-for-tf2, cerberus, python-json-logger, alembic, prometheus-client, termcolor, absl-py, gast, wrapt, py-params, params-flow\n",
      "  Building wheel for bert-for-tf2 (setup.py): started\n",
      "  Building wheel for bert-for-tf2 (setup.py): finished with status 'done'\n",
      "  Created wheel for bert-for-tf2: filename=bert_for_tf2-0.14.4-py3-none-any.whl size=30114 sha256=dc03ab4d0523c4fe6fd705d049c29c8d32f43d76e7dcb04b7b5d90bf30de45b6\n",
      "  Stored in directory: /root/.cache/pip/wheels/7c/b5/ea/3e768502fa1032a6bfefcd0e7dd95e2110e026bf3c584940cd\n",
      "  Building wheel for cerberus (setup.py): started\n",
      "  Building wheel for cerberus (setup.py): finished with status 'done'\n",
      "  Created wheel for cerberus: filename=Cerberus-1.3.2-py3-none-any.whl size=54335 sha256=02b4989a972ade5651d73b1829523f72b18acbc067807c830988a548676f87c1\n",
      "  Stored in directory: /root/.cache/pip/wheels/57/73/88/653dda2eaa03ee38e7ab10f48ddaadd192fadda426bb5c0bdf\n",
      "  Building wheel for python-json-logger (setup.py): started\n",
      "  Building wheel for python-json-logger (setup.py): finished with status 'done'\n",
      "  Created wheel for python-json-logger: filename=python_json_logger-0.1.11-py2.py3-none-any.whl size=5075 sha256=68f49ece191d9b329114ebd73409ac94a4dc87d761714945870ea1f60b02ac8c\n",
      "  Stored in directory: /root/.cache/pip/wheels/99/ad/da/22b0dbe95e769350f2641f2f18c25d3ba3a0ce21dd1f655cf1\n",
      "  Building wheel for alembic (PEP 517): started\n",
      "  Building wheel for alembic (PEP 517): finished with status 'done'\n",
      "  Created wheel for alembic: filename=alembic-1.4.2-py2.py3-none-any.whl size=159543 sha256=10b75c5c9a5b956bdd25a8266bb440ed53a3802ed3e1237319f50421e22dde4f\n",
      "  Stored in directory: /root/.cache/pip/wheels/f2/50/61/5cc491b0ca39be60dfb4dce940b389ff91b847d62e0eb2d680\n",
      "  Building wheel for prometheus-client (setup.py): started\n",
      "  Building wheel for prometheus-client (setup.py): finished with status 'done'\n",
      "  Created wheel for prometheus-client: filename=prometheus_client-0.7.1-py3-none-any.whl size=41404 sha256=38114b16991bc0f88fffb229964893171d0b34f7dfc1e9640d3d2fa11d9e7841\n",
      "  Stored in directory: /root/.cache/pip/wheels/1d/4a/79/a3ad3f74b3495b4555359375ca33ad7b64e77f8b7a53c8894f\n",
      "  Building wheel for termcolor (setup.py): started\n",
      "  Building wheel for termcolor (setup.py): finished with status 'done'\n",
      "  Created wheel for termcolor: filename=termcolor-1.1.0-py3-none-any.whl size=4830 sha256=269c12e7350d6782ce935e2cb9d040d33095765fab41350f37252eca12c36d55\n",
      "  Stored in directory: /root/.cache/pip/wheels/93/2a/eb/e58dbcbc963549ee4f065ff80a59f274cc7210b6eab962acdc\n",
      "  Building wheel for absl-py (setup.py): started\n",
      "  Building wheel for absl-py (setup.py): finished with status 'done'\n",
      "  Created wheel for absl-py: filename=absl_py-0.9.0-py3-none-any.whl size=121931 sha256=462508fc23e26d38a985b2a955ff83dea24859ffc371ae2d640ea2b565df0847\n",
      "  Stored in directory: /root/.cache/pip/wheels/c3/af/84/3962a6af7b4ab336e951b7877dcfb758cf94548bb1771e0679\n",
      "  Building wheel for gast (setup.py): started\n",
      "  Building wheel for gast (setup.py): finished with status 'done'\n",
      "  Created wheel for gast: filename=gast-0.2.2-py3-none-any.whl size=7539 sha256=bc6fb1480664fa5588adfd03da22efbbd0bde3c7b51a07a1b2207b4dc611cb4d\n",
      "  Stored in directory: /root/.cache/pip/wheels/19/a7/b9/0740c7a3a7d1d348f04823339274b90de25fbcd217b2ee1fbe\n",
      "  Building wheel for wrapt (setup.py): started\n",
      "  Building wheel for wrapt (setup.py): finished with status 'done'\n",
      "  Created wheel for wrapt: filename=wrapt-1.12.1-cp36-cp36m-linux_x86_64.whl size=75208 sha256=9ee1a5fa6c8e3fe3471f5d4c194588f4d9681643f97b46aa18455011fbc23083\n",
      "  Stored in directory: /root/.cache/pip/wheels/32/42/7f/23cae9ff6ef66798d00dc5d659088e57dbba01566f6c60db63\n",
      "  Building wheel for py-params (setup.py): started\n",
      "  Building wheel for py-params (setup.py): finished with status 'done'\n",
      "  Created wheel for py-params: filename=py_params-0.9.7-py3-none-any.whl size=7302 sha256=ab548fc73ca0a33b7d43024f0c13caced381d57d2262066098cf2c3df6acf3b3\n",
      "  Stored in directory: /root/.cache/pip/wheels/18/e2/80/244168a554fc8469897bb86b3ea7183b56ecc92f484e67b375\n",
      "  Building wheel for params-flow (setup.py): started\n",
      "  Building wheel for params-flow (setup.py): finished with status 'done'\n",
      "  Created wheel for params-flow: filename=params_flow-0.8.1-py3-none-any.whl size=16121 sha256=0e444e7ee4c9cb0467404383187929aa838cbdb63b896d151ae672c3277f5254\n",
      "  Stored in directory: /root/.cache/pip/wheels/cc/70/6f/98e4cd9239940e8160866872af2be0ca038cdfbdc8113db2ad\n",
      "Successfully built bert-for-tf2 cerberus python-json-logger alembic prometheus-client termcolor absl-py gast wrapt py-params params-flow\n",
      "\u001b[91mERROR: tensorflow 2.1.0 has requirement numpy<2.0,>=1.16.0, but you'll have numpy 1.14.2 which is incompatible.\n",
      "\u001b[0mInstalling collected packages: cerberus, configparser, sqlalchemy, python-dateutil, pytz, pandas, pyparsing, packaging, humanfriendly, protobuf, python-json-logger, python-editor, MarkupSafe, Mako, alembic, ruamel.yaml.clib, ruamel.yaml, gunicorn, tabulate, grpcio, click, itsdangerous, Jinja2, Werkzeug, flask, prometheus-client, pathlib2, docutils, jmespath, botocore, s3transfer, boto3, websocket-client, docker, bentoml, termcolor, tensorflow-estimator, absl-py, h5py, keras-applications, google-pasta, opt-einsum, astor, pyasn1, rsa, pyasn1-modules, cachetools, google-auth, oauthlib, requests-oauthlib, google-auth-oauthlib, markdown, tensorboard, gast, keras-preprocessing, wrapt, scipy, tensorflow, py-params, params-flow, bert-for-tf2\n",
      "  Attempting uninstall: scipy\n",
      "    Found existing installation: scipy 1.2.1\n",
      "    Uninstalling scipy-1.2.1:\n",
      "      Successfully uninstalled scipy-1.2.1\n",
      "Successfully installed Jinja2-2.11.2 Mako-1.1.2 MarkupSafe-1.1.1 Werkzeug-1.0.1 absl-py-0.9.0 alembic-1.4.2 astor-0.8.1 bentoml-0.5.2 bert-for-tf2-0.14.4 boto3-1.12.44 botocore-1.15.44 cachetools-4.1.0 cerberus-1.3.2 click-7.1.1 configparser-5.0.0 docker-4.2.0 docutils-0.15.2 flask-1.1.2 gast-0.2.2 google-auth-1.14.1 google-auth-oauthlib-0.4.1 google-pasta-0.2.0 grpcio-1.28.1 gunicorn-20.0.4 h5py-2.10.0 humanfriendly-8.2 itsdangerous-1.1.0 jmespath-0.9.5 keras-applications-1.0.8 keras-preprocessing-1.1.0 markdown-3.2.1 oauthlib-3.1.0 opt-einsum-3.2.1 packaging-20.3 pandas-1.0.3 params-flow-0.8.1 pathlib2-2.3.5 prometheus-client-0.7.1 protobuf-3.11.3 py-params-0.9.7 pyasn1-0.4.8 pyasn1-modules-0.2.8 pyparsing-2.4.7 python-dateutil-2.8.1 python-editor-1.0.4 python-json-logger-0.1.11 pytz-2019.3 requests-oauthlib-1.3.0 rsa-4.0 ruamel.yaml-0.16.10 ruamel.yaml.clib-0.2.0 s3transfer-0.3.3 scipy-1.4.1 sqlalchemy-1.3.16 tabulate-0.8.7 tensorboard-2.1.1 tensorflow-2.1.0 tensorflow-estimator-2.1.0 termcolor-1.1.0 websocket-client-0.57.0 wrapt-1.12.1\n",
      "Removing intermediate container 45ef9a05351b\n",
      " ---> 4c095636c56f\n",
      "Step 13/15 : RUN if [ -f /bento/bentoml_init.sh ]; then /bin/bash -c /bento/bentoml_init.sh; fi\n",
      " ---> Running in b1448dd3093d\n",
      "Processing ./bundled_pip_dependencies/BentoML-0.5.2+173.ga4278d4.dirty.tar.gz\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "    Preparing wheel metadata: started\n",
      "    Preparing wheel metadata: finished with status 'done'\n",
      "Requirement already satisfied, skipping upgrade: protobuf>=3.6.0 in /opt/conda/lib/python3.6/site-packages (from BentoML==0.5.2+173.ga4278d4.dirty) (3.11.3)\n",
      "Collecting contextvars; python_version < \"3.7\"\n",
      "  Downloading contextvars-2.4.tar.gz (9.6 kB)\n",
      "Requirement already satisfied, skipping upgrade: numpy in /opt/conda/lib/python3.6/site-packages (from BentoML==0.5.2+173.ga4278d4.dirty) (1.14.2)\n",
      "Requirement already satisfied, skipping upgrade: humanfriendly in /opt/conda/lib/python3.6/site-packages (from BentoML==0.5.2+173.ga4278d4.dirty) (8.2)\n",
      "Requirement already satisfied, skipping upgrade: boto3 in /opt/conda/lib/python3.6/site-packages (from BentoML==0.5.2+173.ga4278d4.dirty) (1.12.44)\n",
      "Requirement already satisfied, skipping upgrade: sqlalchemy>=1.3.0 in /opt/conda/lib/python3.6/site-packages (from BentoML==0.5.2+173.ga4278d4.dirty) (1.3.16)\n",
      "Collecting sqlalchemy-utils\n",
      "  Downloading SQLAlchemy-Utils-0.36.3.tar.gz (128 kB)\n",
      "Requirement already satisfied, skipping upgrade: docker in /opt/conda/lib/python3.6/site-packages (from BentoML==0.5.2+173.ga4278d4.dirty) (4.2.0)\n",
      "Collecting grpcio<=1.27.2\n",
      "  Downloading grpcio-1.27.2-cp36-cp36m-manylinux2010_x86_64.whl (2.7 MB)\n",
      "Requirement already satisfied, skipping upgrade: python-json-logger in /opt/conda/lib/python3.6/site-packages (from BentoML==0.5.2+173.ga4278d4.dirty) (0.1.11)\n",
      "Collecting aiohttp\n",
      "  Downloading aiohttp-3.6.2-cp36-cp36m-manylinux1_x86_64.whl (1.2 MB)\n",
      "Requirement already satisfied, skipping upgrade: gunicorn in /opt/conda/lib/python3.6/site-packages (from BentoML==0.5.2+173.ga4278d4.dirty) (20.0.4)\n",
      "Requirement already satisfied, skipping upgrade: tabulate in /opt/conda/lib/python3.6/site-packages (from BentoML==0.5.2+173.ga4278d4.dirty) (0.8.7)\n",
      "Requirement already satisfied, skipping upgrade: alembic in /opt/conda/lib/python3.6/site-packages (from BentoML==0.5.2+173.ga4278d4.dirty) (1.4.2)\n",
      "Collecting py-zipkin\n",
      "  Downloading py_zipkin-0.20.0-py2.py3-none-any.whl (52 kB)\n",
      "Collecting python-dateutil<2.8.1,>=2.1\n",
      "  Downloading python_dateutil-2.8.0-py2.py3-none-any.whl (226 kB)\n",
      "Requirement already satisfied, skipping upgrade: click>=7.0 in /opt/conda/lib/python3.6/site-packages (from BentoML==0.5.2+173.ga4278d4.dirty) (7.1.1)\n",
      "Requirement already satisfied, skipping upgrade: prometheus-client in /opt/conda/lib/python3.6/site-packages (from BentoML==0.5.2+173.ga4278d4.dirty) (0.7.1)\n",
      "Requirement already satisfied, skipping upgrade: packaging in /opt/conda/lib/python3.6/site-packages (from BentoML==0.5.2+173.ga4278d4.dirty) (20.3)\n",
      "Requirement already satisfied, skipping upgrade: requests in /opt/conda/lib/python3.6/site-packages (from BentoML==0.5.2+173.ga4278d4.dirty) (2.23.0)\n",
      "Requirement already satisfied, skipping upgrade: configparser in /opt/conda/lib/python3.6/site-packages (from BentoML==0.5.2+173.ga4278d4.dirty) (5.0.0)\n",
      "Requirement already satisfied, skipping upgrade: flask in /opt/conda/lib/python3.6/site-packages (from BentoML==0.5.2+173.ga4278d4.dirty) (1.1.2)\n",
      "Requirement already satisfied, skipping upgrade: pandas in /opt/conda/lib/python3.6/site-packages (from BentoML==0.5.2+173.ga4278d4.dirty) (1.0.3)\n",
      "Requirement already satisfied, skipping upgrade: ruamel.yaml>=0.15.0 in /opt/conda/lib/python3.6/site-packages (from BentoML==0.5.2+173.ga4278d4.dirty) (0.16.10)\n",
      "Requirement already satisfied, skipping upgrade: cerberus in /opt/conda/lib/python3.6/site-packages (from BentoML==0.5.2+173.ga4278d4.dirty) (1.3.2)\n",
      "Requirement already satisfied, skipping upgrade: six>=1.9 in /opt/conda/lib/python3.6/site-packages (from protobuf>=3.6.0->BentoML==0.5.2+173.ga4278d4.dirty) (1.14.0)\n",
      "Requirement already satisfied, skipping upgrade: setuptools in /opt/conda/lib/python3.6/site-packages (from protobuf>=3.6.0->BentoML==0.5.2+173.ga4278d4.dirty) (46.1.3.post20200330)\n",
      "Collecting immutables>=0.9\n",
      "  Downloading immutables-0.12-cp36-cp36m-manylinux1_x86_64.whl (97 kB)\n",
      "Requirement already satisfied, skipping upgrade: jmespath<1.0.0,>=0.7.1 in /opt/conda/lib/python3.6/site-packages (from boto3->BentoML==0.5.2+173.ga4278d4.dirty) (0.9.5)\n",
      "Requirement already satisfied, skipping upgrade: botocore<1.16.0,>=1.15.44 in /opt/conda/lib/python3.6/site-packages (from boto3->BentoML==0.5.2+173.ga4278d4.dirty) (1.15.44)\n",
      "Requirement already satisfied, skipping upgrade: s3transfer<0.4.0,>=0.3.0 in /opt/conda/lib/python3.6/site-packages (from boto3->BentoML==0.5.2+173.ga4278d4.dirty) (0.3.3)\n",
      "Requirement already satisfied, skipping upgrade: websocket-client>=0.32.0 in /opt/conda/lib/python3.6/site-packages (from docker->BentoML==0.5.2+173.ga4278d4.dirty) (0.57.0)\n",
      "Collecting async-timeout<4.0,>=3.0\n",
      "  Downloading async_timeout-3.0.1-py3-none-any.whl (8.2 kB)\n",
      "Collecting idna-ssl>=1.0; python_version < \"3.7\"\n",
      "  Downloading idna-ssl-1.1.0.tar.gz (3.4 kB)\n",
      "Requirement already satisfied, skipping upgrade: chardet<4.0,>=2.0 in /opt/conda/lib/python3.6/site-packages (from aiohttp->BentoML==0.5.2+173.ga4278d4.dirty) (3.0.4)\n",
      "Collecting attrs>=17.3.0\n",
      "  Downloading attrs-19.3.0-py2.py3-none-any.whl (39 kB)\n",
      "Collecting typing-extensions>=3.6.5; python_version < \"3.7\"\n",
      "  Downloading typing_extensions-3.7.4.2-py3-none-any.whl (22 kB)\n",
      "Collecting yarl<2.0,>=1.0\n",
      "  Downloading yarl-1.4.2-cp36-cp36m-manylinux1_x86_64.whl (252 kB)\n",
      "Collecting multidict<5.0,>=4.5\n",
      "  Downloading multidict-4.7.5-cp36-cp36m-manylinux1_x86_64.whl (148 kB)\n",
      "Requirement already satisfied, skipping upgrade: Mako in /opt/conda/lib/python3.6/site-packages (from alembic->BentoML==0.5.2+173.ga4278d4.dirty) (1.1.2)\n",
      "Requirement already satisfied, skipping upgrade: python-editor>=0.3 in /opt/conda/lib/python3.6/site-packages (from alembic->BentoML==0.5.2+173.ga4278d4.dirty) (1.0.4)\n",
      "Collecting thriftpy2>=0.4.0\n",
      "  Downloading thriftpy2-0.4.11.tar.gz (498 kB)\n",
      "Requirement already satisfied, skipping upgrade: pyparsing>=2.0.2 in /opt/conda/lib/python3.6/site-packages (from packaging->BentoML==0.5.2+173.ga4278d4.dirty) (2.4.7)\n",
      "Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in /opt/conda/lib/python3.6/site-packages (from requests->BentoML==0.5.2+173.ga4278d4.dirty) (2.9)\n",
      "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /opt/conda/lib/python3.6/site-packages (from requests->BentoML==0.5.2+173.ga4278d4.dirty) (2020.4.5.1)\n",
      "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/lib/python3.6/site-packages (from requests->BentoML==0.5.2+173.ga4278d4.dirty) (1.25.8)\n",
      "Requirement already satisfied, skipping upgrade: itsdangerous>=0.24 in /opt/conda/lib/python3.6/site-packages (from flask->BentoML==0.5.2+173.ga4278d4.dirty) (1.1.0)\n",
      "Requirement already satisfied, skipping upgrade: Werkzeug>=0.15 in /opt/conda/lib/python3.6/site-packages (from flask->BentoML==0.5.2+173.ga4278d4.dirty) (1.0.1)\n",
      "Requirement already satisfied, skipping upgrade: Jinja2>=2.10.1 in /opt/conda/lib/python3.6/site-packages (from flask->BentoML==0.5.2+173.ga4278d4.dirty) (2.11.2)\n",
      "Requirement already satisfied, skipping upgrade: pytz>=2017.2 in /opt/conda/lib/python3.6/site-packages (from pandas->BentoML==0.5.2+173.ga4278d4.dirty) (2019.3)\n",
      "Requirement already satisfied, skipping upgrade: ruamel.yaml.clib>=0.1.2; platform_python_implementation == \"CPython\" and python_version < \"3.9\" in /opt/conda/lib/python3.6/site-packages (from ruamel.yaml>=0.15.0->BentoML==0.5.2+173.ga4278d4.dirty) (0.2.0)\n",
      "Requirement already satisfied, skipping upgrade: docutils<0.16,>=0.10 in /opt/conda/lib/python3.6/site-packages (from botocore<1.16.0,>=1.15.44->boto3->BentoML==0.5.2+173.ga4278d4.dirty) (0.15.2)\n",
      "Requirement already satisfied, skipping upgrade: MarkupSafe>=0.9.2 in /opt/conda/lib/python3.6/site-packages (from Mako->alembic->BentoML==0.5.2+173.ga4278d4.dirty) (1.1.1)\n",
      "Collecting ply<4.0,>=3.4\n",
      "  Downloading ply-3.11-py2.py3-none-any.whl (49 kB)\n",
      "Building wheels for collected packages: BentoML, contextvars, sqlalchemy-utils, idna-ssl, thriftpy2\n",
      "  Building wheel for BentoML (PEP 517): started\n",
      "  Building wheel for BentoML (PEP 517): finished with status 'done'\n",
      "  Created wheel for BentoML: filename=BentoML-0.5.2+173.ga4278d4.dirty-py3-none-any.whl size=553995 sha256=f98b2b46504c4955466088ddb2e000ac0486d403cbaf50ad3784b293895b5f4c\n",
      "  Stored in directory: /root/.cache/pip/wheels/94/a6/ab/42dcd1c63a9024fdeb1825d3f6345b71c927dff77c2f106814\n",
      "  Building wheel for contextvars (setup.py): started\n",
      "  Building wheel for contextvars (setup.py): finished with status 'done'\n",
      "  Created wheel for contextvars: filename=contextvars-2.4-py3-none-any.whl size=7664 sha256=0be4f58266e970c71a94bd89439691f95ad0b79016b7410f86844f1abc9c7a19\n",
      "  Stored in directory: /root/.cache/pip/wheels/41/11/53/911724983aa48deb94792432e14e518447212dd6c5477d49d3\n",
      "  Building wheel for sqlalchemy-utils (setup.py): started\n",
      "  Building wheel for sqlalchemy-utils (setup.py): finished with status 'done'\n",
      "  Created wheel for sqlalchemy-utils: filename=SQLAlchemy_Utils-0.36.3-py2.py3-none-any.whl size=89208 sha256=8165ac0a571cf1589269320cb65421ec7f41a9c4ad38c543b35785b094c2bdf1\n",
      "  Stored in directory: /root/.cache/pip/wheels/e2/72/f1/5c9586f19dcdf1b034fc9a3184c689ae234c7fbda5b971a3e1\n",
      "  Building wheel for idna-ssl (setup.py): started\n",
      "  Building wheel for idna-ssl (setup.py): finished with status 'done'\n",
      "  Created wheel for idna-ssl: filename=idna_ssl-1.1.0-py3-none-any.whl size=3161 sha256=0f95965c567674975d646fe55f22fc0bfaa6c35a4f95040aa199cb311371ea06\n",
      "  Stored in directory: /root/.cache/pip/wheels/6a/f5/9c/f8331a854f7a8739cf0e74c13854e4dd7b1af11b04fe1dde13\n",
      "  Building wheel for thriftpy2 (setup.py): started\n",
      "  Building wheel for thriftpy2 (setup.py): finished with status 'done'\n",
      "  Created wheel for thriftpy2: filename=thriftpy2-0.4.11-cp36-cp36m-linux_x86_64.whl size=1070008 sha256=e08353b91e469b33a4d4912de88de2e5ffd8488d89e5fa932cb5717f4c0ed9ed\n",
      "  Stored in directory: /root/.cache/pip/wheels/64/c6/25/269aea238ed60341cdca72c4a4efc161345553f50cd96fa6ee\n",
      "Successfully built BentoML contextvars sqlalchemy-utils idna-ssl thriftpy2\n",
      "\u001b[91mERROR: tensorflow 2.1.0 has requirement numpy<2.0,>=1.16.0, but you'll have numpy 1.14.2 which is incompatible.\n",
      "\u001b[0mInstalling collected packages: immutables, contextvars, sqlalchemy-utils, grpcio, async-timeout, idna-ssl, attrs, typing-extensions, multidict, yarl, aiohttp, ply, thriftpy2, py-zipkin, python-dateutil, BentoML\n",
      "  Attempting uninstall: grpcio\n",
      "    Found existing installation: grpcio 1.28.1\n",
      "    Uninstalling grpcio-1.28.1:\n",
      "      Successfully uninstalled grpcio-1.28.1\n",
      "  Attempting uninstall: python-dateutil\n",
      "    Found existing installation: python-dateutil 2.8.1\n",
      "    Uninstalling python-dateutil-2.8.1:\n",
      "      Successfully uninstalled python-dateutil-2.8.1\n",
      "  Attempting uninstall: BentoML\n",
      "    Found existing installation: BentoML 0.5.2\n",
      "    Uninstalling BentoML-0.5.2:\n",
      "      Successfully uninstalled BentoML-0.5.2\n",
      "Successfully installed BentoML-0.5.2+173.ga4278d4.dirty aiohttp-3.6.2 async-timeout-3.0.1 attrs-19.3.0 contextvars-2.4 grpcio-1.27.2 idna-ssl-1.1.0 immutables-0.12 multidict-4.7.5 ply-3.11 py-zipkin-0.20.0 python-dateutil-2.8.0 sqlalchemy-utils-0.36.3 thriftpy2-0.4.11 typing-extensions-3.7.4.2 yarl-1.4.2\n",
      "Removing intermediate container b1448dd3093d\n",
      " ---> c153392c14eb\n",
      "Step 14/15 : ENV FLAGS=\"\"\n",
      " ---> Running in 8c3b0d4a59ac\n",
      "Removing intermediate container 8c3b0d4a59ac\n",
      " ---> 17b4b7e8749e\n",
      "Step 15/15 : CMD [\"bentoml serve-gunicorn /bento $FLAGS\"]\n",
      " ---> Running in fc56a9768af2\n",
      "Removing intermediate container fc56a9768af2\n",
      " ---> 0f14aba82b37\n",
      "Successfully built 0f14aba82b37\n",
      "Successfully tagged 20200423233754_0df217:latest\n",
      "08d8ad1431f904928c96f281a3d12c6701aa060763b7fe418a8192afac97aebe\n"
     ]
    }
   ],
   "source": [
    "# Option 2: serve in docker\n",
    "!cd {bentoml_bundle_path}\n",
    "IMG_NAME = bentoml_bundle_path.split('/')[-1].lower()\n",
    "!docker build -t {IMG_NAME} {bentoml_bundle_path}\n",
    "# launch docker instances\n",
    "!docker run -itd -p {PORT}:5000 -e FLAGS=\"--workers 1 --enable-microbatch\" {IMG_NAME}:latest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the API with requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'[\"negative\"]'\n",
      "CPU times: user 4.14 ms, sys: 3.43 ms, total: 7.57 ms\n",
      "Wall time: 214 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "server_url = f\"http://127.0.0.1:{PORT}/predict\"\n",
    "method = \"POST\"\n",
    "headers = {\"content-type\": \"application/json\"}\n",
    "pred_sentences =  [\"The acting was a bit lacking.\"]\n",
    "data = pd.DataFrame(pred_sentences).to_json()\n",
    "\n",
    "r = requests.request(method, server_url, headers=headers, data=data)\n",
    "print(r.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "Movie Reviews with bert-for-tf2.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "bentoml-dev-py36",
   "language": "python",
   "name": "bentoml-dev-py36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
