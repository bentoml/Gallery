{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BentoML Example: Tensorflow 2.0 example (Echo model)\n",
    "\n",
    "[BentoML](http://bentoml.ai) is an open source platform for machine learning model serving and deployment. \n",
    "\n",
    "This notebook demonstrates how to use BentoML to turn a Tensorflow model into a docker image containing a REST API server serving this model, how to use your ML service built with BentoML as a CLI tool, and how to distribute it a pypi package.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dzLKpmZICaWN"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.0\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "print(tf.__version__)\n",
    "\n",
    "import os\n",
    "import time\n",
    "import requests\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1 samples\n",
      "WARNING:tensorflow:The list of trainable weights is empty. Make sure that you are not setting model.trainable to False before compiling the model.\n",
      "1/1 [==============================] - 0s 87ms/sample - loss: 0.0000e+00 - accuracy: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=192, shape=(4, 2, 3), dtype=float32, numpy=\n",
       "array([[[1., 1., 1.],\n",
       "        [1., 1., 1.]],\n",
       "\n",
       "       [[1., 1., 1.],\n",
       "        [1., 1., 1.]],\n",
       "\n",
       "       [[1., 1., 1.],\n",
       "        [1., 1., 1.]],\n",
       "\n",
       "       [[1., 1., 1.],\n",
       "        [1., 1., 1.]]], dtype=float32)>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class EchoModel(tf.keras.Model):\n",
    "    def call(self, x):\n",
    "        return tf.multiply(x, 1)\n",
    "\n",
    "custom_model = EchoModel()\n",
    "custom_model.compile(optimizer='sgd',\n",
    "              loss=\"mean_squared_error\",\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "test_input =  tf.constant(np.zeros([1, 2, 2]))\n",
    "test_output = tf.constant(np.zeros([1, 2, 2]))\n",
    "\n",
    "custom_model.fit(test_input, test_output, epochs=1)  # required. it will generate the signature automaticlly\n",
    "\n",
    "# test\n",
    "custom_model(tf.constant(np.ones([4, 2, 3]), dtype=tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_tensor = tf.constant(np.zeros([2,4,1]), dtype=tf.float32)\n",
    "custom_model(test_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Serve with bentoml\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting tensorflow_echo.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile tensorflow_echo.py\n",
    "\n",
    "import bentoml\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "from bentoml.artifact import (\n",
    "    TensorflowSavedModelArtifact,\n",
    ")\n",
    "from bentoml.handlers import TensorflowTensorHandler\n",
    "\n",
    "\n",
    "FASHION_MNIST_CLASSES = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
    "               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
    "\n",
    "\n",
    "@bentoml.env(pip_dependencies=['tensorflow', 'numpy', 'scikit-learn'])\n",
    "@bentoml.artifacts([TensorflowSavedModelArtifact('model')])\n",
    "class EchoServicer(bentoml.BentoService):\n",
    "\n",
    "    @bentoml.api(TensorflowTensorHandler)\n",
    "    def predict(self, tensor):\n",
    "        outputs = self.artifacts.model(tensor)\n",
    "        return outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2019-12-20 11:36:37,121] WARNING - BentoML local changes detected - Local BentoML repository including all code changes will be bundled together with the BentoService bundle. When used with docker, the base docker image will be default to same version as last PyPI release at version: 0.5.2. You can also force bentoml to use a specific version for deploying your BentoService bundle, by setting the config 'core/bentoml_deploy_version' to a pinned version or your custom BentoML on github, e.g.:'bentoml_deploy_version = git+https://github.com/{username}/bentoml.git@{branch}'\n",
      "[2019-12-20 11:36:37,122] WARNING - BentoML local changes detected - Local BentoML repository including all code changes will be bundled together with the BentoService bundle. When used with docker, the base docker image will be default to same version as last PyPI release at version: 0.5.2. You can also force bentoml to use a specific version for deploying your BentoService bundle, by setting the config 'core/bentoml_deploy_version' to a pinned version or your custom BentoML on github, e.g.:'bentoml_deploy_version = git+https://github.com/{username}/bentoml.git@{branch}'\n",
      "[2019-12-20 11:36:38,162] WARNING - BentoML local changes detected - Local BentoML repository including all code changes will be bundled together with the BentoService bundle. When used with docker, the base docker image will be default to same version as last PyPI release at version: 0.5.2. You can also force bentoml to use a specific version for deploying your BentoService bundle, by setting the config 'core/bentoml_deploy_version' to a pinned version or your custom BentoML on github, e.g.:'bentoml_deploy_version = git+https://github.com/{username}/bentoml.git@{branch}'\n",
      "INFO:tensorflow:Assets written to: /tmp/bentoml-temp-mic06xz3/EchoServicer/artifacts/model_saved_model/assets\n",
      "[2019-12-20 11:36:47,720] WARNING - BentoML local changes detected - Local BentoML repository including all code changes will be bundled together with the BentoService bundle. When used with docker, the base docker image will be default to same version as last PyPI release at version: 0.5.2. You can also force bentoml to use a specific version for deploying your BentoService bundle, by setting the config 'core/bentoml_deploy_version' to a pinned version or your custom BentoML on github, e.g.:'bentoml_deploy_version = git+https://github.com/{username}/bentoml.git@{branch}'\n",
      "running sdist\n",
      "running egg_info\n",
      "writing BentoML.egg-info/PKG-INFO\n",
      "writing dependency_links to BentoML.egg-info/dependency_links.txt\n",
      "writing entry points to BentoML.egg-info/entry_points.txt\n",
      "writing requirements to BentoML.egg-info/requires.txt\n",
      "writing top-level names to BentoML.egg-info/top_level.txt\n",
      "reading manifest file 'BentoML.egg-info/SOURCES.txt'\n",
      "reading manifest template 'MANIFEST.in'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "no previously-included directories found matching 'examples'\n",
      "no previously-included directories found matching 'tests'\n",
      "no previously-included directories found matching 'docs'\n",
      "warning: no previously-included files matching '*~' found anywhere in distribution\n",
      "warning: no previously-included files matching '*.pyo' found anywhere in distribution\n",
      "warning: no previously-included files matching '.git' found anywhere in distribution\n",
      "warning: no previously-included files matching '.ipynb_checkpoints' found anywhere in distribution\n",
      "warning: no previously-included files matching '__pycache__' found anywhere in distribution\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "writing manifest file 'BentoML.egg-info/SOURCES.txt'\n",
      "running check\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "warning: check: missing meta-data: if 'author' supplied, 'author_email' must be supplied too\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating BentoML-0.5.2+41.g2222433.dirty\n",
      "creating BentoML-0.5.2+41.g2222433.dirty/BentoML.egg-info\n",
      "creating BentoML-0.5.2+41.g2222433.dirty/bentoml\n",
      "creating BentoML-0.5.2+41.g2222433.dirty/bentoml/artifact\n",
      "creating BentoML-0.5.2+41.g2222433.dirty/bentoml/bundler\n",
      "creating BentoML-0.5.2+41.g2222433.dirty/bentoml/cli\n",
      "creating BentoML-0.5.2+41.g2222433.dirty/bentoml/clipper\n",
      "creating BentoML-0.5.2+41.g2222433.dirty/bentoml/configuration\n",
      "creating BentoML-0.5.2+41.g2222433.dirty/bentoml/deployment\n",
      "creating BentoML-0.5.2+41.g2222433.dirty/bentoml/deployment/aws_lambda\n",
      "creating BentoML-0.5.2+41.g2222433.dirty/bentoml/deployment/sagemaker\n",
      "creating BentoML-0.5.2+41.g2222433.dirty/bentoml/handlers\n",
      "creating BentoML-0.5.2+41.g2222433.dirty/bentoml/migrations\n",
      "creating BentoML-0.5.2+41.g2222433.dirty/bentoml/migrations/versions\n",
      "creating BentoML-0.5.2+41.g2222433.dirty/bentoml/proto\n",
      "creating BentoML-0.5.2+41.g2222433.dirty/bentoml/repository\n",
      "creating BentoML-0.5.2+41.g2222433.dirty/bentoml/server\n",
      "creating BentoML-0.5.2+41.g2222433.dirty/bentoml/server/static\n",
      "creating BentoML-0.5.2+41.g2222433.dirty/bentoml/utils\n",
      "creating BentoML-0.5.2+41.g2222433.dirty/bentoml/utils/validator\n",
      "creating BentoML-0.5.2+41.g2222433.dirty/bentoml/yatai\n",
      "copying files to BentoML-0.5.2+41.g2222433.dirty...\n",
      "copying LICENSE -> BentoML-0.5.2+41.g2222433.dirty\n",
      "copying MANIFEST.in -> BentoML-0.5.2+41.g2222433.dirty\n",
      "copying README.md -> BentoML-0.5.2+41.g2222433.dirty\n",
      "copying setup.cfg -> BentoML-0.5.2+41.g2222433.dirty\n",
      "copying setup.py -> BentoML-0.5.2+41.g2222433.dirty\n",
      "copying versioneer.py -> BentoML-0.5.2+41.g2222433.dirty\n",
      "copying BentoML.egg-info/PKG-INFO -> BentoML-0.5.2+41.g2222433.dirty/BentoML.egg-info\n",
      "copying BentoML.egg-info/SOURCES.txt -> BentoML-0.5.2+41.g2222433.dirty/BentoML.egg-info\n",
      "copying BentoML.egg-info/dependency_links.txt -> BentoML-0.5.2+41.g2222433.dirty/BentoML.egg-info\n",
      "copying BentoML.egg-info/entry_points.txt -> BentoML-0.5.2+41.g2222433.dirty/BentoML.egg-info\n",
      "copying BentoML.egg-info/requires.txt -> BentoML-0.5.2+41.g2222433.dirty/BentoML.egg-info\n",
      "copying BentoML.egg-info/top_level.txt -> BentoML-0.5.2+41.g2222433.dirty/BentoML.egg-info\n",
      "copying bentoml/__init__.py -> BentoML-0.5.2+41.g2222433.dirty/bentoml\n",
      "copying bentoml/_version.py -> BentoML-0.5.2+41.g2222433.dirty/bentoml\n",
      "copying bentoml/alembic.ini -> BentoML-0.5.2+41.g2222433.dirty/bentoml\n",
      "copying bentoml/db.py -> BentoML-0.5.2+41.g2222433.dirty/bentoml\n",
      "copying bentoml/exceptions.py -> BentoML-0.5.2+41.g2222433.dirty/bentoml\n",
      "copying bentoml/service.py -> BentoML-0.5.2+41.g2222433.dirty/bentoml\n",
      "copying bentoml/service_env.py -> BentoML-0.5.2+41.g2222433.dirty/bentoml\n",
      "copying bentoml/artifact/__init__.py -> BentoML-0.5.2+41.g2222433.dirty/bentoml/artifact\n",
      "copying bentoml/artifact/artifact.py -> BentoML-0.5.2+41.g2222433.dirty/bentoml/artifact\n",
      "copying bentoml/artifact/fastai_model_artifact.py -> BentoML-0.5.2+41.g2222433.dirty/bentoml/artifact\n",
      "copying bentoml/artifact/h2o_model_artifact.py -> BentoML-0.5.2+41.g2222433.dirty/bentoml/artifact\n",
      "copying bentoml/artifact/keras_model_artifact.py -> BentoML-0.5.2+41.g2222433.dirty/bentoml/artifact\n",
      "copying bentoml/artifact/lightgbm_model_artifact.py -> BentoML-0.5.2+41.g2222433.dirty/bentoml/artifact\n",
      "copying bentoml/artifact/pickle_artifact.py -> BentoML-0.5.2+41.g2222433.dirty/bentoml/artifact\n",
      "copying bentoml/artifact/pytorch_model_artifact.py -> BentoML-0.5.2+41.g2222433.dirty/bentoml/artifact\n",
      "copying bentoml/artifact/sklearn_model_artifact.py -> BentoML-0.5.2+41.g2222433.dirty/bentoml/artifact\n",
      "copying bentoml/artifact/text_file_artifact.py -> BentoML-0.5.2+41.g2222433.dirty/bentoml/artifact\n",
      "copying bentoml/artifact/tf_savedmodel_artifact.py -> BentoML-0.5.2+41.g2222433.dirty/bentoml/artifact\n",
      "copying bentoml/artifact/xgboost_model_artifact.py -> BentoML-0.5.2+41.g2222433.dirty/bentoml/artifact\n",
      "copying bentoml/bundler/__init__.py -> BentoML-0.5.2+41.g2222433.dirty/bentoml/bundler\n",
      "copying bentoml/bundler/bundler.py -> BentoML-0.5.2+41.g2222433.dirty/bentoml/bundler\n",
      "copying bentoml/bundler/config.py -> BentoML-0.5.2+41.g2222433.dirty/bentoml/bundler\n",
      "copying bentoml/bundler/loader.py -> BentoML-0.5.2+41.g2222433.dirty/bentoml/bundler\n",
      "copying bentoml/bundler/py_module_utils.py -> BentoML-0.5.2+41.g2222433.dirty/bentoml/bundler\n",
      "copying bentoml/bundler/templates.py -> BentoML-0.5.2+41.g2222433.dirty/bentoml/bundler\n",
      "copying bentoml/bundler/utils.py -> BentoML-0.5.2+41.g2222433.dirty/bentoml/bundler\n",
      "copying bentoml/cli/__init__.py -> BentoML-0.5.2+41.g2222433.dirty/bentoml/cli\n",
      "copying bentoml/cli/click_utils.py -> BentoML-0.5.2+41.g2222433.dirty/bentoml/cli\n",
      "copying bentoml/cli/config.py -> BentoML-0.5.2+41.g2222433.dirty/bentoml/cli\n",
      "copying bentoml/cli/deployment.py -> BentoML-0.5.2+41.g2222433.dirty/bentoml/cli\n",
      "copying bentoml/cli/utils.py -> BentoML-0.5.2+41.g2222433.dirty/bentoml/cli\n",
      "copying bentoml/clipper/__init__.py -> BentoML-0.5.2+41.g2222433.dirty/bentoml/clipper\n",
      "copying bentoml/configuration/__init__.py -> BentoML-0.5.2+41.g2222433.dirty/bentoml/configuration\n",
      "copying bentoml/configuration/configparser.py -> BentoML-0.5.2+41.g2222433.dirty/bentoml/configuration\n",
      "copying bentoml/configuration/default_bentoml.cfg -> BentoML-0.5.2+41.g2222433.dirty/bentoml/configuration\n",
      "copying bentoml/deployment/__init__.py -> BentoML-0.5.2+41.g2222433.dirty/bentoml/deployment\n",
      "copying bentoml/deployment/operator.py -> BentoML-0.5.2+41.g2222433.dirty/bentoml/deployment\n",
      "copying bentoml/deployment/store.py -> BentoML-0.5.2+41.g2222433.dirty/bentoml/deployment\n",
      "copying bentoml/deployment/utils.py -> BentoML-0.5.2+41.g2222433.dirty/bentoml/deployment\n",
      "copying bentoml/deployment/aws_lambda/__init__.py -> BentoML-0.5.2+41.g2222433.dirty/bentoml/deployment/aws_lambda\n",
      "copying bentoml/deployment/aws_lambda/download_extra_resources.py -> BentoML-0.5.2+41.g2222433.dirty/bentoml/deployment/aws_lambda\n",
      "copying bentoml/deployment/aws_lambda/lambda_app.py -> BentoML-0.5.2+41.g2222433.dirty/bentoml/deployment/aws_lambda\n",
      "copying bentoml/deployment/aws_lambda/utils.py -> BentoML-0.5.2+41.g2222433.dirty/bentoml/deployment/aws_lambda\n",
      "copying bentoml/deployment/sagemaker/__init__.py -> BentoML-0.5.2+41.g2222433.dirty/bentoml/deployment/sagemaker\n",
      "copying bentoml/deployment/sagemaker/templates.py -> BentoML-0.5.2+41.g2222433.dirty/bentoml/deployment/sagemaker\n",
      "copying bentoml/handlers/__init__.py -> BentoML-0.5.2+41.g2222433.dirty/bentoml/handlers\n",
      "copying bentoml/handlers/base_handlers.py -> BentoML-0.5.2+41.g2222433.dirty/bentoml/handlers\n",
      "copying bentoml/handlers/clipper_handler.py -> BentoML-0.5.2+41.g2222433.dirty/bentoml/handlers\n",
      "copying bentoml/handlers/dataframe_handler.py -> BentoML-0.5.2+41.g2222433.dirty/bentoml/handlers\n",
      "copying bentoml/handlers/fastai_image_handler.py -> BentoML-0.5.2+41.g2222433.dirty/bentoml/handlers\n",
      "copying bentoml/handlers/image_handler.py -> BentoML-0.5.2+41.g2222433.dirty/bentoml/handlers\n",
      "copying bentoml/handlers/json_handler.py -> BentoML-0.5.2+41.g2222433.dirty/bentoml/handlers\n",
      "copying bentoml/handlers/pytorch_tensor_handler.py -> BentoML-0.5.2+41.g2222433.dirty/bentoml/handlers\n",
      "copying bentoml/handlers/tensorflow_tensor_handler.py -> BentoML-0.5.2+41.g2222433.dirty/bentoml/handlers\n",
      "copying bentoml/handlers/utils.py -> BentoML-0.5.2+41.g2222433.dirty/bentoml/handlers\n",
      "copying bentoml/migrations/README -> BentoML-0.5.2+41.g2222433.dirty/bentoml/migrations\n",
      "copying bentoml/migrations/env.py -> BentoML-0.5.2+41.g2222433.dirty/bentoml/migrations\n",
      "copying bentoml/migrations/script.py.mako -> BentoML-0.5.2+41.g2222433.dirty/bentoml/migrations\n",
      "copying bentoml/migrations/versions/a6b00ae45279_add_last_updated_at_for_deployments.py -> BentoML-0.5.2+41.g2222433.dirty/bentoml/migrations/versions\n",
      "copying bentoml/proto/__init__.py -> BentoML-0.5.2+41.g2222433.dirty/bentoml/proto\n",
      "copying bentoml/proto/deployment_pb2.py -> BentoML-0.5.2+41.g2222433.dirty/bentoml/proto\n",
      "copying bentoml/proto/repository_pb2.py -> BentoML-0.5.2+41.g2222433.dirty/bentoml/proto\n",
      "copying bentoml/proto/status_pb2.py -> BentoML-0.5.2+41.g2222433.dirty/bentoml/proto\n",
      "copying bentoml/proto/yatai_service_pb2.py -> BentoML-0.5.2+41.g2222433.dirty/bentoml/proto\n",
      "copying bentoml/proto/yatai_service_pb2_grpc.py -> BentoML-0.5.2+41.g2222433.dirty/bentoml/proto\n",
      "copying bentoml/repository/__init__.py -> BentoML-0.5.2+41.g2222433.dirty/bentoml/repository\n",
      "copying bentoml/repository/metadata_store.py -> BentoML-0.5.2+41.g2222433.dirty/bentoml/repository\n",
      "copying bentoml/server/__init__.py -> BentoML-0.5.2+41.g2222433.dirty/bentoml/server\n",
      "copying bentoml/server/bento_api_server.py -> BentoML-0.5.2+41.g2222433.dirty/bentoml/server\n",
      "copying bentoml/server/bento_sagemaker_server.py -> BentoML-0.5.2+41.g2222433.dirty/bentoml/server\n",
      "copying bentoml/server/gunicorn_config.py -> BentoML-0.5.2+41.g2222433.dirty/bentoml/server\n",
      "copying bentoml/server/gunicorn_server.py -> BentoML-0.5.2+41.g2222433.dirty/bentoml/server\n",
      "copying bentoml/server/middlewares.py -> BentoML-0.5.2+41.g2222433.dirty/bentoml/server\n",
      "copying bentoml/server/utils.py -> BentoML-0.5.2+41.g2222433.dirty/bentoml/server\n",
      "copying bentoml/server/static/swagger-ui-bundle.js -> BentoML-0.5.2+41.g2222433.dirty/bentoml/server/static\n",
      "copying bentoml/server/static/swagger-ui.css -> BentoML-0.5.2+41.g2222433.dirty/bentoml/server/static\n",
      "copying bentoml/utils/__init__.py -> BentoML-0.5.2+41.g2222433.dirty/bentoml/utils\n",
      "copying bentoml/utils/cloudpickle.py -> BentoML-0.5.2+41.g2222433.dirty/bentoml/utils\n",
      "copying bentoml/utils/hybirdmethod.py -> BentoML-0.5.2+41.g2222433.dirty/bentoml/utils\n",
      "copying bentoml/utils/log.py -> BentoML-0.5.2+41.g2222433.dirty/bentoml/utils\n",
      "copying bentoml/utils/s3.py -> BentoML-0.5.2+41.g2222433.dirty/bentoml/utils\n",
      "copying bentoml/utils/tempdir.py -> BentoML-0.5.2+41.g2222433.dirty/bentoml/utils\n",
      "copying bentoml/utils/usage_stats.py -> BentoML-0.5.2+41.g2222433.dirty/bentoml/utils\n",
      "copying bentoml/utils/validator/__init__.py -> BentoML-0.5.2+41.g2222433.dirty/bentoml/utils/validator\n",
      "copying bentoml/yatai/__init__.py -> BentoML-0.5.2+41.g2222433.dirty/bentoml/yatai\n",
      "copying bentoml/yatai/deployment_utils.py -> BentoML-0.5.2+41.g2222433.dirty/bentoml/yatai\n",
      "copying bentoml/yatai/python_api.py -> BentoML-0.5.2+41.g2222433.dirty/bentoml/yatai\n",
      "copying bentoml/yatai/status.py -> BentoML-0.5.2+41.g2222433.dirty/bentoml/yatai\n",
      "copying bentoml/yatai/yatai_service_impl.py -> BentoML-0.5.2+41.g2222433.dirty/bentoml/yatai\n",
      "Writing BentoML-0.5.2+41.g2222433.dirty/setup.cfg\n",
      "UPDATING BentoML-0.5.2+41.g2222433.dirty/bentoml/_version.py\n",
      "set BentoML-0.5.2+41.g2222433.dirty/bentoml/_version.py to '0.5.2+41.g2222433.dirty'\n",
      "Creating tar archive\n",
      "removing 'BentoML-0.5.2+41.g2222433.dirty' (and everything under it)\n",
      "[2019-12-20 11:36:48,370] INFO - BentoService bundle 'EchoServicer:20191220113637_F25E91' created at: /tmp/bentoml-temp-mic06xz3\n",
      "[2019-12-20 11:36:48,371] WARNING - BentoML local changes detected - Local BentoML repository including all code changes will be bundled together with the BentoService bundle. When used with docker, the base docker image will be default to same version as last PyPI release at version: 0.5.2. You can also force bentoml to use a specific version for deploying your BentoService bundle, by setting the config 'core/bentoml_deploy_version' to a pinned version or your custom BentoML on github, e.g.:'bentoml_deploy_version = git+https://github.com/{username}/bentoml.git@{branch}'\n",
      "[2019-12-20 11:36:48,381] WARNING - Saved BentoService bundle version mismatch: loading BentoServie bundle create with BentoML version 0.5.2,  but loading from BentoML version 0.5.2+41.g2222433.dirty\n",
      "[2019-12-20 11:36:48,555] INFO - BentoService bundle 'EchoServicer:20191220113637_F25E91' created at: /home/bentoml/bentoml/repository/EchoServicer/20191220113637_F25E91\n"
     ]
    }
   ],
   "source": [
    "# save model\n",
    "from tensorflow_echo import EchoServicer\n",
    "bento_svc = EchoServicer()\n",
    "bento_svc.pack(\"model\", custom_model)\n",
    "saved_path = bento_svc.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test packed BentoML service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=599, shape=(3,), dtype=int32, numpy=array([1, 2, 3], dtype=int32)>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bento_svc.predict([1, 2, 3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run REST API server locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2019-12-20 11:40:23,757] WARNING - BentoML local changes detected - Local BentoML repository including all code changes will be bundled together with the BentoService bundle. When used with docker, the base docker image will be default to same version as last PyPI release at version: 0.5.2. You can also force bentoml to use a specific version for deploying your BentoService bundle, by setting the config 'core/bentoml_deploy_version' to a pinned version or your custom BentoML on github, e.g.:'bentoml_deploy_version = git+https://github.com/{username}/bentoml.git@{branch}'\n",
      "[2019-12-20 11:40:23,768] WARNING - Saved BentoService bundle version mismatch: loading BentoServie bundle create with BentoML version 0.5.2,  but loading from BentoML version 0.5.2+41.g2222433.dirty\n",
      "[2019-12-20 11:40:24,852] WARNING - BentoML local changes detected - Local BentoML repository including all code changes will be bundled together with the BentoService bundle. When used with docker, the base docker image will be default to same version as last PyPI release at version: 0.5.2. You can also force bentoml to use a specific version for deploying your BentoService bundle, by setting the config 'core/bentoml_deploy_version' to a pinned version or your custom BentoML on github, e.g.:'bentoml_deploy_version = git+https://github.com/{username}/bentoml.git@{branch}'\n",
      "2019-12-20 11:40:24.857641: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instructions in performance critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2019-12-20 11:40:24.879917: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2712000000 Hz\n",
      "2019-12-20 11:40:24.880228: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55f9480ba9c0 executing computations on platform Host. Devices:\n",
      "2019-12-20 11:40:24.880265: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): Host, Default Version\n",
      "2019-12-20 11:40:24.880511: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n",
      "[2019-12-20 11:40:24,910] WARNING - BentoML local changes detected - Local BentoML repository including all code changes will be bundled together with the BentoService bundle. When used with docker, the base docker image will be default to same version as last PyPI release at version: 0.5.2. You can also force bentoml to use a specific version for deploying your BentoService bundle, by setting the config 'core/bentoml_deploy_version' to a pinned version or your custom BentoML on github, e.g.:'bentoml_deploy_version = git+https://github.com/{username}/bentoml.git@{branch}'\n",
      " * Serving Flask app \"EchoServicer\" (lazy loading)\n",
      " * Environment: production\n",
      "\u001b[31m   WARNING: This is a development server. Do not use it in a production deployment.\u001b[0m\n",
      "\u001b[2m   Use a production WSGI server instead.\u001b[0m\n",
      " * Debug mode: off\n",
      " * Running on http://127.0.0.1:5000/ (Press CTRL+C to quit)\n",
      "^C\n"
     ]
    }
   ],
   "source": [
    "!bentoml serve {saved_path}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Send prediction request to REST API server\n",
    "\n",
    "*Run the following command in terminal to make a HTTP request to the API server*\n",
    "```bash\n",
    "curl -i \\\n",
    "--header \"Content-Type: application/json\" \\\n",
    "--request POST \\\n",
    "--data '{\"instances\": [[1, 2]]}' \\\n",
    "localhost:5000/predict\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data: {\"instances\": [[1, 2, 2, 3], [2, 3, 3, 4]]} ... , 3, 4]]}\n",
      "<Response [200]>\n",
      "{\"predictions\": [[1, 2, 2, 3], [2, 3, 3, 4]]}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "headers = {\"content-type\": \"application/json\"}\n",
    "data = json.dumps(\n",
    "    {\"instances\": [[1, 2, 2, 3], [2, 3, 3, 4]]}\n",
    ")\n",
    "print('Data: {} ... {}'.format(data[:50], data[len(data)-52:]))\n",
    "json_response = requests.post(f'http://127.0.0.1:5000/predict', data=data, headers=headers)\n",
    "print(json_response)\n",
    "print(json_response.text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \"pip install\" a BentoML archive\n",
    "\n",
    "BentoML user can directly pip install saved BentoML archive with `pip install $SAVED_PATH`,  and use it as a regular python package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install {saved_path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import EchoServicer\n",
    "\n",
    "installed_svc = EchoServicer.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1]], dtype=int32)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "installed_svc.predict({ 'instances': [[1, 2] })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CLI access\n",
    "\n",
    "`pip install $SAVED_PATH` also installs a CLI tool for accessing the BentoML service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!EchoServicer --help"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print model service information:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!EchoServicer info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run 'predict' api with json data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!EchoServicer predict --input='{\"instances\": [[1, 2]]}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additional: Serve with tf-serving\n",
    "----\n",
    "Bentoml TensorFlow handler and artifact is following the API of tensorflow-serving REST API.  \n",
    "To install tensorflow-serving, see: https://www.tensorflow.org/tfx/serving/setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/test-echo-model/2/assets\n",
      "2019-12-20 12:03:01.458521: I tensorflow_serving/model_servers/server.cc:85] Building single TensorFlow model file config:  model_name: echo_model model_base_path: /tmp/test-echo-model\n",
      "2019-12-20 12:03:01.458658: I tensorflow_serving/model_servers/server_core.cc:462] Adding/updating models.\n",
      "2019-12-20 12:03:01.458673: I tensorflow_serving/model_servers/server_core.cc:573]  (Re-)adding model: echo_model\n",
      "2019-12-20 12:03:01.559267: I tensorflow_serving/core/basic_manager.cc:739] Successfully reserved resources to load servable {name: echo_model version: 2}\n",
      "2019-12-20 12:03:01.559323: I tensorflow_serving/core/loader_harness.cc:66] Approving load for servable version {name: echo_model version: 2}\n",
      "2019-12-20 12:03:01.559349: I tensorflow_serving/core/loader_harness.cc:74] Loading servable version {name: echo_model version: 2}\n",
      "2019-12-20 12:03:01.559384: I external/org_tensorflow/tensorflow/cc/saved_model/reader.cc:31] Reading SavedModel from: /tmp/test-echo-model/2\n",
      "2019-12-20 12:03:01.560368: I external/org_tensorflow/tensorflow/cc/saved_model/reader.cc:54] Reading meta graph with tags { serve }\n",
      "2019-12-20 12:03:01.561479: I external/org_tensorflow/tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "2019-12-20 12:03:01.597759: I external/org_tensorflow/tensorflow/cc/saved_model/loader.cc:202] Restoring SavedModel bundle.\n",
      "2019-12-20 12:03:01.607093: I external/org_tensorflow/tensorflow/cc/saved_model/loader.cc:151] Running initialization op on SavedModel bundle at path: /tmp/test-echo-model/2\n",
      "2019-12-20 12:03:01.609348: I external/org_tensorflow/tensorflow/cc/saved_model/loader.cc:311] SavedModel load for tags { serve }; Status: success. Took 49959 microseconds.\n",
      "2019-12-20 12:03:01.609474: I tensorflow_serving/servables/tensorflow/saved_model_warmup.cc:105] No warmup data file found at /tmp/test-echo-model/2/assets.extra/tf_serving_warmup_requests\n",
      "2019-12-20 12:03:01.609596: I tensorflow_serving/core/loader_harness.cc:87] Successfully loaded servable version {name: echo_model version: 2}\n",
      "2019-12-20 12:03:01.610932: I tensorflow_serving/model_servers/server.cc:353] Running gRPC ModelServer at 0.0.0.0:8500 ...\n",
      "[evhttp_server.cc : 223] NET_LOG: Couldn't bind to port 5001\n",
      "[evhttp_server.cc : 63] NET_LOG: Serer has not been terminated. Force termination now.\n",
      "[evhttp_server.cc : 258] NET_LOG: Server is not running ...\n",
      "2019-12-20 12:03:01.612138: E tensorflow_serving/model_servers/server.cc:375] Failed to start HTTP Server at localhost:5001\n",
      "^C\n"
     ]
    }
   ],
   "source": [
    "TMP_MODEL_DIR = \"/tmp/test-echo-model\"\n",
    "TMP_MODEL_VERSION = \"1\"\n",
    "TMP_MODEL_DIR_V = f\"{TMP_MODEL_DIR}/{TMP_MODEL_VERSION}\"\n",
    "MODEL_NAME = \"echo_model\"\n",
    "\n",
    "tf.saved_model.save(custom_model, TMP_MODEL_DIR_V)\n",
    "!tensorflow_model_server --rest_api_port=5001 --model_name={MODEL_NAME} --model_base_path={TMP_MODEL_DIR}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data: {\"instances\": [[1, 2, 2, 3], [2, 3, 3, 4]]} ... , 3, 4]]}\n",
      "<Response [200]>\n",
      "{\n",
      "    \"predictions\": [[1.0, 2.0, 2.0, 3.0], [2.0, 3.0, 3.0, 4.0]\n",
      "    ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "TMP_MODEL_DIR = \"/tmp/test-echo-model\"\n",
    "TMP_MODEL_VERSION = \"1\"\n",
    "TMP_MODEL_DIR_V = f\"{TMP_MODEL_DIR}/{TMP_MODEL_VERSION}\"\n",
    "MODEL_NAME = \"echo_model\"\n",
    "headers = {\"content-type\": \"application/json\"}\n",
    "data = json.dumps(\n",
    "    {\"instances\": [[1, 2, 2, 3], [2, 3, 3, 4]]}\n",
    ")\n",
    "print('Data: {} ... {}'.format(data[:50], data[len(data)-52:]))\n",
    "json_response = requests.post(f'http://127.0.0.1:5001/v{TMP_MODEL_VERSION}/models/{MODEL_NAME}:predict',\n",
    "                              data=data, headers=headers)\n",
    "print(json_response)\n",
    "print(json_response.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "classification.ipynb",
   "private_outputs": true,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "bentoml-dev",
   "language": "python",
   "name": "bentoml-dev"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
