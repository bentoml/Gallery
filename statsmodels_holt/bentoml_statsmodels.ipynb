{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploying Time-Series Statistical Model on BEntoML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step of creating a prediction service with BentoML, is to write a prediction service class inheriting from bentoml.BentoService, and specify the required model artifacts, PyPI dependencies and write the service API function. Here is a minimal prediction service definition with BentoML:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing holt.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile holt.py\n",
    "\n",
    "# holt.py\n",
    "from bentoml import env, artifacts, api, BentoService\n",
    "from bentoml.handlers import DataframeHandler,DataframeInput\n",
    "from bentoml.artifact import PickleArtifact\n",
    "import numpy as np\n",
    "\n",
    "@env(pip_dependencies=[\"statsmodels==0.10.1\",\"joblib\",\"numpy\"],conda_dependencies=[\"ruamel.yaml==0.16\"])\n",
    "\n",
    "@artifacts([PickleArtifact('model')])\n",
    "class holt(BentoService):\n",
    "    @api(input=DataframeInput())\n",
    "    def predict(self, df):\n",
    "\n",
    "        # Printing the dataframe to cross-cjheck\n",
    "        print(df.head())\n",
    "\n",
    "        # Parsing the dataframe values\n",
    "        weeks=int(df.iat[0,0])\n",
    "        print(weeks)\n",
    "        return((self.artifacts.model).forecast(weeks))\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The bentoml.api decorator defines a service API, which is the entry point for accessing the prediction service. The DataframeInput here denotes that this service API will convert HTTP JSON request into pandas.DataFrame object before passing it to the user-defined API function code for inference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pip_dependencies specify the libraries that you would need in your code. Any library specified here will automatically get added to the requirements.txt folder . "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we're using the PickleArtifact. However,  BentoML also provide model artifact for other frameworks such as PytorchModelArtifact, KerasModelArtifact, FastaiModelArtifact, and XgboostModelArtifact etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code trains a scikit-learn model and bundles the trained model with an Holt instance. The Holt instance is then saved to disk in the BentoML SavedBundle format, which is a versioned file archive that is ready for production models serving deployment,we've considered a shampoo sales data which is available publicly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing main.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile main.py\n",
    "\n",
    "\n",
    "#main.py\n",
    "\n",
    "import pandas as pd\n",
    "from statsmodels.tsa.holtwinters import ExponentialSmoothing\n",
    "import numpy as np\n",
    "import joblib\n",
    "from holt import holt\n",
    "\n",
    "df=pd.read_csv('https://raw.githubusercontent.com/jbrownlee/Datasets/master/shampoo.csv')\n",
    "\n",
    "#Taking a test-train split of 80 %\n",
    "train=df[0:int(len(df)*0.8)] \n",
    "test=df[int(len(df)*0.8):]\n",
    "\n",
    "#Pre-processing the  Month  field\n",
    "train.Timestamp = pd.to_datetime(train.Month,format='%m-%d') \n",
    "train.index = train.Timestamp \n",
    "test.Timestamp = pd.to_datetime(test.Month,format='%m-%d') \n",
    "test.index = test.Timestamp \n",
    "\n",
    "#fitting the model based on  optimal parameters\n",
    "model = ExponentialSmoothing(np.asarray(train['Sales']) ,seasonal_periods=7 ,trend='add', seasonal='add',).fit()\n",
    "\n",
    "#creating an instance of the holt class\n",
    "holt_obj = holt()\n",
    "\n",
    "\n",
    "# Pack the newly trained model artifact\n",
    "holt_obj.pack('model', model)\n",
    "saved_path = holt_obj.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can start a REST API server by specifying the BentoServiceâ€™s name and version, or provide the file path to the saved bundle:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!bentoml serve holt:latest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The IrisClassifier model is now served at localhost:5000,replace 2 with the number of weeks fow which you want a prediction  . Use curl command to send a prediction request:\n",
    "\n",
    "curl -i \\\n",
    "  --header \"Content-Type: application/json\" \\\n",
    "  --request POST \\\n",
    "  --data '[[2]]' \\\n",
    "  http://localhost:5000/predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BentoML provides a convenient way to containerize the model API server with Docker:\n",
    "\n",
    "1.Find the SavedBundle directory with bentoml get command\n",
    "\n",
    "2.Run docker build with the SavedBundle directory which contains a generated Dockerfile\n",
    "\n",
    "3.Run the generated docker image to start a docker container serving the model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!saved_path=$(bentoml get IrisClassifier:latest -q | jq -r \".uri.uri\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker build -t {docker_username}/iris-classifier $saved_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker run -p 5000:5000 -e BENTOML_ENABLE_MICROBATCH=True {docker_username}/iris-classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can check out more on deploying the model on cluster at:<br>\n",
    "1.https://docs.bentoml.org/en/stable/quickstart.html#deploy-api-server-to-the-cloud<br>\n",
    "2.https://www.kubeflow.org/docs/components/serving/bentoml/ (if you have kubeflow installed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
