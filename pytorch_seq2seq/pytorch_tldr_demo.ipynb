{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a682ea0b",
   "metadata": {},
   "source": [
    "# BentoML PyTorch TLDR Tutorial\n",
    "\n",
    "Link to source code: https://github.com/bentoml/gallery/tree/main/TLDR/pytorch_tldr_demo/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c8ce1c9",
   "metadata": {},
   "source": [
    "Install required dependencies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3ad00863",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m pip install --quiet -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fb49fd0",
   "metadata": {},
   "source": [
    "Import required libraries:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45393b74",
   "metadata": {},
   "source": [
    "## Download and Extract Reddit TL;DR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5e227583",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from os.path import exists\n",
    "import zipfile\n",
    "import json\n",
    "from tqdm import trange, tqdm\n",
    "\n",
    "from __future__ import unicode_literals, print_function, division\n",
    "from io import open\n",
    "import unicodedata\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "070965aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = 'dataset/corpus-webis-tldr-17.zip'\n",
    "\n",
    "if not exists(dataset_path):\n",
    "    r = requests.get('https://zenodo.org/record/1043504/files/corpus-webis-tldr-17.zip?download=1')\n",
    "    with open(dataset_path, 'wb') as f:\n",
    "        f.write(r.content)\n",
    "    with zipfile.ZipFile(dataset_path, 'r') as z:\n",
    "        z.extractall('dataset/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d7397d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_json('dataset/corpus-webis-tldr-17.json', \\\n",
    "                  lines=True, chunksize=100000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "320ed520",
   "metadata": {},
   "source": [
    "## Begin processing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cbbd3118",
   "metadata": {},
   "outputs": [],
   "source": [
    "SOS_token = 0\n",
    "EOS_token = 1\n",
    "\n",
    "\n",
    "class Lang:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {0: \"SOS\", 1: \"EOS\"}\n",
    "        self.n_words = 2  # Count SOS and EOS\n",
    "\n",
    "    def addSentence(self, sentence):\n",
    "        for word in sentence.split(' '):\n",
    "            self.addWord(word)\n",
    "\n",
    "    def addWord(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.n_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.n_words] = word\n",
    "            self.n_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1\n",
    "\n",
    "# Turn a Unicode string to plain ASCII, thanks to\n",
    "# https://stackoverflow.com/a/518232\n",
    "def unicodeToAscii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "    )\n",
    "\n",
    "# Lowercase, trim, and remove non-letter characters\n",
    "def normalizeString(s):\n",
    "    s = unicodeToAscii(s.lower().strip())\n",
    "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
    "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9e854567",
   "metadata": {},
   "outputs": [],
   "source": [
    "def readLangs(lang1, lang2, reverse=False):\n",
    "    print(\"Reading lines...\")\n",
    "    \n",
    "    lines = next(df)\n",
    "\n",
    "    # Split every line into pairs and normalize\n",
    "    pairs = [[normalizeString(s) for s in l] for l in \\\n",
    "             tqdm(zip(lines['content'], lines['summary']))]\n",
    "\n",
    "    # Reverse pairs, make Lang instances\n",
    "    if reverse:\n",
    "        pairs = [list(reversed(p)) for p in pairs]\n",
    "        input_lang = Lang(lang2)\n",
    "        output_lang = Lang(lang1)\n",
    "    else:\n",
    "        input_lang = Lang(lang1)\n",
    "        output_lang = Lang(lang2)\n",
    "\n",
    "    return input_lang, output_lang, pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2acc9a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = 260\n",
    "\n",
    "eng_prefixes = (\n",
    "    \"i am \", \"i m \",\n",
    "    \"he is\", \"he s \",\n",
    "    \"she is\", \"she s \",\n",
    "    \"you are\", \"you re \",\n",
    "    \"we are\", \"we re \",\n",
    "    \"they are\", \"they re \",\n",
    "    \n",
    ")\n",
    "\n",
    "\n",
    "def filterPair(p):\n",
    "    return len(p[0].split(' ')) < MAX_LENGTH and \\\n",
    "        len(p[1].split(' ')) < MAX_LENGTH and \\\n",
    "        p[1].startswith(eng_prefixes)\n",
    "\n",
    "\n",
    "def filterPairs(pairs):\n",
    "    return [pair for pair in pairs if filterPair(pair)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "baa20f98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading lines...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100000it [00:35, 2846.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read 100000 sentence pairs\n",
      "Trimmed to 2841 sentence pairs\n",
      "Counting words...\n",
      "Counted words:\n",
      "content 19584\n",
      "summary 6463\n",
      "['i am going to expand on your comment to clarify it is a combination of both but depending on where you are coming from changes how you talk about it . every single top player knows the odds and how to make the right decisions based on the math . but if you simply fallow this and everyone else is then you are all just leaving it to chance . so at a pro level picking the right times to push your odds based on how you are reading the other players sets you apart . in a game with non professionals most don t really understand the math . so knowing these things gives you an edge because they think it is all about the players . and they will generally try and overdo it and they ll put themselves out of the game . on the flip side it is hard to bluff people when they think their shit hand is good and similarly it is hard to semi bluff when they don t see the chance of failure .', 'you are both right and wrong .']\n"
     ]
    }
   ],
   "source": [
    "def prepareData(lang1, lang2, reverse=False):\n",
    "    input_lang, output_lang, pairs = readLangs(lang1, lang2, reverse)\n",
    "    print(\"Read %s sentence pairs\" % len(pairs))\n",
    "    pairs = filterPairs(pairs)\n",
    "    print(\"Trimmed to %s sentence pairs\" % len(pairs))\n",
    "    print(\"Counting words...\")\n",
    "    for pair in pairs:\n",
    "        input_lang.addSentence(pair[0])\n",
    "        output_lang.addSentence(pair[1])\n",
    "    print(\"Counted words:\")\n",
    "    print(input_lang.name, input_lang.n_words)\n",
    "    print(output_lang.name, output_lang.n_words)\n",
    "    return input_lang, output_lang, pairs\n",
    "\n",
    "\n",
    "input_lang, output_lang, pairs = prepareData('content', 'summary')\n",
    "print(random.choice(pairs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59d8209c",
   "metadata": {},
   "source": [
    "## Define the model\n",
    "\n",
    "First let's define a simple PyTorch network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f46fd8b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        embedded = self.embedding(input).view(1, 1, -1)\n",
    "        output = embedded\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "05e42d08",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        output = self.embedding(input).view(1, 1, -1)\n",
    "        output = F.relu(output)\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        output = self.softmax(self.out(output[0]))\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "47139511",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttnDecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, dropout_p=0.1, max_length=MAX_LENGTH):\n",
    "        super(AttnDecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.dropout_p = dropout_p\n",
    "        self.max_length = max_length\n",
    "\n",
    "        self.embedding = nn.Embedding(self.output_size, self.hidden_size)\n",
    "        self.attn = nn.Linear(self.hidden_size * 2, self.max_length)\n",
    "        self.attn_combine = nn.Linear(self.hidden_size * 2, self.hidden_size)\n",
    "        self.dropout = nn.Dropout(self.dropout_p)\n",
    "        self.gru = nn.GRU(self.hidden_size, self.hidden_size)\n",
    "        self.out = nn.Linear(self.hidden_size, self.output_size)\n",
    "\n",
    "    def forward(self, input, hidden, encoder_outputs):\n",
    "        embedded = self.embedding(input).view(1, 1, -1)\n",
    "        embedded = self.dropout(embedded)\n",
    "\n",
    "        attn_weights = F.softmax(\n",
    "            self.attn(torch.cat((embedded[0], hidden[0]), 1)), dim=1)\n",
    "        attn_applied = torch.bmm(attn_weights.unsqueeze(0),\n",
    "                                 encoder_outputs.unsqueeze(0))\n",
    "\n",
    "        output = torch.cat((embedded[0], attn_applied[0]), 1)\n",
    "        output = self.attn_combine(output).unsqueeze(0)\n",
    "\n",
    "        output = F.relu(output)\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "\n",
    "        output = F.log_softmax(self.out(output[0]), dim=1)\n",
    "        return output, hidden, attn_weights\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38888f0a",
   "metadata": {},
   "source": [
    "## Training and Saving the model\n",
    "\n",
    "Then we define a simple PyTorch network and some helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1bcda1bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def indexesFromSentence(lang, sentence):\n",
    "    indices = []\n",
    "    for word in sentence.split(' '):\n",
    "        if word in lang.word2index:\n",
    "            # lang.addWord(word)\n",
    "            indices.append(lang.word2index[word])\n",
    "    return indices\n",
    "\n",
    "\n",
    "def tensorFromSentence(lang, sentence):\n",
    "    indexes = indexesFromSentence(lang, sentence)\n",
    "    indexes.append(EOS_token)\n",
    "    return torch.tensor(indexes, dtype=torch.long, device=device).view(-1, 1)\n",
    "\n",
    "\n",
    "def tensorsFromPair(pair):\n",
    "    input_tensor = tensorFromSentence(input_lang, pair[0])\n",
    "    target_tensor = tensorFromSentence(output_lang, pair[1])\n",
    "    return (input_tensor, target_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f2f334de",
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher_forcing_ratio = 0.5\n",
    "\n",
    "\n",
    "def train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length=MAX_LENGTH):\n",
    "    encoder_hidden = encoder.initHidden()\n",
    "\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "\n",
    "    input_length = input_tensor.size(0)\n",
    "    target_length = target_tensor.size(0)\n",
    "\n",
    "    encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
    "\n",
    "    loss = 0\n",
    "\n",
    "    for ei in range(input_length):\n",
    "        encoder_output, encoder_hidden = encoder(\n",
    "            input_tensor[ei], encoder_hidden)\n",
    "        encoder_outputs[ei] = encoder_output[0, 0]\n",
    "\n",
    "    decoder_input = torch.tensor([[SOS_token]], device=device)\n",
    "\n",
    "    decoder_hidden = encoder_hidden\n",
    "\n",
    "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
    "\n",
    "    if use_teacher_forcing:\n",
    "        # Teacher forcing: Feed the target as the next input\n",
    "        for di in range(target_length):\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs)\n",
    "            loss += criterion(decoder_output, target_tensor[di])\n",
    "            decoder_input = target_tensor[di]  # Teacher forcing\n",
    "\n",
    "    else:\n",
    "        # Without teacher forcing: use its own predictions as the next input\n",
    "        for di in range(target_length):\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs)\n",
    "            topv, topi = decoder_output.topk(1)\n",
    "            decoder_input = topi.squeeze().detach()  # detach from history as input\n",
    "\n",
    "            loss += criterion(decoder_output, target_tensor[di])\n",
    "            if decoder_input.item() == EOS_token:\n",
    "                break\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    return loss.item() / target_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a1d10bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "\n",
    "\n",
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d4a87c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainIters(encoder, decoder, n_iters, print_every=1000, plot_every=100, learning_rate=0.01):\n",
    "    start = time.time()\n",
    "    plot_losses = []\n",
    "    print_loss_total = 0  # Reset every print_every\n",
    "    plot_loss_total = 0  # Reset every plot_every\n",
    "\n",
    "    encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate)\n",
    "    decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate)\n",
    "    training_pairs = [tensorsFromPair(random.choice(pairs))\n",
    "                      for i in range(n_iters)]\n",
    "    criterion = nn.NLLLoss()\n",
    "\n",
    "    for iter in range(1, n_iters + 1):\n",
    "        training_pair = training_pairs[iter - 1]\n",
    "        input_tensor = training_pair[0]\n",
    "        target_tensor = training_pair[1]\n",
    "\n",
    "        loss = train(input_tensor, target_tensor, encoder,\n",
    "                     decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
    "        print_loss_total += loss\n",
    "        plot_loss_total += loss\n",
    "\n",
    "        if iter % print_every == 0:\n",
    "            print_loss_avg = print_loss_total / print_every\n",
    "            print_loss_total = 0\n",
    "            print('%s (%d %d%%) %.4f' % (timeSince(start, iter / n_iters),\n",
    "                                         iter, iter / n_iters * 100, print_loss_avg))\n",
    "            \n",
    "            plot_loss_avg = plot_loss_total / plot_every\n",
    "            plot_losses.append(plot_loss_avg)\n",
    "            plot_loss_total = 0\n",
    "            \n",
    "            # input_lang, output_lang, pairs_2 = prepareData('content', 'summary')\n",
    "            # training_pairs = [tensorsFromPair(random.choice(pairs_2))\n",
    "            #       for i in range(n_iters)]\n",
    "\n",
    "    showPlot(plot_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "788c19a0",
   "metadata": {},
   "source": [
    "### Cross Validation\n",
    "\n",
    "We can do some cross validation and the results can be saved with the model as metadata\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bd96a260",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(encoder, decoder, sentence, max_length=MAX_LENGTH):\n",
    "    with torch.no_grad():\n",
    "        input_tensor = tensorFromSentence(input_lang, sentence)\n",
    "        input_length = input_tensor.size()[0]\n",
    "        encoder_hidden = encoder.initHidden()\n",
    "\n",
    "        encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
    "\n",
    "        for ei in range(input_length):\n",
    "            encoder_output, encoder_hidden = encoder(input_tensor[ei],\n",
    "                                                     encoder_hidden)\n",
    "            encoder_outputs[ei] += encoder_output[0, 0]\n",
    "\n",
    "        decoder_input = torch.tensor([[SOS_token]], device=device)  # SOS\n",
    "\n",
    "        decoder_hidden = encoder_hidden\n",
    "\n",
    "        decoded_words = []\n",
    "        decoder_attentions = torch.zeros(max_length, max_length)\n",
    "\n",
    "        for di in range(max_length):\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs)\n",
    "            decoder_attentions[di] = decoder_attention.data\n",
    "            topv, topi = decoder_output.data.topk(1)\n",
    "            if topi.item() == EOS_token:\n",
    "                decoded_words.append('<EOS>')\n",
    "                break\n",
    "            else:\n",
    "                decoded_words.append(output_lang.index2word[topi.item()])\n",
    "\n",
    "            decoder_input = topi.squeeze().detach()\n",
    "\n",
    "        return decoded_words, decoder_attentions[:di + 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "521e4203",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluateRandomly(encoder, decoder, n=10):\n",
    "    for i in range(n):\n",
    "        pair = random.choice(pairs)\n",
    "        print('>', pair[0])\n",
    "        print('=', pair[1])\n",
    "        output_words, attentions = evaluate(encoder, decoder, pair[0])\n",
    "        output_sentence = ' '.join(output_words)\n",
    "        print('<', output_sentence)\n",
    "        print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "50b1f0dd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1m 58s (- 116m 41s) (100 1%) 5.4868\n",
      "3m 59s (- 115m 53s) (200 3%) 4.7123\n",
      "5m 48s (- 110m 24s) (300 5%) 4.1829\n",
      "7m 50s (- 109m 43s) (400 6%) 3.7229\n",
      "9m 38s (- 106m 3s) (500 8%) 4.0344\n",
      "11m 38s (- 104m 44s) (600 10%) 4.1306\n",
      "13m 35s (- 102m 56s) (700 11%) 3.5692\n",
      "15m 58s (- 103m 51s) (800 13%) 3.3792\n",
      "18m 3s (- 102m 20s) (900 15%) 3.8816\n",
      "20m 7s (- 100m 36s) (1000 16%) 3.9263\n",
      "22m 8s (- 98m 35s) (1100 18%) 3.7119\n",
      "24m 4s (- 96m 16s) (1200 20%) 3.6045\n",
      "26m 2s (- 94m 10s) (1300 21%) 3.6248\n",
      "27m 53s (- 91m 37s) (1400 23%) 3.8138\n",
      "29m 57s (- 89m 51s) (1500 25%) 3.9532\n",
      "31m 44s (- 87m 16s) (1600 26%) 3.8311\n",
      "33m 42s (- 85m 16s) (1700 28%) 3.8689\n",
      "35m 42s (- 83m 19s) (1800 30%) 3.2818\n",
      "37m 38s (- 81m 12s) (1900 31%) 3.4302\n",
      "39m 44s (- 79m 29s) (2000 33%) 3.6502\n",
      "41m 49s (- 77m 39s) (2100 35%) 3.5135\n",
      "43m 56s (- 75m 54s) (2200 36%) 4.0707\n",
      "46m 5s (- 74m 8s) (2300 38%) 3.7621\n",
      "48m 2s (- 72m 4s) (2400 40%) 3.6728\n",
      "50m 20s (- 70m 28s) (2500 41%) 3.6468\n",
      "52m 36s (- 68m 47s) (2600 43%) 3.6676\n",
      "54m 47s (- 66m 57s) (2700 45%) 3.6588\n",
      "57m 4s (- 65m 13s) (2800 46%) 3.7077\n",
      "59m 0s (- 63m 4s) (2900 48%) 3.3002\n",
      "61m 3s (- 61m 3s) (3000 50%) 3.5600\n",
      "63m 22s (- 59m 17s) (3100 51%) 3.5013\n",
      "65m 33s (- 57m 22s) (3200 53%) 3.5350\n",
      "67m 39s (- 55m 21s) (3300 55%) 3.5401\n",
      "69m 56s (- 53m 29s) (3400 56%) 3.3220\n",
      "72m 7s (- 51m 31s) (3500 58%) 3.3330\n",
      "74m 18s (- 49m 32s) (3600 60%) 3.1858\n",
      "76m 33s (- 47m 35s) (3700 61%) 3.4698\n",
      "78m 34s (- 45m 29s) (3800 63%) 3.4466\n",
      "80m 48s (- 43m 30s) (3900 65%) 3.3338\n",
      "83m 1s (- 41m 30s) (4000 66%) 3.4792\n",
      "84m 59s (- 39m 23s) (4100 68%) 3.1444\n",
      "86m 57s (- 37m 16s) (4200 70%) 3.4183\n",
      "89m 8s (- 35m 14s) (4300 71%) 3.3309\n",
      "91m 26s (- 33m 15s) (4400 73%) 3.5009\n",
      "93m 49s (- 31m 16s) (4500 75%) 3.4601\n",
      "96m 16s (- 29m 18s) (4600 76%) 3.5392\n",
      "98m 9s (- 27m 9s) (4700 78%) 3.4521\n",
      "100m 28s (- 25m 7s) (4800 80%) 3.5114\n",
      "102m 31s (- 23m 0s) (4900 81%) 3.3809\n",
      "104m 40s (- 20m 56s) (5000 83%) 3.4090\n",
      "106m 50s (- 18m 51s) (5100 85%) 3.4324\n",
      "108m 54s (- 16m 45s) (5200 86%) 3.2799\n",
      "110m 59s (- 14m 39s) (5300 88%) 3.4774\n",
      "112m 54s (- 12m 32s) (5400 90%) 3.3456\n",
      "115m 20s (- 10m 29s) (5500 91%) 3.5208\n",
      "117m 21s (- 8m 22s) (5600 93%) 3.2459\n",
      "119m 44s (- 6m 18s) (5700 95%) 3.6615\n",
      "121m 53s (- 4m 12s) (5800 96%) 3.1016\n",
      "123m 43s (- 2m 5s) (5900 98%) 3.2291\n",
      "125m 37s (- 0m 0s) (6000 100%) 3.5481\n"
     ]
    }
   ],
   "source": [
    "hidden_size = 256\n",
    "\n",
    "encoder1 = EncoderRNN(input_lang.n_words, hidden_size).to(device)\n",
    "attn_decoder1 = AttnDecoderRNN(hidden_size, output_lang.n_words, dropout_p=0.1).to(device)\n",
    "\n",
    "trainIters(encoder1, attn_decoder1, 6000, print_every=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "79d363e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> i m going to have to get back to you on this i don t know for sure . i just treat them the same . edit research led me to this the differences between hypertrophic and keloid scars can be confusing . although they both have excessive collagen in common they act differently . for instance a hypertrophic scar forms soon after the skin has been lacerated while keloids may not begin forming for up to a year after the injury .the tissue structure also varies between the two scar types . collagen fiber formation in keloid scars actively spreads into the nearby healthy tissue while hypertrophic scars remain contained within the original boundary of the injury . the surrounding healthy tissue is not affected .\n",
      "= they re the same though they can form at different times and one can spread . baby your piercing hole .\n",
      "< you are a a you . <EOS>\n",
      "\n",
      "> well his allergy is really strange . he doesn t just just swell up and die if he eats something with a spec of gluten in it . basically if he eats something containing more than trace amounts of gluten he ll be fine for a while but then he ll slowly break out in hives and he ll begin to get groggy and unresponsive until his throat closes up and he passes out . it took the doctors a while to figure out what was happening . apparently they think the allergic reaction starts when he sweats after eating gluten causing him to break out in hives . so as long as he has an epipen on him and he or my mom notice him breaking out in hives early enough to give him the shot or take him to the er he s fine . it is kind of scary though because his allergy developed randomly when he was about my age mid thirties . o o\n",
      "= he s fine if he gets medical attention and doesn t have a problem unless the food is specifically prepared with some form of gluten .\n",
      "< i m a a . <EOS>\n",
      "\n",
      "> they are predominantly a racistic supremacist tribal people . they help themselves only against every other ethnies . they never ever assimilate . that s why . nota bene the real scapegoat are the gentiles always have been . they blame everything on the gentile while they clearly are the problem .\n",
      "= they re doing it to themselves and never learn from their mistakes .\n",
      "< you re a a . <EOS>\n",
      "\n",
      "> this really sounds like compass group being jerks . if they re anything like the companies i m accustomed to seeing in universities sodexo aramark it s probably on their end . the way the response is phrased sounds like they are attempting to reason w compass to stock the product that they actually want in due time but chose to do this compromise as a means to grow their brand . i m not sure this is a serious breach of integrity as compass is assuredly using dairy they would already purchasing for other means .\n",
      "= i am reading way too far into this .\n",
      "< i m a a . <EOS>\n",
      "\n",
      ">  yeaaa . .so for me breakfast is big lunch is big dinner is big and fourth meal is big .\n",
      "= i m skinny\n",
      "< you are a a . . <EOS>\n",
      "\n",
      "> our number receivers get more attention than other teams starters . . . cowboy fans are so fucking suspended from reality we literally think any th rounder and udfa can become a hof er . . .they won t admit it but i ll tell you we have a lot of expectations and love up and down the roster . how many other fans go out and buy their teams rd receivers jersey in a cult like nature ? the guy won t get a shoe deal or his own brand . but people are probably already buying his jersey .\n",
      "= you are totally right but the dallas market is a tad different .\n",
      "< you re a a . . <EOS>\n",
      "\n",
      "> no . devices set how much current they can draw not the charger . too much voltage on the other hand can kill things but that s not a modern problem with almost everything on the usb standard .\n",
      "= you re safe . it won t draw more current than it can use . it may not take advantage of the higher current available but it won t hurt it .\n",
      "< i m a a . <EOS>\n",
      "\n",
      "> you re right they didn t know what they were doing but not because hurrdurr it was on gfwl and overpriced . their mistake was to believe that even though it was leaked early people would respect that it had no drm and wouldn t pirate it . but wait no matter what people will pirate . for example everyone still loves to complain about how diablo streams most of its content from blizzard servers even when you play solo but look at how hard it s been to get a working pirated version . i don t know about now but for many months afterwards the few pirated versions available sucked because they couldn t duplicate blizzard s streaming setup effectively . and guess what ? diablo a shitty game sold millions of copies for the ridiculously low price of dollars or more for the collector edition . would it have sold anywhere near that much if people had been able to torrent it and play lan with their friends instead of over battle .net ? i don t know if you pirate or know much about torrenting but check any of the public torrent sites tpb kat and count the seeds from drm free games versus diablo .\n",
      "= you re wrong .\n",
      "< i m a a . <EOS>\n",
      "\n",
      "> i m the volume license training voucher admin . they re hard to find so here s how you do it log in to your volume license servicing center software assurance training vouchers . select your enrollment number then see how many eligible days you have under training vouchers . then contact a software assurance training vendor and they ll walk you through the rest . i recommend interface or new horizons my company is fairly small so the only place i can really move would be if they created a new position to head up it infrastructure so that my boss can spend less of his time dealing with our side and more time on projects . honestly that s at least years out . thankfully i get to do lots of exciting projects like implement sccm and stuff like that so i feel like i get to at least think every day . and it s stable with good benefits so i could be happy staying here for forever . most of our senior it staff have been with the company years . i m considering moonlighting consulting work on the side some day but i m not sure if i m willing to take that much time away from my family .\n",
      "= i m mostly training so that if anything happened i would be employable but i don t see myself moving up at my current company any time soon . the thing that really makes it folks stand out at my company is the ability to understand what the business really needs and think of technical solutions . many of our staff just think they should always upgrade for the sake of upgrading without stopping to think if our business would actually use any of the additional features etc . maybe some project management ability or something similar might help ?\n",
      "< i m a a . <EOS>\n",
      "\n",
      "> i would say create a budget and stick to it but your problem is more fundamental than that . you wouldn t be able to do either of those steps . you certainly cant stick to a budget . but more fundamentally you wouldn t even be able to create a budget . because you don t have an understanding of your situation . you don t have money . which means you cant buy nice things . people with money get to go on once in a life time trips to europe . people who don t have money dont get to do that . once you understand that you might be ready to begin step create a budget . and once you grow up and mature a little bit you might be ready to take on step stick to it . until you grow up and get a grip on your life you are going to continue to do things like thinking if you ignore the mail the bills will go away .\n",
      "= you re a child\n",
      "< you are not a . . <EOS>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "evaluateRandomly(encoder1, attn_decoder1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92d9b23c",
   "metadata": {},
   "source": [
    "### saving the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e1d7a0ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify a path to save to\n",
    "PATH = \"summarizer.pt\"\n",
    "\n",
    "torch.save({\n",
    "            'encoder1': encoder1,\n",
    "            'attn_decoder1': attn_decoder1,\n",
    "            }, PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1fe9c4a7",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'SingletonFactory' object has no attribute 'register'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [38]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mbentoml\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m tag_encoder \u001b[38;5;241m=\u001b[39m \u001b[43mbentoml\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpytorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpytorch_tldr_encoder\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m tag_decoder \u001b[38;5;241m=\u001b[39m bentoml\u001b[38;5;241m.\u001b[39mpytorch\u001b[38;5;241m.\u001b[39msave(\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpytorch_tldr_decoder\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     10\u001b[0m     attn_decoder1,\n\u001b[1;32m     11\u001b[0m )\n",
      "File \u001b[0;32m~/Library/Python/3.8/lib/python/site-packages/simple_di/__init__.py:124\u001b[0m, in \u001b[0;36m_inject.<locals>._\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    121\u001b[0m bind \u001b[38;5;241m=\u001b[39m sig\u001b[38;5;241m.\u001b[39mbind_partial(\u001b[38;5;241m*\u001b[39mfiltered_args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfiltered_kwargs)\n\u001b[1;32m    122\u001b[0m bind\u001b[38;5;241m.\u001b[39mapply_defaults()\n\u001b[0;32m--> 124\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m_inject_args\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbind\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m_inject_kwargs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbind\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.8/lib/python/site-packages/bentoml/_internal/frameworks/pytorch.py:193\u001b[0m, in \u001b[0;36msave\u001b[0;34m(name, model, metadata, model_store)\u001b[0m\n\u001b[1;32m    190\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(weight_file, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[1;32m    191\u001b[0m         torch\u001b[38;5;241m.\u001b[39msave(model, file, pickle_module\u001b[38;5;241m=\u001b[39mcloudpickle)\n\u001b[0;32m--> 193\u001b[0m \u001b[43m_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_store\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    194\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _model\u001b[38;5;241m.\u001b[39mtag\n",
      "File \u001b[0;32m~/Library/Python/3.8/lib/python/site-packages/simple_di/__init__.py:124\u001b[0m, in \u001b[0;36m_inject.<locals>._\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    121\u001b[0m bind \u001b[38;5;241m=\u001b[39m sig\u001b[38;5;241m.\u001b[39mbind_partial(\u001b[38;5;241m*\u001b[39mfiltered_args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfiltered_kwargs)\n\u001b[1;32m    122\u001b[0m bind\u001b[38;5;241m.\u001b[39mapply_defaults()\n\u001b[0;32m--> 124\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m_inject_args\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbind\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m_inject_kwargs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbind\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.8/lib/python/site-packages/bentoml/_internal/models/model.py:130\u001b[0m, in \u001b[0;36mModel.save\u001b[0;34m(self, model_store)\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[38;5;129m@inject\u001b[39m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msave\u001b[39m(\n\u001b[1;32m    128\u001b[0m     \u001b[38;5;28mself\u001b[39m, model_store: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModelStore\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m=\u001b[39m Provide[BentoMLContainer\u001b[38;5;241m.\u001b[39mmodel_store]\n\u001b[1;32m    129\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 130\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_save\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_store\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    131\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSuccessfully saved \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    132\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m~/Library/Python/3.8/lib/python/site-packages/bentoml/_internal/models/model.py:144\u001b[0m, in \u001b[0;36mModel._save\u001b[0;34m(self, model_store)\u001b[0m\n\u001b[1;32m    141\u001b[0m     logger\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to create Model for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtag\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, not saving.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    142\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m BentoMLException(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to save Model because it was invalid\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 144\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mmodel_store\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mregister\u001b[49m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtag) \u001b[38;5;28;01mas\u001b[39;00m model_path:\n\u001b[1;32m    145\u001b[0m     out_fs \u001b[38;5;241m=\u001b[39m fs\u001b[38;5;241m.\u001b[39mopen_fs(model_path, create\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, writeable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    146\u001b[0m     fs\u001b[38;5;241m.\u001b[39mmirror\u001b[38;5;241m.\u001b[39mmirror(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fs, out_fs, copy_if_newer\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'SingletonFactory' object has no attribute 'register'"
     ]
    }
   ],
   "source": [
    "import bentoml\n",
    "\n",
    "tag_encoder = bentoml.pytorch.save(\n",
    "    \"pytorch_tldr_encoder\",\n",
    "    encoder1,\n",
    ")\n",
    "\n",
    "tag_decoder = bentoml.pytorch.save(\n",
    "    \"pytorch_tldr_decoder\",\n",
    "    attn_decoder1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdf35e55",
   "metadata": {},
   "source": [
    "## Create a BentoML Service for serving the model\n",
    "\n",
    "Note: using `%%writefile` here because `bentoml.Service` instance must be created in a separate `.py` file\n",
    "\n",
    "Even though we have only one model, we can create as many api endpoints as we want. Here we create two end points `predict_ndarray` and `predict_image`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef0ed56c",
   "metadata": {},
   "source": [
    "### load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "14c05ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checkpoint = torch.load(PATH)\n",
    "\n",
    "# encoder = checkpoint['encoder1']\n",
    "# decoder = checkpoint['attn_decoder1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f3e2f590",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "get() takes 1 positional argument but 2 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [41]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m     s \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[^a-zA-Z.!?]+\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m, s)\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m s\n\u001b[0;32m---> 21\u001b[0m encoder \u001b[38;5;241m=\u001b[39m \u001b[43mbentoml\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpytorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_runner\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpytorch_tldr_encoder\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m     23\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m decoder \u001b[38;5;241m=\u001b[39m bentoml\u001b[38;5;241m.\u001b[39mpytorch\u001b[38;5;241m.\u001b[39mload_runner(\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpytorch_tldr_decoder\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     27\u001b[0m )\n\u001b[1;32m     29\u001b[0m svc \u001b[38;5;241m=\u001b[39m bentoml\u001b[38;5;241m.\u001b[39mService(\n\u001b[1;32m     30\u001b[0m     name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpytorch_tldr_demo\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     31\u001b[0m     runners\u001b[38;5;241m=\u001b[39m[\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     34\u001b[0m     ],\n\u001b[1;32m     35\u001b[0m )\n",
      "File \u001b[0;32m~/Library/Python/3.8/lib/python/site-packages/simple_di/__init__.py:124\u001b[0m, in \u001b[0;36m_inject.<locals>._\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    121\u001b[0m bind \u001b[38;5;241m=\u001b[39m sig\u001b[38;5;241m.\u001b[39mbind_partial(\u001b[38;5;241m*\u001b[39mfiltered_args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfiltered_kwargs)\n\u001b[1;32m    122\u001b[0m bind\u001b[38;5;241m.\u001b[39mapply_defaults()\n\u001b[0;32m--> 124\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m_inject_args\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbind\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m_inject_kwargs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbind\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.8/lib/python/site-packages/bentoml/_internal/frameworks/pytorch.py:348\u001b[0m, in \u001b[0;36mload_runner\u001b[0;34m(tag, predict_fn_name, device_id, partial_kwargs, name, resource_quota, batch_options, model_store)\u001b[0m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    347\u001b[0m     name \u001b[38;5;241m=\u001b[39m tag\u001b[38;5;241m.\u001b[39mname\n\u001b[0;32m--> 348\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_PyTorchRunner\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtag\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtag\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpredict_fn_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpredict_fn_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpartial_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpartial_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresource_quota\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresource_quota\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    356\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_store\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_store\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    357\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.8/lib/python/site-packages/simple_di/__init__.py:124\u001b[0m, in \u001b[0;36m_inject.<locals>._\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    121\u001b[0m bind \u001b[38;5;241m=\u001b[39m sig\u001b[38;5;241m.\u001b[39mbind_partial(\u001b[38;5;241m*\u001b[39mfiltered_args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfiltered_kwargs)\n\u001b[1;32m    122\u001b[0m bind\u001b[38;5;241m.\u001b[39mapply_defaults()\n\u001b[0;32m--> 124\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m_inject_args\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbind\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m_inject_kwargs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbind\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.8/lib/python/site-packages/bentoml/_internal/frameworks/pytorch.py:210\u001b[0m, in \u001b[0;36m_PyTorchRunner.__init__\u001b[0;34m(self, tag, predict_fn_name, device_id, name, partial_kwargs, resource_quota, batch_options, model_store)\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;129m@inject\u001b[39m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m    200\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    208\u001b[0m     model_store: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModelStore\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m=\u001b[39m Provide[BentoMLContainer\u001b[38;5;241m.\u001b[39mmodel_store],\n\u001b[1;32m    209\u001b[0m ):\n\u001b[0;32m--> 210\u001b[0m     in_store_tag \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_store\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtag\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mtag\n\u001b[1;32m    212\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(name, resource_quota, batch_options)\n\u001b[1;32m    213\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_predict_fn_name \u001b[38;5;241m=\u001b[39m predict_fn_name\n",
      "\u001b[0;31mTypeError\u001b[0m: get() takes 1 positional argument but 2 were given"
     ]
    }
   ],
   "source": [
    "import typing as t\n",
    "\n",
    "import bentoml\n",
    "from bentoml.io import Text\n",
    "\n",
    "\n",
    "def unicodeToAscii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "    )\n",
    "\n",
    "\n",
    "def normalizeString(s):\n",
    "    s = unicodeToAscii(s.lower().strip())\n",
    "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
    "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
    "    return s\n",
    "\n",
    "\n",
    "encoder = bentoml.pytorch.load_runner(\n",
    "    \"pytorch_tldr_encoder\"\n",
    ")\n",
    "\n",
    "decoder = bentoml.pytorch.load_runner(\n",
    "    \"pytorch_tldr_decoder\"\n",
    ")\n",
    "\n",
    "svc = bentoml.Service(\n",
    "    name=\"pytorch_tldr_demo\",\n",
    "    runners=[\n",
    "        encoder,\n",
    "        decoder,\n",
    "    ],\n",
    ")\n",
    "\n",
    "\n",
    "@svc.api(input=Text(), output=Text())\n",
    "def summarize(input_arr: t.List[str]) -> t.List[str]:\n",
    "    input_arr = list(map(normalizeString, input_arr))\n",
    "    enc_arr = encoder.run_batch(input_arr)\n",
    "    res = decoder.run_batch(enc_arr)\n",
    "    return res[0]['generated_text']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "590147aa",
   "metadata": {},
   "source": [
    "Start a dev model server to test out the service defined above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29173871",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m[13:51:28]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Starting development BentoServer from \u001b[32m\"service.py:svc\"\u001b[0m      \n",
      "\u001b[2;36m[13:51:36]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Service imported from source:                               \n",
      "\u001b[2;36m           \u001b[0m         \u001b[1;35mbentoml.Service\u001b[0m\u001b[1m(\u001b[0m\u001b[33mname\u001b[0m=\u001b[32m\"pytorch_tldr_demo\"\u001b[0m,                   \n",
      "\u001b[2;36m           \u001b[0m         \u001b[33mimport_str\u001b[0m=\u001b[32m\"service\u001b[0m\u001b[32m:svc\"\u001b[0m, \u001b[33mworking_dir\u001b[0m=\u001b[32m\"/Users/spence/Documen\u001b[0m\n",
      "\u001b[2;36m           \u001b[0m         \u001b[32mts/GitHub/gallery/pytorch_seq2seq\"\u001b[0m\u001b[1m)\u001b[0m                         \n",
      "\u001b[2;36m[13:51:36]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Started server process \u001b[1m[\u001b[0m\u001b[1;36m17881\u001b[0m\u001b[1m]\u001b[0m                  \u001b]8;id=341799;file:///Users/spence/Library/Python/3.8/lib/python/site-packages/uvicorn/server.py\u001b\\\u001b[2mserver.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=96038;file:///Users/spence/Library/Python/3.8/lib/python/site-packages/uvicorn/server.py#82\u001b\\\u001b[2m82\u001b[0m\u001b]8;;\u001b\\\n",
      "\u001b[2;36m          \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Waiting for application startup.                    \u001b]8;id=42783;file:///Users/spence/Library/Python/3.8/lib/python/site-packages/uvicorn/lifespan/on.py\u001b\\\u001b[2mon.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=171161;file:///Users/spence/Library/Python/3.8/lib/python/site-packages/uvicorn/lifespan/on.py#45\u001b\\\u001b[2m45\u001b[0m\u001b]8;;\u001b\\\n",
      "\u001b[2;36m          \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Application startup complete.                       \u001b]8;id=411341;file:///Users/spence/Library/Python/3.8/lib/python/site-packages/uvicorn/lifespan/on.py\u001b\\\u001b[2mon.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=152286;file:///Users/spence/Library/Python/3.8/lib/python/site-packages/uvicorn/lifespan/on.py#59\u001b\\\u001b[2m59\u001b[0m\u001b]8;;\u001b\\\n",
      "\u001b[2;36m          \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Uvicorn running on \u001b[4;94mhttp://127.0.0.1:5000\u001b[0m       \u001b]8;id=849184;file:///Users/spence/Library/Python/3.8/lib/python/site-packages/uvicorn/server.py\u001b\\\u001b[2mserver.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=684105;file:///Users/spence/Library/Python/3.8/lib/python/site-packages/uvicorn/server.py#215\u001b\\\u001b[2m215\u001b[0m\u001b]8;;\u001b\\\n",
      "\u001b[2;36m           \u001b[0m         \u001b[1m(\u001b[0mPress CTRL+C to quit\u001b[1m)\u001b[0m                         \u001b[2m             \u001b[0m\n",
      "\u001b[2;36m[13:51:48]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m \u001b[1;92m127.0.0.1\u001b[0m:\u001b[1;36m62837\u001b[0m - \u001b[32m\"\u001b[0m\u001b[1;33mGET\u001b[0m\u001b[32m / HTTP/1.1\"\u001b[0m \u001b[1;36m200\u001b[0m OK    \u001b]8;id=806983;file:///Users/spence/Library/Python/3.8/lib/python/site-packages/uvicorn/protocols/http/h11_impl.py\u001b\\\u001b[2mh11_impl.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=900083;file:///Users/spence/Library/Python/3.8/lib/python/site-packages/uvicorn/protocols/http/h11_impl.py#442\u001b\\\u001b[2m442\u001b[0m\u001b]8;;\u001b\\\n",
      "\u001b[2;36m          \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m \u001b[1;92m127.0.0.1\u001b[0m:\u001b[1;36m62837\u001b[0m - \u001b[32m\"\u001b[0m\u001b[1;33mGET\u001b[0m\u001b[32m /docs.json HTTP/1.1\"\u001b[0m  \u001b]8;id=322082;file:///Users/spence/Library/Python/3.8/lib/python/site-packages/uvicorn/protocols/http/h11_impl.py\u001b\\\u001b[2mh11_impl.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=298238;file:///Users/spence/Library/Python/3.8/lib/python/site-packages/uvicorn/protocols/http/h11_impl.py#442\u001b\\\u001b[2m442\u001b[0m\u001b]8;;\u001b\\\n",
      "\u001b[2;36m           \u001b[0m         \u001b[1;36m200\u001b[0m OK                                       \u001b[2m               \u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python3 -m bentoml serve service.py:svc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7f03564",
   "metadata": {},
   "source": [
    "## Build a Bento for distribution and deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "207561bc",
   "metadata": {},
   "outputs": [
    {
     "ename": "BentoMLException",
     "evalue": "API predict is already defined in Service pytorch_tldr_demo",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mBentoMLException\u001b[0m                          Traceback (most recent call last)",
      "Input \u001b[0;32mIn [29]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mbentoml\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mservice.py:svc\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43minclude\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m*.py\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexclude\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtests/\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdescription\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfile:./README.md\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpython\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpackages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtorch\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.8/lib/python/site-packages/simple_di/__init__.py:124\u001b[0m, in \u001b[0;36m_inject.<locals>._\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    121\u001b[0m bind \u001b[38;5;241m=\u001b[39m sig\u001b[38;5;241m.\u001b[39mbind_partial(\u001b[38;5;241m*\u001b[39mfiltered_args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfiltered_kwargs)\n\u001b[1;32m    122\u001b[0m bind\u001b[38;5;241m.\u001b[39mapply_defaults()\n\u001b[0;32m--> 124\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m_inject_args\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbind\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m_inject_kwargs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbind\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.8/lib/python/site-packages/bentoml/bentos.py:155\u001b[0m, in \u001b[0;36mbuild\u001b[0;34m(service, labels, description, include, exclude, additional_models, docker, python, conda, version, build_ctx, _bento_store, _model_store)\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;124;03mUser-facing API for building a Bento, the available build options are symmetrical to\u001b[39;00m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;124;03mthe content of a valid bentofile.yaml file, for building Bento from CLI.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    141\u001b[0m \n\u001b[1;32m    142\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m  \u001b[38;5;66;03m# noqa: LN001\u001b[39;00m\n\u001b[1;32m    143\u001b[0m build_config \u001b[38;5;241m=\u001b[39m BentoBuildConfig(\n\u001b[1;32m    144\u001b[0m     service\u001b[38;5;241m=\u001b[39mservice,\n\u001b[1;32m    145\u001b[0m     description\u001b[38;5;241m=\u001b[39mdescription,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    152\u001b[0m     conda\u001b[38;5;241m=\u001b[39mconda,\n\u001b[1;32m    153\u001b[0m )\n\u001b[0;32m--> 155\u001b[0m bento \u001b[38;5;241m=\u001b[39m \u001b[43mBento\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    156\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbuild_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbuild_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    157\u001b[0m \u001b[43m    \u001b[49m\u001b[43mversion\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mversion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    158\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbuild_ctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbuild_ctx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    159\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_store\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_model_store\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    160\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39msave(_bento_store)\n\u001b[1;32m    161\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBento build success, \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m created\u001b[39m\u001b[38;5;124m\"\u001b[39m, bento)\n\u001b[1;32m    162\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m bento\n",
      "File \u001b[0;32m~/Library/Python/3.8/lib/python/site-packages/simple_di/__init__.py:124\u001b[0m, in \u001b[0;36m_inject.<locals>._\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    121\u001b[0m bind \u001b[38;5;241m=\u001b[39m sig\u001b[38;5;241m.\u001b[39mbind_partial(\u001b[38;5;241m*\u001b[39mfiltered_args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfiltered_kwargs)\n\u001b[1;32m    122\u001b[0m bind\u001b[38;5;241m.\u001b[39mapply_defaults()\n\u001b[0;32m--> 124\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m_inject_args\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbind\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m_inject_kwargs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbind\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.8/lib/python/site-packages/bentoml/_internal/bento/bento.py:127\u001b[0m, in \u001b[0;36mBento.create\u001b[0;34m(cls, build_config, version, build_ctx, model_store)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misdir(build_ctx), \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbuild ctx \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbuild_ctx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not exist\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    126\u001b[0m \u001b[38;5;66;03m# This also verifies that svc can be imported correctly\u001b[39;00m\n\u001b[0;32m--> 127\u001b[0m svc \u001b[38;5;241m=\u001b[39m \u001b[43mimport_service\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbuild_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mservice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mworking_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbuild_ctx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;66;03m# Apply default build options\u001b[39;00m\n\u001b[1;32m    130\u001b[0m build_config \u001b[38;5;241m=\u001b[39m build_config\u001b[38;5;241m.\u001b[39mwith_defaults()\n",
      "File \u001b[0;32m~/Library/Python/3.8/lib/python/site-packages/simple_di/__init__.py:124\u001b[0m, in \u001b[0;36m_inject.<locals>._\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    121\u001b[0m bind \u001b[38;5;241m=\u001b[39m sig\u001b[38;5;241m.\u001b[39mbind_partial(\u001b[38;5;241m*\u001b[39mfiltered_args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfiltered_kwargs)\n\u001b[1;32m    122\u001b[0m bind\u001b[38;5;241m.\u001b[39mapply_defaults()\n\u001b[0;32m--> 124\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m_inject_args\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbind\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m_inject_kwargs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbind\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.8/lib/python/site-packages/bentoml/_internal/service/loader.py:125\u001b[0m, in \u001b[0;36mimport_service\u001b[0;34m(svc_import_path, working_dir, model_store)\u001b[0m\n\u001b[1;32m    123\u001b[0m BentoMLContainer\u001b[38;5;241m.\u001b[39mmodel_store\u001b[38;5;241m.\u001b[39mset(model_store)\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 125\u001b[0m     module \u001b[38;5;241m=\u001b[39m \u001b[43mimportlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodule_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mworking_dir\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    126\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    127\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ImportServiceError(\n\u001b[1;32m    128\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m happens when importing \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodule_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m in \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    129\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcurrent path: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mrepr\u001b[39m(sys\u001b[38;5;241m.\u001b[39mpath)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. working dir: \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mworking_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    130\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcurrent dir: \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mos\u001b[38;5;241m.\u001b[39mgetcwd()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    131\u001b[0m     )\n",
      "File \u001b[0;32m/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.8/lib/python3.8/importlib/__init__.py:127\u001b[0m, in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m    125\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m    126\u001b[0m         level \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m--> 127\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1014\u001b[0m, in \u001b[0;36m_gcd_import\u001b[0;34m(name, package, level)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:991\u001b[0m, in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:975\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:671\u001b[0m, in \u001b[0;36m_load_unlocked\u001b[0;34m(spec)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap_external>:783\u001b[0m, in \u001b[0;36mexec_module\u001b[0;34m(self, module)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:219\u001b[0m, in \u001b[0;36m_call_with_frames_removed\u001b[0;34m(f, *args, **kwds)\u001b[0m\n",
      "File \u001b[0;32m~/Documents/GitHub/gallery/pytorch_seq2seq/service.py:41\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     27\u001b[0m decoder \u001b[38;5;241m=\u001b[39m bentoml\u001b[38;5;241m.\u001b[39mpytorch\u001b[38;5;241m.\u001b[39mload_runner(\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpytorch_tldr_decoder\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     29\u001b[0m )\n\u001b[1;32m     31\u001b[0m svc \u001b[38;5;241m=\u001b[39m bentoml\u001b[38;5;241m.\u001b[39mService(\n\u001b[1;32m     32\u001b[0m     name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpytorch_tldr_demo\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     33\u001b[0m     runners\u001b[38;5;241m=\u001b[39m[\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     36\u001b[0m     ],\n\u001b[1;32m     37\u001b[0m )\n\u001b[1;32m     40\u001b[0m \u001b[38;5;129;43m@svc\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapi\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mText\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mText\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m---> 41\u001b[0m \u001b[38;5;28;43;01mdef\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;21;43mpredict\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43minput_arr\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mList\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m>\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mList\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m     42\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_arr\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mmap\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mnormalizeString\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_arr\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     43\u001b[0m \u001b[43m    \u001b[49m\u001b[43menc_arr\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mencoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_arr\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.8/lib/python/site-packages/bentoml/_internal/service/service.py:123\u001b[0m, in \u001b[0;36mService.api.<locals>.decorator\u001b[0;34m(func)\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(output, Multipart):\n\u001b[1;32m    117\u001b[0m     logger\u001b[38;5;241m.\u001b[39mwarning(\n\u001b[1;32m    118\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound Multipart as the output of API `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname \u001b[38;5;129;01mor\u001b[39;00m func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m`. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    119\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMultipart response is rarely used in the real world,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    120\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m less clients/browsers support it. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    121\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMake sure you know what you are doing.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    122\u001b[0m     )\n\u001b[0;32m--> 123\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_add_inference_api\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mroute\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func\n",
      "File \u001b[0;32m~/Library/Python/3.8/lib/python/site-packages/bentoml/_internal/service/service.py:147\u001b[0m, in \u001b[0;36mService._add_inference_api\u001b[0;34m(self, func, input, output, name, doc, route)\u001b[0m\n\u001b[1;32m    137\u001b[0m api \u001b[38;5;241m=\u001b[39m InferenceAPI(\n\u001b[1;32m    138\u001b[0m     name\u001b[38;5;241m=\u001b[39mname,\n\u001b[1;32m    139\u001b[0m     user_defined_callback\u001b[38;5;241m=\u001b[39mfunc,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    143\u001b[0m     route\u001b[38;5;241m=\u001b[39mroute,\n\u001b[1;32m    144\u001b[0m )\n\u001b[1;32m    146\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m api\u001b[38;5;241m.\u001b[39mname \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapis:\n\u001b[0;32m--> 147\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m BentoMLException(\n\u001b[1;32m    148\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAPI \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mapi\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is already defined in Service \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    149\u001b[0m     )\n\u001b[1;32m    150\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapis[api\u001b[38;5;241m.\u001b[39mname] \u001b[38;5;241m=\u001b[39m api\n",
      "\u001b[0;31mBentoMLException\u001b[0m: API predict is already defined in Service pytorch_tldr_demo"
     ]
    }
   ],
   "source": [
    "bentoml.build(\n",
    "    \"service.py:svc\",\n",
    "    include=[\"*.py\"],\n",
    "    exclude=[\"tests/\"],\n",
    "    description=\"file:./README.md\",\n",
    "    python=dict(\n",
    "        packages=[\"torch\"],\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36306933",
   "metadata": {},
   "source": [
    "Starting a dev server with the Bento build:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec4b9dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "!bentoml serve pytorch_tldr_demo:latest"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  },
  "name": "pytorch_mnist.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
