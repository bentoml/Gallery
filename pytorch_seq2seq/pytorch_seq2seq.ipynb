{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a682ea0b",
   "metadata": {},
   "source": [
    "# BentoML PyTorch TLDR Tutorial\n",
    "\n",
    "Link to source code: https://github.com/bentoml/gallery/tree/main/TLDR/pytorch_seq2seq/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c8ce1c9",
   "metadata": {},
   "source": [
    "Install required dependencies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3ad00863",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Value for scheme.headers does not match. Please report this to <https://github.com/pypa/pip/issues/10151>\n",
      "distutils: /Users/spence/Library/Python/3.8/include/python3.8/UNKNOWN\n",
      "sysconfig: /Users/spence/Library/Python/3.8/include/UNKNOWN\u001b[0m\n",
      "\u001b[33mWARNING: Additional context:\n",
      "user = True\n",
      "home = None\n",
      "root = None\n",
      "prefix = None\u001b[0m\n",
      "\u001b[33mWARNING: You are using pip version 21.2; however, version 22.0.3 is available.\n",
      "You should consider upgrading via the '/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python3 -m pip install --quiet -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fb49fd0",
   "metadata": {},
   "source": [
    "Import required libraries:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45393b74",
   "metadata": {},
   "source": [
    "## Download and Extract Reddit TL;DR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5e227583",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from os.path import exists\n",
    "import zipfile\n",
    "import json\n",
    "from tqdm import trange, tqdm\n",
    "import pickle\n",
    "\n",
    "from __future__ import unicode_literals, print_function, division\n",
    "from io import open\n",
    "import unicodedata\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "070965aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = 'dataset/corpus-webis-tldr-17.zip'\n",
    "\n",
    "if not exists(dataset_path):\n",
    "    r = requests.get('https://zenodo.org/record/1043504/files/corpus-webis-tldr-17.zip?download=1')\n",
    "    with open(dataset_path, 'wb') as f:\n",
    "        f.write(r.content)\n",
    "    with zipfile.ZipFile(dataset_path, 'r') as z:\n",
    "        z.extractall('dataset/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "03145717",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_json('dataset/corpus-webis-tldr-17.json', \\\n",
    "                  lines=True, chunksize=1000000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "320ed520",
   "metadata": {},
   "source": [
    "## Begin processing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cbbd3118",
   "metadata": {},
   "outputs": [],
   "source": [
    "SOS_token = 0\n",
    "EOS_token = 1\n",
    "\n",
    "\n",
    "class Lang:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {0: 'SOS', 1: 'EOS'}\n",
    "        self.n_words = 2  # Count SOS and EOS\n",
    "\n",
    "    def addSentence(self, sentence):\n",
    "        for word in sentence.split(' '):\n",
    "            self.addWord(word)\n",
    "\n",
    "    def addWord(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.n_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.n_words] = word\n",
    "            self.n_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1\n",
    "\n",
    "# Turn a Unicode string to plain ASCII, thanks to\n",
    "# https://stackoverflow.com/a/518232\n",
    "def unicodeToAscii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "    )\n",
    "\n",
    "# Lowercase, trim, and remove non-letter characters\n",
    "def normalizeString(s):\n",
    "    s = unicodeToAscii(s.lower().strip())\n",
    "    s = re.sub(r'([.!?])', r' \\1', s)\n",
    "    s = re.sub(r'[^a-zA-Z.!?]+', r' ', s)\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9e854567",
   "metadata": {},
   "outputs": [],
   "source": [
    "def readLangs(lang1, lang2, reverse=False):\n",
    "    print('Reading lines...')\n",
    "\n",
    "    lines = next(df)\n",
    "\n",
    "    # Split every line into pairs and normalize\n",
    "    pairs = [[normalizeString(s) for s in l] for l in \\\n",
    "             tqdm(zip(lines['content'], lines['summary']))]\n",
    "\n",
    "    # Reverse pairs, make Lang instances\n",
    "    if reverse:\n",
    "        pairs = [list(reversed(p)) for p in pairs]\n",
    "        input_lang = Lang(lang2)\n",
    "        output_lang = Lang(lang1)\n",
    "    else:\n",
    "        input_lang = Lang(lang1)\n",
    "        output_lang = Lang(lang2)\n",
    "\n",
    "    return input_lang, output_lang, pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bb60cd49",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = 260\n",
    "\n",
    "eng_prefixes = (\n",
    "    'i am ', 'i m ',\n",
    "    'he is', 'he s ',\n",
    "    'she is', 'she s ',\n",
    "    'you are', 'you re ',\n",
    "    'we are', 'we re ',\n",
    "    'they are', 'they re ',\n",
    "    \n",
    ")\n",
    "\n",
    "\n",
    "def filterPair(p):\n",
    "    return len(p[0].split(' ')) < MAX_LENGTH and \\\n",
    "        len(p[1].split(' ')) < MAX_LENGTH and \\\n",
    "        p[1].startswith(eng_prefixes)\n",
    "\n",
    "\n",
    "def filterPairs(pairs):\n",
    "    return [pair for pair in pairs if filterPair(pair)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0cd2201f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading lines...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1000000it [06:22, 2616.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read 1000000 sentence pairs\n",
      "Trimmed to 27733 sentence pairs\n",
      "Counting words...\n",
      "Counted words:\n",
      "content 58613\n",
      "summary 22268\n"
     ]
    }
   ],
   "source": [
    "def prepareData(lang1, lang2, reverse=False):\n",
    "    input_lang, output_lang, pairs = readLangs(lang1, lang2, reverse)\n",
    "    print('Read %s sentence pairs' % len(pairs))\n",
    "    pairs = filterPairs(pairs)\n",
    "    print('Trimmed to %s sentence pairs' % len(pairs))\n",
    "    print('Counting words...')\n",
    "    for pair in pairs:\n",
    "        input_lang.addSentence(pair[0])\n",
    "        output_lang.addSentence(pair[1])\n",
    "    print('Counted words:')\n",
    "    print(input_lang.name, input_lang.n_words)\n",
    "    print(output_lang.name, output_lang.n_words)\n",
    "    return input_lang, output_lang, pairs\n",
    "\n",
    "\n",
    "input_lang, output_lang, pairs = prepareData('content', 'summary')\n",
    "\n",
    "with open('models/vectors.pkl', 'w') as f:\n",
    "    pickle.dump({\n",
    "        'input_lang': input_lang,\n",
    "        'output_lang': output_lang,\n",
    "    }, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59d8209c",
   "metadata": {},
   "source": [
    "## Define the model\n",
    "\n",
    "First let's define a simple PyTorch network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f46fd8b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        embedded = self.embedding(input).view(1, 1, -1)\n",
    "        output = embedded\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "05e42d08",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        output = self.embedding(input).view(1, 1, -1)\n",
    "        output = F.relu(output)\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        output = self.softmax(self.out(output[0]))\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a7f4fece",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttnDecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, dropout_p=0.1, max_length=MAX_LENGTH):\n",
    "        super(AttnDecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.dropout_p = dropout_p\n",
    "        self.max_length = max_length\n",
    "\n",
    "        self.embedding = nn.Embedding(self.output_size, self.hidden_size)\n",
    "        self.attn = nn.Linear(self.hidden_size * 2, self.max_length)\n",
    "        self.attn_combine = nn.Linear(self.hidden_size * 2, self.hidden_size)\n",
    "        self.dropout = nn.Dropout(self.dropout_p)\n",
    "        self.gru = nn.GRU(self.hidden_size, self.hidden_size)\n",
    "        self.out = nn.Linear(self.hidden_size, self.output_size)\n",
    "\n",
    "    def forward(self, input, hidden, encoder_outputs):\n",
    "        embedded = self.embedding(input).view(1, 1, -1)\n",
    "        embedded = self.dropout(embedded)\n",
    "\n",
    "        attn_weights = F.softmax(\n",
    "            self.attn(torch.cat((embedded[0], hidden[0]), 1)), dim=1)\n",
    "        attn_applied = torch.bmm(attn_weights.unsqueeze(0),\n",
    "                                 encoder_outputs.unsqueeze(0))\n",
    "\n",
    "        output = torch.cat((embedded[0], attn_applied[0]), 1)\n",
    "        output = self.attn_combine(output).unsqueeze(0)\n",
    "\n",
    "        output = F.relu(output)\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "\n",
    "        output = F.log_softmax(self.out(output[0]), dim=1)\n",
    "        return output, hidden, attn_weights\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38888f0a",
   "metadata": {},
   "source": [
    "## Training and Saving the model\n",
    "\n",
    "Then we define a simple PyTorch network and some helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1bcda1bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def indexesFromSentence(lang, sentence):\n",
    "    indices = []\n",
    "    for word in sentence.split(' '):\n",
    "        if word in lang.word2index:\n",
    "            # lang.addWord(word)\n",
    "            indices.append(lang.word2index[word])\n",
    "    return indices\n",
    "\n",
    "\n",
    "def tensorFromSentence(lang, sentence):\n",
    "    indexes = indexesFromSentence(lang, sentence)\n",
    "    indexes.append(EOS_token)\n",
    "    return torch.tensor(indexes, dtype=torch.long, device=device).view(-1, 1)\n",
    "\n",
    "\n",
    "def tensorsFromPair(pair):\n",
    "    input_tensor = tensorFromSentence(input_lang, pair[0])\n",
    "    target_tensor = tensorFromSentence(output_lang, pair[1])\n",
    "    return (input_tensor, target_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f2f334de",
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher_forcing_ratio = 0.5\n",
    "\n",
    "\n",
    "def train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length=MAX_LENGTH):\n",
    "    encoder_hidden = encoder.initHidden()\n",
    "\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "\n",
    "    input_length = input_tensor.size(0)\n",
    "    target_length = target_tensor.size(0)\n",
    "\n",
    "    encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
    "\n",
    "    loss = 0\n",
    "\n",
    "    for ei in range(input_length):\n",
    "        encoder_output, encoder_hidden = encoder(\n",
    "            input_tensor[ei], encoder_hidden)\n",
    "        encoder_outputs[ei] = encoder_output[0, 0]\n",
    "\n",
    "    decoder_input = torch.tensor([[SOS_token]], device=device)\n",
    "\n",
    "    decoder_hidden = encoder_hidden\n",
    "\n",
    "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
    "\n",
    "    if use_teacher_forcing:\n",
    "        # Teacher forcing: Feed the target as the next input\n",
    "        for di in range(target_length):\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs)\n",
    "            loss += criterion(decoder_output, target_tensor[di])\n",
    "            decoder_input = target_tensor[di]  # Teacher forcing\n",
    "\n",
    "    else:\n",
    "        # Without teacher forcing: use its own predictions as the next input\n",
    "        for di in range(target_length):\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs)\n",
    "            topv, topi = decoder_output.topk(1)\n",
    "            decoder_input = topi.squeeze().detach()  # detach from history as input\n",
    "\n",
    "            loss += criterion(decoder_output, target_tensor[di])\n",
    "            if decoder_input.item() == EOS_token:\n",
    "                break\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    return loss.item() / target_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a1d10bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "\n",
    "\n",
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d4a87c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainIters(encoder, decoder, n_iters, print_every=1000, plot_every=100, learning_rate=0.01):\n",
    "    start = time.time()\n",
    "    plot_losses = []\n",
    "    print_loss_total = 0  # Reset every print_every\n",
    "    plot_loss_total = 0  # Reset every plot_every\n",
    "\n",
    "    encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate)\n",
    "    decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate)\n",
    "    training_pairs = [tensorsFromPair(random.choice(pairs))\n",
    "                      for i in range(n_iters)]\n",
    "    criterion = nn.NLLLoss()\n",
    "\n",
    "    for iter in range(1, n_iters + 1):\n",
    "        training_pair = training_pairs[iter - 1]\n",
    "        input_tensor = training_pair[0]\n",
    "        target_tensor = training_pair[1]\n",
    "\n",
    "        loss = train(input_tensor, target_tensor, encoder,\n",
    "                     decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
    "        print_loss_total += loss\n",
    "        plot_loss_total += loss\n",
    "\n",
    "        if iter % print_every == 0:\n",
    "            print_loss_avg = print_loss_total / print_every\n",
    "            print_loss_total = 0\n",
    "            print('%s (%d %d%%) %.4f' % (timeSince(start, iter / n_iters),\n",
    "                                         iter, iter / n_iters * 100, print_loss_avg))\n",
    "            \n",
    "            plot_loss_avg = plot_loss_total / plot_every\n",
    "            plot_losses.append(plot_loss_avg)\n",
    "            plot_loss_total = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "788c19a0",
   "metadata": {},
   "source": [
    "### Cross Validation\n",
    "\n",
    "We can do some cross validation and the results can be saved with the model as metadata\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "56d179ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(encoder, decoder, sentence, max_length=MAX_LENGTH):\n",
    "    with torch.no_grad():\n",
    "        input_tensor = tensorFromSentence(input_lang, sentence)\n",
    "        input_length = input_tensor.size()[0]\n",
    "        encoder_hidden = encoder.initHidden()\n",
    "\n",
    "        encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
    "\n",
    "        for ei in range(input_length):\n",
    "            encoder_output, encoder_hidden = encoder(input_tensor[ei],\n",
    "                                                     encoder_hidden)\n",
    "            encoder_outputs[ei] += encoder_output[0, 0]\n",
    "\n",
    "        decoder_input = torch.tensor([[SOS_token]], device=device)  # SOS\n",
    "\n",
    "        decoder_hidden = encoder_hidden\n",
    "\n",
    "        decoded_words = []\n",
    "        decoder_attentions = torch.zeros(max_length, max_length)\n",
    "\n",
    "        for di in range(max_length):\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs)\n",
    "            decoder_attentions[di] = decoder_attention.data\n",
    "            topv, topi = decoder_output.data.topk(1)\n",
    "            if topi.item() == EOS_token:\n",
    "                decoded_words.append('<EOS>')\n",
    "                break\n",
    "            else:\n",
    "                decoded_words.append(output_lang.index2word[topi.item()])\n",
    "\n",
    "            decoder_input = topi.squeeze().detach()\n",
    "\n",
    "        return decoded_words, decoder_attentions[:di + 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0b828039",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluateRandomly(encoder, decoder, n=10):\n",
    "    for i in range(n):\n",
    "        pair = random.choice(pairs)\n",
    "        print('>', pair[0])\n",
    "        print('=', pair[1])\n",
    "        output_words, attentions = evaluate(encoder, decoder, pair[0])\n",
    "        output_sentence = ' '.join(output_words)\n",
    "        print('<', output_sentence)\n",
    "        print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8e0d628b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53m 47s (- 3981m 5s) (1000 1%) 4.5772\n",
      "105m 1s (- 3833m 41s) (2000 2%) 3.6802\n",
      "166m 58s (- 4007m 35s) (3000 4%) 3.7354\n",
      "225m 44s (- 4006m 55s) (4000 5%) 3.8387\n",
      "276m 6s (- 3865m 35s) (5000 6%) 3.7919\n",
      "326m 30s (- 3754m 50s) (6000 8%) 3.5912\n",
      "379m 53s (- 3690m 22s) (7000 9%) 3.5397\n",
      "430m 38s (- 3606m 41s) (8000 10%) 3.5652\n",
      "483m 39s (- 3546m 51s) (9000 12%) 3.6746\n",
      "534m 42s (- 3475m 37s) (10000 13%) 3.5899\n",
      "589m 9s (- 3427m 51s) (11000 14%) 3.4993\n",
      "644m 26s (- 3383m 19s) (12000 16%) 3.3854\n",
      "699m 10s (- 3334m 31s) (13000 17%) 3.5558\n",
      "756m 8s (- 3294m 36s) (14000 18%) 3.5011\n",
      "819m 17s (- 3277m 11s) (15000 20%) 3.6550\n",
      "879m 30s (- 3243m 9s) (16000 21%) 3.4463\n",
      "936m 1s (- 3193m 29s) (17000 22%) 3.4391\n",
      "990m 26s (- 3136m 23s) (18000 24%) 3.5100\n",
      "1042m 8s (- 3071m 35s) (19000 25%) 3.4663\n",
      "1096m 32s (- 3015m 29s) (20000 26%) 3.3888\n",
      "1147m 28s (- 2950m 39s) (21000 28%) 3.3953\n",
      "1199m 43s (- 2890m 15s) (22000 29%) 3.5080\n",
      "1249m 29s (- 2824m 55s) (23000 30%) 3.4492\n",
      "1301m 3s (- 2764m 45s) (24000 32%) 3.5475\n",
      "1354m 12s (- 2708m 25s) (25000 33%) 3.3766\n",
      "1407m 50s (- 2653m 13s) (26000 34%) 3.4128\n",
      "1460m 18s (- 2596m 6s) (27000 36%) 3.5166\n",
      "1515m 22s (- 2543m 40s) (28000 37%) 3.4680\n",
      "1570m 0s (- 2490m 21s) (29000 38%) 3.4848\n",
      "1623m 1s (- 2434m 31s) (30000 40%) 3.3749\n",
      "1673m 56s (- 2375m 55s) (31000 41%) 3.5017\n",
      "1724m 7s (- 2316m 48s) (32000 42%) 3.3898\n",
      "1773m 37s (- 2257m 20s) (33000 44%) 3.5329\n",
      "1823m 21s (- 2198m 45s) (34000 45%) 3.4501\n",
      "1872m 35s (- 2140m 5s) (35000 46%) 3.4294\n",
      "1924m 42s (- 2085m 6s) (36000 48%) 3.3438\n",
      "1974m 28s (- 2027m 50s) (37000 49%) 3.4062\n",
      "2032m 12s (- 1978m 44s) (38000 50%) 3.4693\n",
      "2090m 20s (- 1929m 32s) (39000 52%) 3.2995\n",
      "2148m 35s (- 1880m 0s) (40000 53%) 3.3220\n",
      "2208m 17s (- 1831m 16s) (41000 54%) 3.4754\n",
      "2265m 58s (- 1780m 24s) (42000 56%) 3.3932\n",
      "2323m 20s (- 1729m 0s) (43000 57%) 3.3527\n",
      "2381m 2s (- 1677m 33s) (44000 58%) 3.3222\n",
      "2438m 34s (- 1625m 43s) (45000 60%) 3.4322\n",
      "2496m 22s (- 1573m 48s) (46000 61%) 3.4363\n",
      "2554m 51s (- 1522m 2s) (47000 62%) 3.4041\n",
      "2614m 18s (- 1470m 33s) (48000 64%) 3.4291\n",
      "2672m 9s (- 1417m 52s) (49000 65%) 3.3200\n",
      "2732m 31s (- 1366m 15s) (50000 66%) 3.3098\n",
      "2789m 43s (- 1312m 48s) (51000 68%) 3.3264\n",
      "2852m 19s (- 1261m 36s) (52000 69%) 3.4507\n",
      "2914m 5s (- 1209m 37s) (53000 70%) 3.5303\n",
      "2973m 31s (- 1156m 22s) (54000 72%) 3.3720\n",
      "3034m 42s (- 1103m 31s) (55000 73%) 3.5405\n",
      "3096m 13s (- 1050m 30s) (56000 74%) 3.4879\n",
      "3157m 41s (- 997m 9s) (57000 76%) 3.3560\n",
      "3217m 4s (- 942m 56s) (58000 77%) 3.4345\n",
      "3278m 9s (- 888m 59s) (59000 78%) 3.4586\n",
      "3337m 45s (- 834m 26s) (60000 80%) 3.5549\n",
      "3395m 19s (- 779m 15s) (61000 81%) 3.3440\n",
      "3454m 33s (- 724m 20s) (62000 82%) 3.3431\n",
      "3515m 10s (- 669m 33s) (63000 84%) 3.4217\n",
      "3572m 41s (- 614m 3s) (64000 85%) 3.3582\n",
      "3630m 47s (- 558m 34s) (65000 86%) 3.3860\n",
      "3690m 12s (- 503m 12s) (66000 88%) 3.2525\n",
      "3749m 47s (- 447m 44s) (67000 89%) 3.4545\n",
      "3808m 54s (- 392m 5s) (68000 90%) 3.2990\n",
      "3868m 9s (- 336m 21s) (69000 92%) 3.3365\n",
      "3931m 37s (- 280m 49s) (70000 93%) 3.4112\n",
      "3995m 58s (- 225m 7s) (71000 94%) 3.3736\n",
      "4060m 54s (- 169m 12s) (72000 96%) 3.3137\n",
      "4121m 47s (- 112m 55s) (73000 97%) 3.2680\n",
      "4178m 46s (- 56m 28s) (74000 98%) 3.2029\n",
      "4236m 20s (- 0m 0s) (75000 100%) 3.2304\n"
     ]
    }
   ],
   "source": [
    "hidden_size = 256\n",
    "\n",
    "encoder = EncoderRNN(input_lang.n_words, hidden_size).to(device)\n",
    "decoder = AttnDecoderRNN(hidden_size, output_lang.n_words, dropout_p=0.1).to(device)\n",
    "\n",
    "trainIters(encoder, decoder, 75000, print_every=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0d9d9805",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> its hypocritical govt collects and releases personal information for political gain i .e . irs scandal . yet snowden collects and releases govt information for patriotic reasons and he is a terrorist traitor . snowden has only done to the feds that they continually do to us every day . i am amazed that more people are not sickened by the over reaching power of the federal govt which all started under bush but obama has only made it worse . cispa nsa obamacare irs benghazi fast and furious etc . all examples of how our government is out of control with zero accountability . just say no to big government ! it does not matter if you are democrat republican progressive or whatever when it is not your guy in the white house they will still have to much power . edit \n",
      "= they are all liars and need to be thrown out all of them . . .\n",
      "< i m not to a to in the . . <EOS>\n",
      "\n",
      "> most of the content this subreddit brings to my front page is bullying . the content itself and the act of trying to cringe at it are bullying in and of themselves . this means that the average redditor cannot perceive the difference between being a superiority glutton and an embarrassment empath . in fact i wonder if there is a difference . i don t think there is to the cringe inducing . . .victim is it ? yes . the subject of a cringe video is its victim . the cringe victim doesn t often feel a separation from empathy read pity and derision read bullying passive or not . this imho makes this entire conversation a waste of time . this is the high school cafeteria of the internet .\n",
      "= you are the bully dear reader . you are .\n",
      "< i m not to to the . <EOS>\n",
      "\n",
      "> umm my thoughts . it was a solid date this girl s stupid for not wanting a second date with a guy that really made her feel sexy and happy . couple notes though if you are attracted to the girl and you want to go somewhere and isolate her be honest about the fact that it s a date . oh and if you re on a first date and she just told you she has bad experiences going too far on first dates don t ask to go to her place ! ! ! girls like to be loved for their beauty and for who they are . you certainly conveyed your attraction to her physically but are you showing that you appreciate who she is ? when she talks are you conveying that whatever she s saying and throwing at you is the most interesting amazing thing that you could roll around in your head at that time ? while your text game isn t that important it s important i should tell you . . don t immediately text her sounding like you had one of the best times of you life . don t send several text messages if she s not responding . you shouldn t care because you re a secure man . also don t be afraid to text her back if she tells you she s busy . she would probably want a guy like you to listen to her all the time .\n",
      "= you re not coming from a place of abundance and genuine interest in her as a person where she wants you . go approach women irl where it s easier and more exciting so you also don t have to deal with the pre conception that you re a guy who resorts to okcupid to get women .\n",
      "< you re not . . . you . <EOS>\n",
      "\n",
      "> i rented a kia on a trip once not impressed . very low visibility and the interior of the thing was just off . like they made it for people of a certain size and then realized oh hey we have to make it for westerners so they just kind of enlarged everything or moved it or inches away to accommodate .\n",
      "= i m driving a huge blind spot ! ! !\n",
      "< i m not a <EOS>\n",
      "\n",
      "> of course you re not a trekkie . you re advocating a neo luddite prehistoric diet based from pre agricultural hunter gather times if you re at all consequential in your primitivist ideology something like star trek ought to frustrate you . the paleo diet is a weird new age festishizing of the way things used to be before civilization . as an anthropological matter it s interesting to me . what leads these this small group of people to consider mimicking a lifestyle from thousands of years ago ? will it ever get out of the upper middle class internet ghetto ? or will it die there ? and why do these people myopically choose to embrace only the diet and not other corresponding cultural practices ? like hunting foraging nomadic living and cooking over flames ? i m look forward to reading some anthro or sociology grad student s paper on it sometime . personally i think the term paleo diet will die out as quickly as it popped up . i certainly don t think it s any threat to veganism or animal liberation .\n",
      "= you re being trolled\n",
      "< you re not to <EOS>\n",
      "\n",
      "> strength of schedule is a component of the overall ranking and it is calculated differently . overall nd was number but alabama was number in the sos department . simple averages would show that a team who is overall but with a sos of anything less than read anything but the number one team would harm the number one team in terms of sos bama because their average can only start at as oppposed to like nd can . so in terms of pure sos anyone who plays the team with the number sos alabama benefits while every time alabama plays someone else their sos average can only go down because they can t play anoyone higher than themselves .\n",
      "= you re wrong\n",
      "< you re not to <EOS>\n",
      "\n",
      "> i m not rewriting the wall of text that i ve written so many times before for this kinda comment . . .\n",
      "= i m autofiltered and aperson is indifferent to me posting it .\n",
      "< i m a . <EOS>\n",
      "\n",
      "> our tech comes from overseas because we don t manufacture like we used to . pre s or so the technology we bought here was pretty much made here . this stems from the very favorable control we had on global manufacture following ww . the s produced a lot of technological leaps based on this control of manufacture as well as wartime science being turned towards the civilian sector . the reason why foreign tech is more common here is because it s often cheaper . apple makes the iphone . it is made in manufacturing complexes like the notorious foxconn plant . the overhead in employee payroll is negligible . if the iphone were fabricated in the us people would be taking a mortgage out to purchase one because the increased cost in manufacture from paying the employees plus insuring them etc also i feel like you re referencing gadgets . mobile devices entertainment ease of living type things . one big thing to consider is that technology is more than doodads technical term . in the search for cheaper building materials happy accidents happen allowing more and more developments to occur . for example the guy who invented teflon was looking to make a safer refrigerant and his flub became the non stick coating on your skillet .\n",
      "= we re technologically advanced not production advanced .\n",
      "< you re not . . <EOS>\n",
      "\n",
      "> ok so farting may have been a bad example . . . because i think we have evolved to dislike that smell or the smell of human waste to keep us from trying to eat something contaminated by it . aside from that yes this is a good question that i don t think has a definitive answer for everyone and everything . it s a mind vs . body debate . are rewards rewards because we are taught that they are ? or do they cause a physiological reaction in our brain s reward center which leads to them being treated as rewards by society ? do children like candy because it s used as a reward or because of the reward it causes ? i think more often than not there are usually physiological reasons the positive reinforcements we have are what they are why we consider some foods and smells pleasant .\n",
      "= i m sure this is feasible in some cases and not in others . as for this specific example i don t believe so .\n",
      "< i m not . to . . <EOS>\n",
      "\n",
      "> i don t think any woman even the most fervent feminist would disagree with you there . what you are saying isn t an argument . we have the equivalent with assault rape in the parks in my city rare occurances . the police issues a statement that they recommend not walking this or that area at this or that time of night and that cannot be mistaken for victim blaming . but the sad truth is that those open assaults are the rarity . the norm when it comes to rape in my country is either at a party festival or other venue where people are most likely drinking and the perpetrator is most often someone the girl knows . i am a girl and i know more than girls who have told me personally about their experiences with rape . only of them went to the police . none of them had consensual or even conscious sex with any of their attackers . my point is this if there is anything guys can do to make drunk sex safer even if it is awkward for everyone then what is the real harm in that ? if you are a decent guy and you double check that the girl you are with is happy and in on it then you have done your part . you are not a rapist and the chance of getting arrested for rape in that situation is minimal .\n",
      "= i am a girl not a hardcore feminist . let s be reasonable .\n",
      "< you are not a to . . . <EOS>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "evaluateRandomly(encoder, decoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92d9b23c",
   "metadata": {},
   "source": [
    "### saving the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1fe9c4a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style='white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace'><span style='color: #7fbfbf; text-decoration-color: #7fbfbf'>[20:51:51] </span><span style='color: #000080; text-decoration-color: #000080'>INFO    </span> <span style='font-weight: bold'>[</span>boot<span style='font-weight: bold'>]</span> Successfully saved                                                \n",
       "<span style='color: #7fbfbf; text-decoration-color: #7fbfbf'>           </span>         <span style='color: #800080; text-decoration-color: #800080; font-weight: bold'>Model</span><span style='font-weight: bold'>(</span><span style='color: #808000; text-decoration-color: #808000'>tag</span>=<span style='color: #008000; text-decoration-color: #008000'>'pytorch_seq2seq_encoder:7pirkkus2gfh7chj'</span>, <span style='color: #808000; text-decoration-color: #808000'>path</span>=<span style='color: #008000; text-decoration-color: #008000'>'/Users/spence/be</span>\n",
       "<span style='color: #7fbfbf; text-decoration-color: #7fbfbf'>           </span>         <span style='color: #008000; text-decoration-color: #008000'>ntoml/models/pytorch_seq2seq_encoder/7pirkkus2gfh7chj/'</span><span style='font-weight: bold'>)</span>                    \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[20:51:51]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m \u001b[1m[\u001b[0mboot\u001b[1m]\u001b[0m Successfully saved                                                \n",
       "\u001b[2;36m           \u001b[0m         \u001b[1;35mModel\u001b[0m\u001b[1m(\u001b[0m\u001b[33mtag\u001b[0m=\u001b[32m'pytorch_seq2seq_encoder\u001b[0m\u001b[32m:7pirkkus2gfh7chj'\u001b[0m, \u001b[33mpath\u001b[0m=\u001b[32m'/Users/spence/be\u001b[0m\n",
       "\u001b[2;36m           \u001b[0m         \u001b[32mntoml/models/pytorch_seq2seq_encoder/7pirkkus2gfh7chj/'\u001b[0m\u001b[1m)\u001b[0m                    \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style='white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace'><span style='color: #7fbfbf; text-decoration-color: #7fbfbf'>           </span><span style='color: #000080; text-decoration-color: #000080'>INFO    </span> <span style='font-weight: bold'>[</span>boot<span style='font-weight: bold'>]</span> Successfully saved                                                \n",
       "<span style='color: #7fbfbf; text-decoration-color: #7fbfbf'>           </span>         <span style='color: #800080; text-decoration-color: #800080; font-weight: bold'>Model</span><span style='font-weight: bold'>(</span><span style='color: #808000; text-decoration-color: #808000'>tag</span>=<span style='color: #008000; text-decoration-color: #008000'>'pytorch_seq2seq_decoder:7p4qygus2gfh7chj'</span>, <span style='color: #808000; text-decoration-color: #808000'>path</span>=<span style='color: #008000; text-decoration-color: #008000'>'/Users/spence/be</span>\n",
       "<span style='color: #7fbfbf; text-decoration-color: #7fbfbf'>           </span>         <span style='color: #008000; text-decoration-color: #008000'>ntoml/models/pytorch_seq2seq_decoder/7p4qygus2gfh7chj/'</span><span style='font-weight: bold'>)</span>                    \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m          \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m \u001b[1m[\u001b[0mboot\u001b[1m]\u001b[0m Successfully saved                                                \n",
       "\u001b[2;36m           \u001b[0m         \u001b[1;35mModel\u001b[0m\u001b[1m(\u001b[0m\u001b[33mtag\u001b[0m=\u001b[32m'pytorch_seq2seq_decoder\u001b[0m\u001b[32m:7p4qygus2gfh7chj'\u001b[0m, \u001b[33mpath\u001b[0m=\u001b[32m'/Users/spence/be\u001b[0m\n",
       "\u001b[2;36m           \u001b[0m         \u001b[32mntoml/models/pytorch_seq2seq_decoder/7p4qygus2gfh7chj/'\u001b[0m\u001b[1m)\u001b[0m                    \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import bentoml\n",
    "\n",
    "tag_encoder = bentoml.pytorch.save(\n",
    "    'pytorch_seq2seq_encoder',\n",
    "    encoder,\n",
    ")\n",
    "\n",
    "tag_decoder = bentoml.pytorch.save(\n",
    "    'pytorch_seq2seq_decoder',\n",
    "    decoder,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdf35e55",
   "metadata": {},
   "source": [
    "## Create a BentoML Service for serving the model\n",
    "\n",
    "Note: using `%%writefile` here because `bentoml.Service` instance must be created in a separate `.py` file\n",
    "\n",
    "Even though we have only one model, we can create as many api endpoints as we want. Here we create two end points `summarize` and `summarize_batch`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f3e2f590",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting service.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile service.py\n",
    "\n",
    "import typing as t\n",
    "\n",
    "import bentoml\n",
    "from bentoml.io import Text\n",
    "\n",
    "\n",
    "def unicodeToAscii(s):\n",
    "    return \"\".join(\n",
    "        c for c in unicodedata.normalize(\"NFD\", s) if unicodedata.category(c) != \"Mn\"\n",
    "    )\n",
    "\n",
    "\n",
    "def normalizeString(s):\n",
    "    s = unicodeToAscii(s.lower().strip())\n",
    "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
    "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
    "    return s\n",
    "\n",
    "\n",
    "encoder_runner = bentoml.pytorch.load_runner(\"pytorch_seq2seq_encoder:latest\")\n",
    "decoder_runner = bentoml.pytorch.load_runner(\"pytorch_seq2seq_decoder:latest\")\n",
    "\n",
    "svc = bentoml.Service(\n",
    "    name=\"pytorch_seq2seq\",\n",
    "    runners=[\n",
    "        encoder_runner,\n",
    "        decoder_runner,\n",
    "    ],\n",
    ")\n",
    "\n",
    "\n",
    "@svc.api(input=Text(), output=Text())\n",
    "async def summarize(input_sentence: str) -> str:\n",
    "    input_sentence = normalizeString(input_sentence)\n",
    "    enc_sentence = await encoder.async_run(input_sentence)\n",
    "    res = await decoder.async_run(enc_sentence)\n",
    "    return res[\"generated_text\"]\n",
    "\n",
    "\n",
    "@svc.api(input=Text(), output=Text())\n",
    "async def summarize_batch(input_arr: [str]) -> [str]:\n",
    "    input_arr = list(map(normalizeString, input_arr))\n",
    "    results = []\n",
    "    for input_sentence in input_arr:\n",
    "        enc_arr = await encoder.async_run(input_arr)\n",
    "        res = await decoder.async_run(enc_arr)\n",
    "        results.append(res[\"generated_text\"])\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "590147aa",
   "metadata": {},
   "source": [
    "Start a dev model server to test out the service defined above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "29173871",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m[20:52:45]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m \u001b[1m[\u001b[0mboot\u001b[1m]\u001b[0m Starting development BentoServer from                \n",
      "\u001b[2;36m           \u001b[0m         \u001b[32m'service.py:svc'\u001b[0m                                            \n",
      "\u001b[2;36m[20:52:52]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m \u001b[1m[\u001b[0mdev_api_server\u001b[1m]\u001b[0m Service imported from source:              \n",
      "\u001b[2;36m           \u001b[0m         \u001b[1;35mbentoml.Service\u001b[0m\u001b[1m(\u001b[0m\u001b[33mname\u001b[0m=\u001b[32m'pytorch_seq2seq'\u001b[0m,                   \n",
      "\u001b[2;36m           \u001b[0m         \u001b[33mimport_str\u001b[0m=\u001b[32m'service\u001b[0m\u001b[32m:svc'\u001b[0m, \u001b[33mworking_dir\u001b[0m=\u001b[32m'/Users/spence/Documen\u001b[0m\n",
      "\u001b[2;36m           \u001b[0m         \u001b[32mts/GitHub/gallery/pytorch_seq2seq'\u001b[0m\u001b[1m)\u001b[0m                         \n",
      "\u001b[2;36m[20:52:52]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m \u001b[1m[\u001b[0mdev_api_server\u001b[1m]\u001b[0m Started server process \u001b[1m[\u001b[0m\u001b[1;36m12124\u001b[0m\u001b[1m]\u001b[0m             \n",
      "\u001b[2;36m          \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m \u001b[1m[\u001b[0mdev_api_server\u001b[1m]\u001b[0m Waiting for application startup.           \n",
      "\u001b[2;36m          \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m \u001b[1m[\u001b[0mdev_api_server\u001b[1m]\u001b[0m Application startup complete.              \n",
      "\u001b[2;36m          \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m \u001b[1m[\u001b[0mdev_api_server\u001b[1m]\u001b[0m Uvicorn running on \u001b[4;94mhttp://127.0.0.1:5000\u001b[0m   \n",
      "\u001b[2;36m           \u001b[0m         \u001b[1m(\u001b[0mPress CTRL+C to quit\u001b[1m)\u001b[0m                                      \n",
      "^C\n",
      "\u001b[2;36m[20:52:56]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m \u001b[1m[\u001b[0mdev_api_server\u001b[1m]\u001b[0m Shutting down                              \n",
      "\u001b[2;36m          \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m \u001b[1m[\u001b[0mdev_api_server\u001b[1m]\u001b[0m Waiting for application shutdown.          \n",
      "\u001b[2;36m          \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m \u001b[1m[\u001b[0mdev_api_server\u001b[1m]\u001b[0m Application shutdown complete.             \n",
      "\u001b[2;36m          \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m \u001b[1m[\u001b[0mdev_api_server\u001b[1m]\u001b[0m Finished server process \u001b[1m[\u001b[0m\u001b[1;36m12124\u001b[0m\u001b[1m]\u001b[0m            \n"
     ]
    }
   ],
   "source": [
    "!python3 -m bentoml serve service.py:svc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7f03564",
   "metadata": {},
   "source": [
    "## Build a Bento for distribution and deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "207561bc",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style='white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace'><span style='color: #7fbfbf; text-decoration-color: #7fbfbf'>[20:53:00] </span><span style='color: #000080; text-decoration-color: #000080'>INFO    </span> <span style='font-weight: bold'>[</span>boot<span style='font-weight: bold'>]</span> Building BentoML service <span style='color: #008000; text-decoration-color: #008000'>'pytorch_seq2seq:eup5ynus2kfh7chj'</span> from\n",
       "<span style='color: #7fbfbf; text-decoration-color: #7fbfbf'>           </span>         build context <span style='color: #008000; text-decoration-color: #008000'>'/Users/spence/Documents/GitHub/gallery/pytorch_seq2seq'</span>   \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[20:53:00]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m \u001b[1m[\u001b[0mboot\u001b[1m]\u001b[0m Building BentoML service \u001b[32m'pytorch_seq2seq:eup5ynus2kfh7chj'\u001b[0m from\n",
       "\u001b[2;36m           \u001b[0m         build context \u001b[32m'/Users/spence/Documents/GitHub/gallery/pytorch_seq2seq'\u001b[0m   \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style='white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace'><span style='color: #7fbfbf; text-decoration-color: #7fbfbf'>           </span><span style='color: #000080; text-decoration-color: #000080'>INFO    </span> <span style='font-weight: bold'>[</span>boot<span style='font-weight: bold'>]</span> Packing model <span style='color: #008000; text-decoration-color: #008000'>'pytorch_seq2seq_encoder:735m27es2gfh7chj'</span> from        \n",
       "<span style='color: #7fbfbf; text-decoration-color: #7fbfbf'>           </span>         <span style='color: #008000; text-decoration-color: #008000'>'/Users/spence/bentoml/models/pytorch_seq2seq_encoder/735m27es2gfh7chj'</span>     \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m          \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m \u001b[1m[\u001b[0mboot\u001b[1m]\u001b[0m Packing model \u001b[32m'pytorch_seq2seq_encoder:735m27es2gfh7chj'\u001b[0m from        \n",
       "\u001b[2;36m           \u001b[0m         \u001b[32m'/Users/spence/bentoml/models/pytorch_seq2seq_encoder/735m27es2gfh7chj'\u001b[0m     \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style='white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace'><span style='color: #7fbfbf; text-decoration-color: #7fbfbf'>           </span><span style='color: #000080; text-decoration-color: #000080'>INFO    </span> <span style='font-weight: bold'>[</span>boot<span style='font-weight: bold'>]</span> Packing model <span style='color: #008000; text-decoration-color: #008000'>'pytorch_seq2seq_decoder:74jnn5us2gfh7chj'</span> from        \n",
       "<span style='color: #7fbfbf; text-decoration-color: #7fbfbf'>           </span>         <span style='color: #008000; text-decoration-color: #008000'>'/Users/spence/bentoml/models/pytorch_seq2seq_decoder/74jnn5us2gfh7chj'</span>     \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m          \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m \u001b[1m[\u001b[0mboot\u001b[1m]\u001b[0m Packing model \u001b[32m'pytorch_seq2seq_decoder:74jnn5us2gfh7chj'\u001b[0m from        \n",
       "\u001b[2;36m           \u001b[0m         \u001b[32m'/Users/spence/bentoml/models/pytorch_seq2seq_decoder/74jnn5us2gfh7chj'\u001b[0m     \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style='white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace'><span style='color: #7fbfbf; text-decoration-color: #7fbfbf'>           </span><span style='color: #000080; text-decoration-color: #000080'>INFO    </span> <span style='font-weight: bold'>[</span>boot<span style='font-weight: bold'>]</span> Locking PyPI package versions..                                   \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m          \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m \u001b[1m[\u001b[0mboot\u001b[1m]\u001b[0m Locking PyPI package versions..                                   \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style='white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace'><span style='color: #7fbfbf; text-decoration-color: #7fbfbf'>[20:53:01] </span><span style='color: #000080; text-decoration-color: #000080'>INFO    </span> <span style='font-weight: bold'>[</span>boot<span style='font-weight: bold'>]</span> Bento build success,                                              \n",
       "<span style='color: #7fbfbf; text-decoration-color: #7fbfbf'>           </span>         <span style='color: #800080; text-decoration-color: #800080; font-weight: bold'>Bento</span><span style='font-weight: bold'>(</span><span style='color: #808000; text-decoration-color: #808000'>tag</span>=<span style='color: #008000; text-decoration-color: #008000'>'pytorch_seq2seq:eup5ynus2kfh7chj'</span><span style='font-weight: bold'>)</span> created                  \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[20:53:01]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m \u001b[1m[\u001b[0mboot\u001b[1m]\u001b[0m Bento build success,                                              \n",
       "\u001b[2;36m           \u001b[0m         \u001b[1;35mBento\u001b[0m\u001b[1m(\u001b[0m\u001b[33mtag\u001b[0m=\u001b[32m'pytorch_seq2seq\u001b[0m\u001b[32m:eup5ynus2kfh7chj'\u001b[0m\u001b[1m)\u001b[0m created                  \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Bento(tag='pytorch_seq2seq:eup5ynus2kfh7chj')"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bentoml.build(\n",
    "    'service.py:svc',\n",
    "    include=['*.py'],\n",
    "    exclude=['tests/'],\n",
    "    description='file:./README.md',\n",
    "    python=dict(\n",
    "        packages=['torch'],\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36306933",
   "metadata": {},
   "source": [
    "Starting a dev server with the Bento build:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ec4b9dff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m[20:53:17]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m \u001b[1m[\u001b[0mboot\u001b[1m]\u001b[0m Starting development BentoServer from                \n",
      "\u001b[2;36m           \u001b[0m         \u001b[32m'pytorch_seq2seq:latest'\u001b[0m                                  \n",
      "\u001b[2;36m[20:53:24]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m \u001b[1m[\u001b[0mdev_api_server\u001b[1m]\u001b[0m Service loaded from Bento store:           \n",
      "\u001b[2;36m           \u001b[0m         \u001b[1;35mbentoml.Service\u001b[0m\u001b[1m(\u001b[0m\u001b[33mtag\u001b[0m=\u001b[32m'pytorch_seq2seq\u001b[0m\u001b[32m:eup5ynus2kfh7chj'\u001b[0m, \u001b[33mpa\u001b[0m\n",
      "\u001b[2;36m           \u001b[0m         \u001b[33mth\u001b[0m=\u001b[32m'/Users/spence/bentoml/bentos/pytorch_seq2seq/eup5ynus2\u001b[0m\n",
      "\u001b[2;36m           \u001b[0m         \u001b[32mkfh7chj'\u001b[0m\u001b[1m)\u001b[0m                                                   \n",
      "\u001b[2;36m[20:53:24]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m \u001b[1m[\u001b[0mdev_api_server\u001b[1m]\u001b[0m Started server process \u001b[1m[\u001b[0m\u001b[1;36m12144\u001b[0m\u001b[1m]\u001b[0m             \n",
      "\u001b[2;36m          \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m \u001b[1m[\u001b[0mdev_api_server\u001b[1m]\u001b[0m Waiting for application startup.           \n",
      "\u001b[2;36m          \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m \u001b[1m[\u001b[0mdev_api_server\u001b[1m]\u001b[0m Application startup complete.              \n",
      "\u001b[2;36m          \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m \u001b[1m[\u001b[0mdev_api_server\u001b[1m]\u001b[0m Uvicorn running on \u001b[4;94mhttp://127.0.0.1:5000\u001b[0m   \n",
      "\u001b[2;36m           \u001b[0m         \u001b[1m(\u001b[0mPress CTRL+C to quit\u001b[1m)\u001b[0m                                      \n",
      "^C\n",
      "\u001b[2;36m[20:53:34]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m \u001b[1m[\u001b[0mdev_api_server\u001b[1m]\u001b[0m Shutting down                              \n",
      "\u001b[2;36m          \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m \u001b[1m[\u001b[0mdev_api_server\u001b[1m]\u001b[0m Waiting for application shutdown.          \n",
      "\u001b[2;36m          \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m \u001b[1m[\u001b[0mdev_api_server\u001b[1m]\u001b[0m Application shutdown complete.             \n",
      "\u001b[2;36m          \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m \u001b[1m[\u001b[0mdev_api_server\u001b[1m]\u001b[0m Finished server process \u001b[1m[\u001b[0m\u001b[1;36m12144\u001b[0m\u001b[1m]\u001b[0m            \n"
     ]
    }
   ],
   "source": [
    "!python3 -m bentoml serve pytorch_seq2seq:latest"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  },
  "name": "pytorch_mnist.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
